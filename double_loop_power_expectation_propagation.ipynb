{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from abc import ABC, abstractmethod\n",
        "from scipy.special import expit, logsumexp, ndtr, ndtri\n",
        "from numpy.polynomial.hermite import hermgauss\n",
        "from scipy.stats import t as student_t, norm\n",
        "from scipy.linalg import cho_factor, cho_solve\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error"
      ],
      "metadata": {
        "id": "LvzLBkXWvWvU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical stability constants\n",
        "JITTER = 1e-8\n",
        "LOG_CLIP = 20.0\n",
        "MIN_DAMPING = 1e-2\n",
        "PROBIT_SCALE = np.sqrt(1.0 + np.pi / 8.0)"
      ],
      "metadata": {
        "id": "V2vuE244vcFz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ],
      "metadata": {
        "id": "Q3Rp1CRVvYuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def robust_invert(\n",
        "    M,\n",
        "    jitter=JITTER,\n",
        "    max_tries=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Robust matrix inversion using Cholesky decomposition with fallback.\n",
        "\n",
        "    Args:\n",
        "        M: Square matrix to invert\n",
        "        jitter: Initial jitter value for numerical stability\n",
        "        max_tries: Maximum number of attempts with increasing jitter\n",
        "\n",
        "    Returns:\n",
        "        Inverted matrix\n",
        "    \"\"\"\n",
        "    n = M.shape[0]\n",
        "    I = np.eye(n)\n",
        "    for _ in range(max_tries):\n",
        "        try:\n",
        "            c, lower = cho_factor(M, lower=True, check_finite=False)\n",
        "            return cho_solve((c, lower), I, check_finite=False)\n",
        "        except np.linalg.LinAlgError:\n",
        "            M = M + jitter * I\n",
        "            jitter *= 10.0\n",
        "    return np.linalg.pinv(M)"
      ],
      "metadata": {
        "id": "Hmzvy-RrvZ32"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_gh_quadrature(gh_points):\n",
        "    \"\"\"\n",
        "    Get Gauss-Hermite quadrature nodes and weights.\n",
        "\n",
        "    Args:\n",
        "        gh_points: Number of quadrature points\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (nodes, normalized_weights)\n",
        "    \"\"\"\n",
        "    nodes, weights = hermgauss(gh_points)\n",
        "    return nodes, weights / np.sqrt(np.pi)"
      ],
      "metadata": {
        "id": "4WMY4tb7vZ1h"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _compute_tilted_moments_standard(\n",
        "    m_c,\n",
        "    v_c,\n",
        "    gh_z,\n",
        "    gh_w,\n",
        "    like\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute tilted moments using standard likelihood function.\n",
        "\n",
        "    Args:\n",
        "        m_c: Cavity mean\n",
        "        v_c: Cavity variance\n",
        "        gh_z: Gauss-Hermite nodes\n",
        "        gh_w: Gauss-Hermite weights\n",
        "        like: Likelihood function\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (first_moment, second_moment, normalization_constant)\n",
        "    \"\"\"\n",
        "    v_c = max(v_c, JITTER)\n",
        "    a = m_c + np.sqrt(2.0 * v_c) * gh_z\n",
        "    phi = like(a)\n",
        "    Z = gh_w.dot(phi)\n",
        "    if not np.isfinite(Z) or Z <= 0:\n",
        "        return None, None, Z\n",
        "    E1 = gh_w.dot(a * phi) / Z\n",
        "    E2 = gh_w.dot(a * a * phi) / Z\n",
        "    return E1, E2, Z"
      ],
      "metadata": {
        "id": "fu4d7WTkvZyq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _compute_tilted_moments_loglike(\n",
        "    m_c,\n",
        "    v_c,\n",
        "    gh_z,\n",
        "    gh_w,\n",
        "    log_phi\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute tilted moments using log-likelihood function.\n",
        "\n",
        "    Args:\n",
        "        m_c: Cavity mean\n",
        "        v_c: Cavity variance\n",
        "        gh_z: Gauss-Hermite nodes\n",
        "        gh_w: Gauss-Hermite weights\n",
        "        log_phi: Log-likelihood function\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (first_moment, second_moment)\n",
        "    \"\"\"\n",
        "    v_c = max(v_c, JITTER)\n",
        "    a = m_c + np.sqrt(2.0 * v_c) * gh_z\n",
        "    a_clip = np.clip(a, -LOG_CLIP, LOG_CLIP)\n",
        "    lwphi = np.log(gh_w) + log_phi(a_clip)\n",
        "    logZ = logsumexp(lwphi)\n",
        "    if not np.isfinite(logZ):\n",
        "        return None, None\n",
        "    w = np.exp(lwphi - logZ)\n",
        "    E1 = np.sum(a * w)\n",
        "    E2 = np.sum(a * a * w)\n",
        "    return E1, E2"
      ],
      "metadata": {
        "id": "Zw5vKiwovm16"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Likelihoods"
      ],
      "metadata": {
        "id": "eCNS7_P-vmzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Likelihood(ABC):\n",
        "    \"\"\"Abstract base class for likelihood functions.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def tilted_moments(\n",
        "        self,\n",
        "        m_c,\n",
        "        v_c,\n",
        "        y,\n",
        "        gh_z,\n",
        "        gh_w\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Compute tilted moments for the likelihood.\n",
        "\n",
        "        Args:\n",
        "            m_c: Cavity mean\n",
        "            v_c: Cavity variance\n",
        "            y: Observed data point\n",
        "            gh_z: Gauss-Hermite nodes\n",
        "            gh_w: Gauss-Hermite weights\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (first_moment, second_moment)\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(\n",
        "        self,\n",
        "        m,\n",
        "        v\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Compute predictive mean given latent moments.\n",
        "\n",
        "        Args:\n",
        "            m: Latent mean\n",
        "            v: Latent variance\n",
        "\n",
        "        Returns:\n",
        "            Predicted mean\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def sample(\n",
        "        self,\n",
        "        A,\n",
        "        rng\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Sample from the likelihood given latent values.\n",
        "\n",
        "        Args:\n",
        "            A: Latent function values\n",
        "            rng: Random number generator\n",
        "\n",
        "        Returns:\n",
        "            Sampled observations\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "metadata": {
        "id": "fsm-6VTovmw9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianLikelihood(Likelihood):\n",
        "    \"\"\"Gaussian likelihood for regression problems.\"\"\"\n",
        "\n",
        "    def __init__(self, obs_var):\n",
        "        \"\"\"\n",
        "        Initialize Gaussian likelihood.\n",
        "\n",
        "        Args:\n",
        "            obs_var: Observation noise variance\n",
        "        \"\"\"\n",
        "        self.obs_var = obs_var\n",
        "\n",
        "    def tilted_moments(\n",
        "        self,\n",
        "        m_c,\n",
        "        v_c,\n",
        "        y,\n",
        "        gh_z,\n",
        "        gh_w\n",
        "    ):\n",
        "        \"\"\"Compute tilted moments for Gaussian likelihood.\"\"\"\n",
        "        like = lambda a: np.exp(-0.5 * (y - a)**2 / self.obs_var)\n",
        "        E1, E2, _ = _compute_tilted_moments_standard(\n",
        "            m_c, v_c, gh_z, gh_w, like\n",
        "        )\n",
        "        return (E1, E2) if E1 is not None else (None, None)\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        m,\n",
        "        v\n",
        "    ):\n",
        "        \"\"\"Predict mean for Gaussian likelihood.\"\"\"\n",
        "        return m\n",
        "\n",
        "    def sample(\n",
        "        self,\n",
        "        A,\n",
        "        rng\n",
        "    ):\n",
        "        \"\"\"Sample from Gaussian likelihood.\"\"\"\n",
        "        return A + rng.randn(*A.shape) * np.sqrt(self.obs_var)"
      ],
      "metadata": {
        "id": "txuNjhW4vmun"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PoissonLikelihood(Likelihood):\n",
        "    \"\"\"Poisson likelihood for count data.\"\"\"\n",
        "\n",
        "    def tilted_moments(\n",
        "        self,\n",
        "        m_c,\n",
        "        v_c,\n",
        "        y,\n",
        "        gh_z,\n",
        "        gh_w\n",
        "    ):\n",
        "        \"\"\"Compute tilted moments for Poisson likelihood.\"\"\"\n",
        "        log_phi = lambda a: y * a - np.exp(a)\n",
        "        return _compute_tilted_moments_loglike(\n",
        "            m_c, v_c, gh_z, gh_w, log_phi\n",
        "        )\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        m,\n",
        "        v\n",
        "    ):\n",
        "        \"\"\"Predict mean for Poisson likelihood.\"\"\"\n",
        "        return np.exp(np.clip(m + 0.5 * v, -LOG_CLIP, LOG_CLIP))\n",
        "\n",
        "    def sample(\n",
        "        self,\n",
        "        A,\n",
        "        rng\n",
        "    ):\n",
        "        \"\"\"Sample from Poisson likelihood.\"\"\"\n",
        "        lam = np.exp(np.clip(A, -LOG_CLIP, LOG_CLIP))\n",
        "        return rng.poisson(lam)"
      ],
      "metadata": {
        "id": "wdr4lE1ovmsD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticLikelihood(Likelihood):\n",
        "    \"\"\"Logistic likelihood for binary classification using probit approximation.\"\"\"\n",
        "\n",
        "    def tilted_moments(\n",
        "        self,\n",
        "        m_c,\n",
        "        v_c,\n",
        "        y,\n",
        "        gh_z,\n",
        "        gh_w\n",
        "    ):\n",
        "        \"\"\"Compute tilted moments for logistic likelihood.\"\"\"\n",
        "        # Convert y to {-1, +1} for symmetric treatment\n",
        "        y_sign = 2 * y - 1  # Maps {0,1} to {-1,+1}\n",
        "\n",
        "        # Adjust cavity mean by the label\n",
        "        m_c_adj = y_sign * m_c\n",
        "\n",
        "        # Compute moments using probit approximation\n",
        "        s2 = v_c + PROBIT_SCALE**2\n",
        "        s = np.sqrt(s2)\n",
        "        z = m_c_adj / s\n",
        "\n",
        "        # Cumulative and density functions\n",
        "        Phi = norm.cdf(z)\n",
        "        phi = norm.pdf(z)\n",
        "\n",
        "        # Handle extreme cases\n",
        "        if Phi < 1e-10:\n",
        "            # Use asymptotic expansion for very negative z\n",
        "            if z < -10:\n",
        "                b = -z / v_c\n",
        "                var_t = v_c / (1 + z**2)\n",
        "            else:\n",
        "                return None, None\n",
        "        else:\n",
        "            # Standard case\n",
        "            b = phi / (Phi * s)\n",
        "            var_t = v_c - v_c**2 * b * (b + z / s2)\n",
        "\n",
        "        # First moment (adjusted back by y_sign)\n",
        "        E1 = m_c + y_sign * v_c * b\n",
        "\n",
        "        # Second moment\n",
        "        var_t = max(var_t, JITTER)\n",
        "        E2 = E1**2 + var_t\n",
        "\n",
        "        return E1, E2\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        m,\n",
        "        v\n",
        "    ):\n",
        "        \"\"\"Predict probability for logistic likelihood.\"\"\"\n",
        "        # Predictive probability using probit approximation\n",
        "        s2 = v + PROBIT_SCALE**2\n",
        "        s = np.sqrt(s2)\n",
        "        z = m / s\n",
        "        return norm.cdf(z)\n",
        "\n",
        "    def sample(\n",
        "        self,\n",
        "        A,\n",
        "        rng\n",
        "    ):\n",
        "        \"\"\"Sample from logistic likelihood.\"\"\"\n",
        "        P = expit(A)\n",
        "        return (rng.rand(*P.shape) < P).astype(int)"
      ],
      "metadata": {
        "id": "4RhGNeEev7pd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StudentTLikelihood(Likelihood):\n",
        "    \"\"\"Student-t likelihood for robust regression.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        df,\n",
        "        scale\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize Student-t likelihood.\n",
        "\n",
        "        Args:\n",
        "            df: Degrees of freedom\n",
        "            scale: Scale parameter\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.scale = scale\n",
        "\n",
        "    def tilted_moments(\n",
        "        self,\n",
        "        m_c,\n",
        "        v_c,\n",
        "        y,\n",
        "        gh_z,\n",
        "        gh_w\n",
        "    ):\n",
        "        \"\"\"Compute tilted moments for Student-t likelihood.\"\"\"\n",
        "        like = lambda a: (\n",
        "            1.0\n",
        "            + ((y - a)**2) / (self.df * self.scale**2)\n",
        "        )**(-(self.df + 1.0)/2.0)\n",
        "        E1, E2, _ = _compute_tilted_moments_standard(\n",
        "            m_c, v_c, gh_z, gh_w, like\n",
        "        )\n",
        "        return (E1, E2) if E1 is not None else (None, None)\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        m,\n",
        "        v\n",
        "    ):\n",
        "        \"\"\"Predict mean for Student-t likelihood.\"\"\"\n",
        "        return m\n",
        "\n",
        "    def sample(\n",
        "        self,\n",
        "        A,\n",
        "        rng\n",
        "    ):\n",
        "        \"\"\"Sample from Student-t likelihood.\"\"\"\n",
        "        eps = student_t(self.df).rvs(\n",
        "            size=A.shape, random_state=rng\n",
        "        ) * self.scale\n",
        "        return A + eps"
      ],
      "metadata": {
        "id": "IcZgoFahv3dc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GammaLikelihood(Likelihood):\n",
        "    \"\"\"Gamma likelihood with stable Laplace approximation.\"\"\"\n",
        "\n",
        "    def __init__(self, shape):\n",
        "        \"\"\"\n",
        "        Initialize Gamma likelihood.\n",
        "\n",
        "        Args:\n",
        "            shape: Shape parameter of Gamma distribution\n",
        "        \"\"\"\n",
        "        self.shape = shape\n",
        "\n",
        "    def tilted_moments(\n",
        "        self,\n",
        "        m_c,\n",
        "        v_c,\n",
        "        y,\n",
        "        gh_z,\n",
        "        gh_w\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Compute tilted moments using robust Newton-Raphson method.\n",
        "\n",
        "        Uses a robust Newton-Raphson method with safeguards for finding\n",
        "        the mode of the tilted distribution.\n",
        "        \"\"\"\n",
        "        v_c = max(v_c, JITTER)\n",
        "\n",
        "        # Better initialization: use moment matching\n",
        "        # For Gamma(shape, scale), mean = shape * scale\n",
        "        # If Y ~ Gamma(shape, exp(a)/shape), then E[Y] = exp(a)\n",
        "        # So reasonable initial guess is a = log(y + 1)\n",
        "        a = np.log(max(y, 0.1))\n",
        "\n",
        "        # Bounds for a to prevent overflow\n",
        "        a_min = -LOG_CLIP\n",
        "        a_max = LOG_CLIP\n",
        "\n",
        "        converged = False\n",
        "        for iter_num in range(20):\n",
        "            # Ensure a stays in bounds\n",
        "            a = np.clip(a, a_min, a_max)\n",
        "\n",
        "            # Compute gradient and Hessian with overflow protection\n",
        "            exp_neg_a = np.exp(-np.clip(a, -LOG_CLIP, LOG_CLIP))\n",
        "\n",
        "            # Gradient: d/da log[N(a|m_c,v_c) * p(y|a)]\n",
        "            g = -(a - m_c) / v_c + self.shape - self.shape * y * exp_neg_a\n",
        "\n",
        "            # Hessian: d²/da²\n",
        "            h = -1.0 / v_c + self.shape * y * exp_neg_a\n",
        "\n",
        "            # Check for numerical issues\n",
        "            if not np.isfinite(g) or not np.isfinite(h):\n",
        "                # Fall back to quadrature if Newton fails\n",
        "                log_phi = lambda a_val: (\n",
        "                    self.shape * a_val -\n",
        "                    self.shape * y * np.exp(-np.clip(a_val, -LOG_CLIP, LOG_CLIP))\n",
        "                )\n",
        "                return _compute_tilted_moments_loglike(\n",
        "                    m_c, v_c, gh_z, gh_w, log_phi\n",
        "                )\n",
        "\n",
        "            # Ensure negative definite Hessian\n",
        "            h = min(h, -JITTER)\n",
        "\n",
        "            # Newton step with step size control\n",
        "            delta = -g / h\n",
        "\n",
        "            # Line search to ensure improvement\n",
        "            step_size = 1.0\n",
        "            for _ in range(10):\n",
        "                a_new = a + step_size * delta\n",
        "                a_new = np.clip(a_new, a_min, a_max)\n",
        "\n",
        "                # Check if objective improved\n",
        "                exp_neg_a_new = np.exp(-np.clip(a_new, -LOG_CLIP, LOG_CLIP))\n",
        "                obj_old = (-0.5 * (a - m_c)**2 / v_c + self.shape * a -\n",
        "                          self.shape * y * exp_neg_a)\n",
        "                obj_new = (-0.5 * (a_new - m_c)**2 / v_c + self.shape * a_new -\n",
        "                          self.shape * y * exp_neg_a_new)\n",
        "\n",
        "                if obj_new > obj_old:\n",
        "                    a = a_new\n",
        "                    break\n",
        "                step_size *= 0.5\n",
        "\n",
        "            # Check convergence\n",
        "            if abs(delta * step_size) < 1e-8:\n",
        "                converged = True\n",
        "                break\n",
        "\n",
        "        if not converged:\n",
        "            # Fall back to quadrature\n",
        "            log_phi = lambda a_val: (\n",
        "                self.shape * a_val -\n",
        "                self.shape * y * np.exp(-np.clip(a_val, -LOG_CLIP, LOG_CLIP))\n",
        "            )\n",
        "            return _compute_tilted_moments_loglike(\n",
        "                m_c, v_c, gh_z, gh_w, log_phi\n",
        "            )\n",
        "\n",
        "        # Compute approximate variance at mode\n",
        "        var_lap = -1.0 / h\n",
        "        var_lap = max(var_lap, JITTER)\n",
        "\n",
        "        # Return moments\n",
        "        E1 = a\n",
        "        E2 = a * a + var_lap\n",
        "\n",
        "        return E1, E2\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        m,\n",
        "        v\n",
        "    ):\n",
        "        \"\"\"Predict mean for Gamma likelihood.\"\"\"\n",
        "        # Mean of Gamma distribution\n",
        "        return self.shape * np.exp(np.clip(m + 0.5 * v, -LOG_CLIP, LOG_CLIP))\n",
        "\n",
        "    def sample(\n",
        "        self,\n",
        "        A,\n",
        "        rng\n",
        "    ):\n",
        "        \"\"\"Sample from Gamma likelihood.\"\"\"\n",
        "        # Y ~ Gamma(shape, exp(A))\n",
        "        scale = np.exp(np.clip(A, -LOG_CLIP, LOG_CLIP))\n",
        "        return rng.gamma(self.shape, scale, size=A.shape)"
      ],
      "metadata": {
        "id": "aPHpwxtxv3XN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DLPEP Class"
      ],
      "metadata": {
        "id": "rfFCt7ojv3Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PowerExpectationPropagation:\n",
        "    \"\"\"\n",
        "    Power Expectation Propagation with double-loop convergence.\n",
        "\n",
        "    Implements Power EP algorithm with adaptive damping, robust numerics,\n",
        "    and double-loop optimization for enhanced convergence properties.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        likelihood: Likelihood,\n",
        "        prior_var=1.0,\n",
        "        max_iter=100,\n",
        "        tol=1e-6,\n",
        "        damping=0.8,\n",
        "        gh_points=20,\n",
        "        power_fraction=1.0,\n",
        "        outer_max_iter=10,\n",
        "        verbose=False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize Power Expectation Propagation.\n",
        "\n",
        "        Args:\n",
        "            likelihood: Likelihood object\n",
        "            prior_var: Prior variance for weights\n",
        "            max_iter: Maximum inner iterations\n",
        "            tol: Convergence tolerance\n",
        "            damping: Global damping parameter\n",
        "            gh_points: Number of Gauss-Hermite quadrature points\n",
        "            power_fraction: Power parameter (alpha)\n",
        "            outer_max_iter: Maximum outer iterations\n",
        "            verbose: Print convergence information\n",
        "        \"\"\"\n",
        "        self.like = likelihood\n",
        "        self.prior_var = prior_var\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.global_damp = damping\n",
        "        self.alpha = power_fraction\n",
        "        self.outer_max = outer_max_iter\n",
        "        self.verbose = verbose\n",
        "        self.gh_z, self.gh_w = _get_gh_quadrature(gh_points)\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        X,\n",
        "        y\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Fit the Power EP model to training data.\n",
        "\n",
        "        Args:\n",
        "            X: Feature matrix of shape (n_samples, n_features)\n",
        "            y: Target vector of shape (n_samples,)\n",
        "\n",
        "        Returns:\n",
        "            Self for method chaining\n",
        "        \"\"\"\n",
        "        X, y = np.asarray(X), np.asarray(y)\n",
        "        n, d = X.shape\n",
        "\n",
        "        # Initialize sites and posterior\n",
        "        self.Lambda0 = np.eye(d) / self.prior_var\n",
        "        self.eta0 = np.zeros(d)\n",
        "        self.tau = np.ones(n) * 0.1  # Better initialization\n",
        "        self.nu = np.zeros(n)\n",
        "        self.damping = np.full(n, self.global_damp)\n",
        "\n",
        "        self.Lambda = (self.Lambda0 +\n",
        "                      np.sum(self.tau[:, None, None] *\n",
        "                            X[:, :, None] * X[:, None, :], axis=0))\n",
        "        self.eta = self.eta0 + np.sum(self.nu[:, None] * X, axis=0)\n",
        "        self._update_posterior()\n",
        "\n",
        "        prev_bound = -np.inf\n",
        "        for outer_iter in range(self.outer_max):\n",
        "            # Inner EP until convergence\n",
        "            for inner_iter in range(self.max_iter):\n",
        "                delta = self._pep_sweep(X, y)\n",
        "                self._update_posterior()\n",
        "                if delta < self.tol:\n",
        "                    break\n",
        "\n",
        "            # Check bound improvement\n",
        "            new_bound = self._compute_bound(X, y)\n",
        "            if self.verbose and new_bound > prev_bound + 1e-8:\n",
        "                print(f\"[EP] Outer iter {outer_iter+1}: \"\n",
        "                     f\"Bound increased from {prev_bound:.3f} to {new_bound:.3f}\")\n",
        "\n",
        "            # Early stopping if no improvement\n",
        "            if new_bound < prev_bound + 1e-8:\n",
        "                break\n",
        "\n",
        "            prev_bound = new_bound\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _pep_sweep(\n",
        "        self,\n",
        "        X,\n",
        "        y\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Perform one sweep of Power EP updates.\n",
        "\n",
        "        Args:\n",
        "            X: Feature matrix\n",
        "            y: Target vector\n",
        "\n",
        "        Returns:\n",
        "            Maximum parameter change in this sweep\n",
        "        \"\"\"\n",
        "        n, d = X.shape\n",
        "        max_delta = 0.0\n",
        "\n",
        "        for i in range(n):\n",
        "            xi, yi = X[i], y[i]\n",
        "            ti, ni = self.tau[i], self.nu[i]\n",
        "\n",
        "            # Remove alpha-powered site\n",
        "            ti_remove = self.alpha * ti\n",
        "            ni_remove = self.alpha * ni\n",
        "\n",
        "            # More stable cavity computation\n",
        "            Lam_cav = self.Lambda - ti_remove * np.outer(xi, xi)\n",
        "            eta_cav = self.eta - ni_remove * xi\n",
        "\n",
        "            # Ensure positive definite cavity\n",
        "            eigvals = np.linalg.eigvalsh(Lam_cav)\n",
        "            if np.min(eigvals) < JITTER:\n",
        "                Lam_cav = Lam_cav + (JITTER - np.min(eigvals)) * np.eye(d)\n",
        "\n",
        "            Sig_cav = robust_invert(Lam_cav)\n",
        "            mu_cav = Sig_cav.dot(eta_cav)\n",
        "\n",
        "            m_c = xi.dot(mu_cav)\n",
        "            v_c = xi.dot(Sig_cav.dot(xi))\n",
        "            v_c = max(v_c, JITTER)\n",
        "\n",
        "            # Compute tilted moments\n",
        "            E1, E2 = self.like.tilted_moments(\n",
        "                m_c, v_c, yi, self.gh_z, self.gh_w\n",
        "            )\n",
        "            if E1 is None:\n",
        "                continue\n",
        "\n",
        "            v_t = max(E2 - E1**2, JITTER)\n",
        "\n",
        "            # Update natural parameters\n",
        "            ti_hat = (1.0 / self.alpha) * (1.0 / v_t - 1.0 / v_c)\n",
        "            ni_hat = (1.0 / self.alpha) * (E1 / v_t - m_c / v_c)\n",
        "\n",
        "            # Ensure ti_hat is positive\n",
        "            ti_hat = max(ti_hat, JITTER)\n",
        "\n",
        "            dti = ti_hat - ti\n",
        "            dni = ni_hat - ni\n",
        "\n",
        "            # Adaptive damping based on change magnitude\n",
        "            change_mag = abs(dti) + abs(dni)\n",
        "            if change_mag > 10:\n",
        "                # Large change - use more damping\n",
        "                step = min(self.damping[i], 0.3)\n",
        "            else:\n",
        "                # Small change - can use less damping\n",
        "                step = self.damping[i]\n",
        "\n",
        "            # Apply update\n",
        "            self.tau[i] += step * dti\n",
        "            self.nu[i] += step * dni\n",
        "            self.Lambda += step * dti * np.outer(xi, xi)\n",
        "            self.eta += step * dni * xi\n",
        "\n",
        "            # Update site-specific damping\n",
        "            if change_mag < 0.1:\n",
        "                self.damping[i] = min(self.damping[i] * 1.1, 0.95)\n",
        "            elif change_mag > 1.0:\n",
        "                self.damping[i] = max(self.damping[i] * 0.9, 0.1)\n",
        "\n",
        "            max_delta = max(max_delta, abs(step * dti), abs(step * dni))\n",
        "\n",
        "        return max_delta\n",
        "\n",
        "    def _compute_bound(\n",
        "        self,\n",
        "        X,\n",
        "        y\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Compute variational lower bound (ELBO).\n",
        "\n",
        "        Args:\n",
        "            X: Feature matrix\n",
        "            y: Target vector\n",
        "\n",
        "        Returns:\n",
        "            Lower bound value\n",
        "        \"\"\"\n",
        "        n, d = X.shape\n",
        "        m, v = self.predict_latent(X)\n",
        "\n",
        "        # Prior term: -0.5 * (tr(K^{-1} * Sigma) + mu^T * K^{-1} * mu)\n",
        "        prior_term = (-0.5 * (np.trace(self.Lambda0.dot(self.Sigma)) +\n",
        "                             self.mu.dot(self.Lambda0.dot(self.mu))))\n",
        "\n",
        "        # Expected log likelihood\n",
        "        like_term = 0.0\n",
        "        for i in range(n):\n",
        "            if isinstance(self.like, GaussianLikelihood):\n",
        "                # Closed form for Gaussian\n",
        "                like_term += -0.5 * np.log(2 * np.pi * self.like.obs_var)\n",
        "                like_term += (-0.5 * ((y[i] - m[i])**2 + v[i]) /\n",
        "                             self.like.obs_var)\n",
        "            elif isinstance(self.like, LogisticLikelihood):\n",
        "                # Use probit approximation\n",
        "                y_sign = 2 * y[i] - 1\n",
        "                m_adj = y_sign * m[i]\n",
        "                s = np.sqrt(v[i] + PROBIT_SCALE**2)\n",
        "                like_term += np.log(norm.cdf(m_adj / s))\n",
        "            else:\n",
        "                # Use quadrature for other likelihoods\n",
        "                a = m[i] + np.sqrt(2 * v[i]) * self.gh_z\n",
        "\n",
        "                if isinstance(self.like, PoissonLikelihood):\n",
        "                    log_phi = y[i] * a - np.exp(np.clip(a, -LOG_CLIP, LOG_CLIP))\n",
        "                elif isinstance(self.like, StudentTLikelihood):\n",
        "                    df, sc = self.like.df, self.like.scale\n",
        "                    log_phi = (-(df + 1) / 2 *\n",
        "                              np.log(1 + ((y[i] - a)**2) / (df * sc**2)))\n",
        "                elif isinstance(self.like, GammaLikelihood):\n",
        "                    log_phi = (self.like.shape * a -\n",
        "                              self.like.shape * y[i] *\n",
        "                              np.exp(-np.clip(a, -LOG_CLIP, LOG_CLIP)))\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                like_term += self.gh_w.dot(log_phi)\n",
        "\n",
        "        # Entropy term: 0.5 * log(abs(2*pi*e*Sigma))\n",
        "        sign, logdet = np.linalg.slogdet(2 * np.pi * np.e * self.Sigma)\n",
        "        entropy = 0.5 * logdet\n",
        "\n",
        "        return prior_term + like_term + entropy\n",
        "\n",
        "    def _update_posterior(self):\n",
        "        \"\"\"Update posterior mean and covariance.\"\"\"\n",
        "        # Add jitter for numerical stability\n",
        "        Lam = self.Lambda + JITTER * np.eye(self.Lambda.shape[0])\n",
        "        self.Sigma = robust_invert(Lam)\n",
        "        self.mu = self.Sigma.dot(self.eta)\n",
        "\n",
        "    def predict_latent(self, X):\n",
        "        \"\"\"\n",
        "        Predict latent function values.\n",
        "\n",
        "        Args:\n",
        "            X: Feature matrix for prediction\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (mean, variance) for latent function\n",
        "        \"\"\"\n",
        "        m = X.dot(self.mu)\n",
        "        v = np.sum(X.dot(self.Sigma) * X, axis=1)\n",
        "        return m, np.maximum(v, JITTER)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict expected outputs.\n",
        "\n",
        "        Args:\n",
        "            X: Feature matrix for prediction\n",
        "\n",
        "        Returns:\n",
        "            Expected output values\n",
        "        \"\"\"\n",
        "        m, v = self.predict_latent(X)\n",
        "        return self.like.predict(m, v)\n",
        "\n",
        "    def sample_predictive(\n",
        "        self,\n",
        "        X,\n",
        "        n_samples=1000,\n",
        "        seed=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Sample from predictive distribution.\n",
        "\n",
        "        Args:\n",
        "            X: Feature matrix for prediction\n",
        "            n_samples: Number of samples to draw\n",
        "            seed: Random seed for reproducibility\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (mean, (lower_quantile, upper_quantile))\n",
        "        \"\"\"\n",
        "        rng = np.random.RandomState(seed)\n",
        "\n",
        "        # Sample weights from posterior\n",
        "        L = np.linalg.cholesky(\n",
        "            self.Sigma + JITTER * np.eye(self.Sigma.shape[0])\n",
        "        )\n",
        "        z = rng.randn(self.Sigma.shape[0], n_samples)\n",
        "        w_samples = self.mu[:, None] + L.dot(z)\n",
        "\n",
        "        # Compute latent values\n",
        "        A = X.dot(w_samples)\n",
        "\n",
        "        # Sample observations\n",
        "        Y = self.like.sample(A, rng)\n",
        "\n",
        "        # Compute statistics\n",
        "        mean = Y.mean(axis=1)\n",
        "        lower = np.percentile(Y, 2.5, axis=1)\n",
        "        upper = np.percentile(Y, 97.5, axis=1)\n",
        "\n",
        "        return mean, (lower, upper)"
      ],
      "metadata": {
        "id": "GzyUj2mHv3H3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Factory"
      ],
      "metadata": {
        "id": "o2zc_nSjwyeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_power_ep(\n",
        "    family,\n",
        "    **kwargs\n",
        "):\n",
        "    \"\"\"\n",
        "    Create Power EP model with specified likelihood family.\n",
        "\n",
        "    Args:\n",
        "        family: Likelihood family name\n",
        "        **kwargs: Additional parameters for likelihood and EP\n",
        "\n",
        "    Returns:\n",
        "        Configured PowerExpectationPropagation instance\n",
        "    \"\"\"\n",
        "    fam = family.lower()\n",
        "    if fam == \"gaussian\":\n",
        "        like = GaussianLikelihood(kwargs.pop(\"obs_var\", 1.0))\n",
        "    elif fam == \"poisson\":\n",
        "        like = PoissonLikelihood()\n",
        "    elif fam == \"logistic\":\n",
        "        like = LogisticLikelihood()\n",
        "    elif fam == \"studentt\":\n",
        "        like = StudentTLikelihood(\n",
        "            kwargs.pop(\"df\", 4.0),\n",
        "            kwargs.pop(\"scale\", 1.0)\n",
        "        )\n",
        "    elif fam == \"gamma\":\n",
        "        like = GammaLikelihood(kwargs.pop(\"shape\", 2.0))\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown family: {family}\")\n",
        "\n",
        "    return PowerExpectationPropagation(likelihood=like, **kwargs)"
      ],
      "metadata": {
        "id": "DqYKJJrZwyb5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Generators"
      ],
      "metadata": {
        "id": "KTK0m7SFwyZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(\n",
        "    n,\n",
        "    frac=0.7,\n",
        "    seed=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Split data indices into train and test sets.\n",
        "\n",
        "    Args:\n",
        "        n: Total number of samples\n",
        "        frac: Fraction for training set\n",
        "        seed: Random seed\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (train_indices, test_indices)\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    idx = rng.permutation(n)\n",
        "    cut = int(frac * n)\n",
        "    return idx[:cut], idx[cut:]"
      ],
      "metadata": {
        "id": "fXEGmjD-w_jd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_logistic(\n",
        "    n,\n",
        "    d,\n",
        "    w_scale=1.0,\n",
        "    seed=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Simulate logistic regression data.\n",
        "\n",
        "    Args:\n",
        "        n: Number of samples\n",
        "        d: Number of features\n",
        "        w_scale: Scale of true weights\n",
        "        seed: Random seed\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (X, y) where X is features and y is binary labels\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    X = rng.randn(n, d)\n",
        "    w = rng.randn(d) * w_scale\n",
        "    y = (rng.rand(n) < expit(X.dot(w))).astype(int)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "RYqiK7iCw_hH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_poisson(\n",
        "    n,\n",
        "    d,\n",
        "    w_scale=0.5,\n",
        "    seed=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Simulate Poisson regression data.\n",
        "\n",
        "    Args:\n",
        "        n: Number of samples\n",
        "        d: Number of features\n",
        "        w_scale: Scale of true weights\n",
        "        seed: Random seed\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (X, y) where X is features and y is count data\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    X = rng.randn(n, d)\n",
        "    w = rng.randn(d) * w_scale\n",
        "    lam = np.exp(X.dot(w))\n",
        "    y = rng.poisson(lam)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "8fa6OhWhw_ey"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_student_t(\n",
        "    n,\n",
        "    d,\n",
        "    df=4,\n",
        "    w_scale=1.0,\n",
        "    seed=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Simulate Student-t regression data.\n",
        "\n",
        "    Args:\n",
        "        n: Number of samples\n",
        "        d: Number of features\n",
        "        df: Degrees of freedom for t-distribution\n",
        "        w_scale: Scale of true weights\n",
        "        seed: Random seed\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (X, y) where X is features and y is continuous data\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    X = rng.randn(n, d)\n",
        "    w = rng.randn(d) * w_scale\n",
        "    loc = X.dot(w)\n",
        "    y = loc + rng.standard_t(df, size=n)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "VCedvjwew_cL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_gaussian(\n",
        "    n,\n",
        "    d,\n",
        "    noise_sd=0.5,\n",
        "    w_scale=1.0,\n",
        "    seed=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Simulate Gaussian regression data.\n",
        "\n",
        "    Args:\n",
        "        n: Number of samples\n",
        "        d: Number of features\n",
        "        noise_sd: Standard deviation of observation noise\n",
        "        w_scale: Scale of true weights\n",
        "        seed: Random seed\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (X, y) where X is features and y is continuous data\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    X = rng.randn(n, d)\n",
        "    w = rng.randn(d) * w_scale\n",
        "    y = X.dot(w) + rng.randn(n) * noise_sd\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "hIwhA_8Qw_Zn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_gamma(\n",
        "    n,\n",
        "    d,\n",
        "    shape=2.0,\n",
        "    w_scale=0.5,\n",
        "    seed=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate Gamma distributed data with log link.\n",
        "\n",
        "    Args:\n",
        "        n: Number of samples\n",
        "        d: Number of features\n",
        "        shape: Shape parameter of Gamma distribution\n",
        "        w_scale: Scale of true weights\n",
        "        seed: Random seed\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (X, y) where X is features and y is positive continuous data\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    X = rng.randn(n, d)\n",
        "    w = rng.randn(d) * w_scale\n",
        "    # Log link: E[Y|X] = exp(X'w)\n",
        "    scale = np.exp(X.dot(w))\n",
        "    y = rng.gamma(shape, scale, size=n)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "ikoHafM5xIxk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration"
      ],
      "metadata": {
        "id": "Et2e-xMKxIvg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3CCOUrrAs3Pb"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Run demonstration of Power Expectation Propagation on various datasets.\"\"\"\n",
        "    np.random.seed(42)\n",
        "    n, d = 500, 10\n",
        "\n",
        "    print(\"=== Running Double-Loop Power EP Demo ===\\n\")\n",
        "\n",
        "    # 1. Logistic Regression\n",
        "    print(\"1. Logistic Regression\")\n",
        "    X, y = simulate_logistic(n, d, seed=1)\n",
        "    i_tr, i_te = train_test_split(n, 0.7, seed=2)\n",
        "\n",
        "    m_log = make_power_ep(\n",
        "        \"logistic\",\n",
        "        prior_var=10.0,\n",
        "        power_fraction=0.7,\n",
        "        damping=0.5,\n",
        "        max_iter=100,\n",
        "        outer_max_iter=5,\n",
        "        tol=1e-6,\n",
        "        gh_points=20,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    m_log.fit(X[i_tr], y[i_tr])\n",
        "    p_te = m_log.predict(X[i_te])\n",
        "    acc_log = accuracy_score(y[i_te], (p_te >= 0.5).astype(int))\n",
        "    print(f\"Test Accuracy: {acc_log:.3f}\\n\")\n",
        "\n",
        "    # 2. Poisson Regression\n",
        "    print(\"2. Poisson Regression\")\n",
        "    X, y = simulate_poisson(n, d, seed=3)\n",
        "    i_tr, i_te = train_test_split(n, 0.7, seed=4)\n",
        "\n",
        "    m_poi = make_power_ep(\n",
        "        \"poisson\",\n",
        "        prior_var=1.0,\n",
        "        power_fraction=0.6,\n",
        "        damping=0.7,\n",
        "        max_iter=100,\n",
        "        outer_max_iter=5,\n",
        "        tol=1e-6,\n",
        "        gh_points=20\n",
        "    )\n",
        "\n",
        "    m_poi.fit(X[i_tr], y[i_tr])\n",
        "    mean_poi, (l_poi, h_poi) = m_poi.sample_predictive(\n",
        "        X[i_te], n_samples=2000, seed=5\n",
        "    )\n",
        "    rmse_poi = np.sqrt(mean_squared_error(y[i_te], mean_poi))\n",
        "    cov_poi = np.mean((y[i_te] >= l_poi) & (y[i_te] <= h_poi))\n",
        "    print(f\"Test RMSE: {rmse_poi:.3f}, Coverage: {cov_poi:.3f}\\n\")\n",
        "\n",
        "    # 3. Student-t Regression\n",
        "    print(\"3. Student-t Regression\")\n",
        "    X, y = simulate_student_t(n, d, seed=6)\n",
        "    i_tr, i_te = train_test_split(n, 0.7, seed=7)\n",
        "\n",
        "    m_stu = make_power_ep(\n",
        "        \"studentt\",\n",
        "        df=4,\n",
        "        scale=1.0,\n",
        "        prior_var=1.0,\n",
        "        power_fraction=0.8,\n",
        "        damping=0.7,\n",
        "        max_iter=100,\n",
        "        outer_max_iter=5,\n",
        "        tol=1e-6,\n",
        "        gh_points=20\n",
        "    )\n",
        "\n",
        "    m_stu.fit(X[i_tr], y[i_tr])\n",
        "    mean_stu, (l_stu, h_stu) = m_stu.sample_predictive(\n",
        "        X[i_te], n_samples=2000, seed=8\n",
        "    )\n",
        "    rmse_stu = np.sqrt(mean_squared_error(y[i_te], mean_stu))\n",
        "    cov_stu = np.mean((y[i_te] >= l_stu) & (y[i_te] <= h_stu))\n",
        "    print(f\"Test RMSE: {rmse_stu:.3f}, Coverage: {cov_stu:.3f}\\n\")\n",
        "\n",
        "    # 4. Gaussian Regression\n",
        "    print(\"4. Gaussian Regression\")\n",
        "    X, y = simulate_gaussian(n, d, seed=9)\n",
        "    i_tr, i_te = train_test_split(n, 0.7, seed=10)\n",
        "\n",
        "    m_gau = make_power_ep(\n",
        "        \"gaussian\",\n",
        "        obs_var=0.25,\n",
        "        prior_var=1.0,\n",
        "        power_fraction=0.9,\n",
        "        damping=0.7,\n",
        "        max_iter=100,\n",
        "        outer_max_iter=5,\n",
        "        tol=1e-6,\n",
        "        gh_points=20\n",
        "    )\n",
        "\n",
        "    m_gau.fit(X[i_tr], y[i_tr])\n",
        "    mean_gau, (l_gau, h_gau) = m_gau.sample_predictive(\n",
        "        X[i_te], n_samples=2000, seed=11\n",
        "    )\n",
        "    rmse_gau = np.sqrt(mean_squared_error(y[i_te], mean_gau))\n",
        "    cov_gau = np.mean((y[i_te] >= l_gau) & (y[i_te] <= h_gau))\n",
        "    print(f\"Test RMSE: {rmse_gau:.3f}, Coverage: {cov_gau:.3f}\\n\")\n",
        "\n",
        "    # 5. Gamma Regression\n",
        "    print(\"5. Gamma Regression\")\n",
        "    X, y = simulate_gamma(n, d, shape=2.0, seed=12)\n",
        "    i_tr, i_te = train_test_split(n, 0.7, seed=13)\n",
        "\n",
        "    m_gam = make_power_ep(\n",
        "        \"gamma\",\n",
        "        shape=2.0,\n",
        "        prior_var=1.0,\n",
        "        power_fraction=0.5,\n",
        "        damping=0.5,\n",
        "        max_iter=200,\n",
        "        outer_max_iter=10,\n",
        "        tol=1e-6,\n",
        "        gh_points=20,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    m_gam.fit(X[i_tr], y[i_tr])\n",
        "    mean_gam, (l_gam, h_gam) = m_gam.sample_predictive(\n",
        "        X[i_te], n_samples=2000, seed=14\n",
        "    )\n",
        "    rmse_gam = np.sqrt(mean_squared_error(y[i_te], mean_gam))\n",
        "    cov_gam = np.mean((y[i_te] >= l_gam) & (y[i_te] <= h_gam))\n",
        "    print(f\"Test RMSE: {rmse_gam:.3f}, Coverage: {cov_gam:.3f}\\n\")\n",
        "\n",
        "    # Summary Report\n",
        "    print(\"\\n=== EP Demo Summary Results ===\")\n",
        "    print(f\"Logistic   Accuracy: {acc_log:.3f}\")\n",
        "    print(f\"Poisson    RMSE: {rmse_poi:.3f}, Coverage: {cov_poi:.3f}\")\n",
        "    print(f\"Student-t  RMSE: {rmse_stu:.3f}, Coverage: {cov_stu:.3f}\")\n",
        "    print(f\"Gaussian   RMSE: {rmse_gau:.3f}, Coverage: {cov_gau:.3f}\")\n",
        "    print(f\"Gamma      RMSE: {rmse_gam:.3f}, Coverage: {cov_gam:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6elQVcjCxl9k",
        "outputId": "4b3011fa-3998-4601-baed-bc23211f6e0f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Running Double-Loop Power EP Demo ===\n",
            "\n",
            "1. Logistic Regression\n",
            "[EP] Outer iter 1: Bound increased from -inf to -136.016\n",
            "Test Accuracy: 0.820\n",
            "\n",
            "2. Poisson Regression\n",
            "Test RMSE: 5.930, Coverage: 0.967\n",
            "\n",
            "3. Student-t Regression\n",
            "Test RMSE: 1.414, Coverage: 0.947\n",
            "\n",
            "4. Gaussian Regression\n",
            "Test RMSE: 0.495, Coverage: 0.947\n",
            "\n",
            "5. Gamma Regression\n",
            "[EP] Outer iter 1: Bound increased from -inf to -1279.743\n",
            "[EP] Outer iter 2: Bound increased from -1279.743 to -1278.861\n",
            "Test RMSE: 4.786, Coverage: 0.900\n",
            "\n",
            "\n",
            "=== EP Demo Summary Results ===\n",
            "Logistic   Accuracy: 0.820\n",
            "Poisson    RMSE: 5.930, Coverage: 0.967\n",
            "Student-t  RMSE: 1.414, Coverage: 0.947\n",
            "Gaussian   RMSE: 0.495, Coverage: 0.947\n",
            "Gamma      RMSE: 4.786, Coverage: 0.900\n"
          ]
        }
      ]
    }
  ]
}