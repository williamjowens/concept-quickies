{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tBBoKENvn4RA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from math import lgamma\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.gam.api import GLMGam, BSplines\n",
        "from statsmodels.genmod.families import Poisson, Binomial, NegativeBinomial\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class ZIGAMEstimator(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, dist='nb', df=None, degree=3, alpha=None, max_iter=100, tol=1e-6):\n",
        "        \"\"\"\n",
        "        Zero-Inflated Generalized Additive Model (ZI-GAM) Estimator.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dist : str\n",
        "            Distribution for the count process: 'poisson' or 'nb' (negative binomial).\n",
        "            Negative binomial ('nb') is the default.\n",
        "        df : int or list, optional\n",
        "            Degrees of freedom (number of spline basis functions) per feature.\n",
        "            If None, a default is chosen based on sample size.\n",
        "        degree : int or list, optional\n",
        "            Degree(s) of the B-spline basis (default is 3 for cubic splines).\n",
        "        alpha : float or list, optional\n",
        "            Smoothing penalty for the spline terms. If None, a default value is used.\n",
        "        max_iter : int\n",
        "            Maximum number of EM iterations.\n",
        "        tol : float\n",
        "            Convergence tolerance on the change in log-likelihood.\n",
        "        \"\"\"\n",
        "        self.dist = dist.lower()\n",
        "        self.df = df\n",
        "        self.degree = degree\n",
        "        self.alpha = alpha\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "\n",
        "        # Attributes set during fitting\n",
        "        self._bs_count = None      # Spline basis for count component\n",
        "        self._bs_infl = None       # Spline basis for inflation component\n",
        "        self.logistic_res_ = None  # Fitted GLMGam result for logistic (structural zero) model\n",
        "        self.count_res_ = None     # Fitted GLMGam result for count model\n",
        "        self.dispersion_ = None    # Fitted dispersion parameter (only for NB)\n",
        "        self.n_iter_ = 0           # Number of EM iterations performed\n",
        "        self.loglikelihood_ = None # Final log-likelihood value\n",
        "        self._r = None             # Internal storage of responsibilities (P(structural zero))\n",
        "\n",
        "    def _initialize_basis(self, X):\n",
        "        \"\"\"Create B-spline basis objects for both count and inflation components.\"\"\"\n",
        "        X = np.asarray(X)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Determine degrees of freedom per feature\n",
        "        if self.df is None:\n",
        "            df = [min(10, max(4, n_samples // 5)) for _ in range(n_features)]\n",
        "        elif isinstance(self.df, (int, float)):\n",
        "            df = [int(self.df)] * n_features\n",
        "        else:\n",
        "            df = list(self.df)\n",
        "            if len(df) != n_features:\n",
        "                raise ValueError(\"Length of df list must equal number of features.\")\n",
        "\n",
        "        # Determine spline degree per feature\n",
        "        if isinstance(self.degree, (int, float)):\n",
        "            degree = [int(self.degree)] * n_features\n",
        "        else:\n",
        "            degree = list(self.degree)\n",
        "            if len(degree) != n_features:\n",
        "                raise ValueError(\"Length of degree list must equal number of features.\")\n",
        "\n",
        "        # Create BSplines objects (both components use the same basis)\n",
        "        self._bs_count = BSplines(X, df=df, degree=degree, include_intercept=False)\n",
        "        self._bs_infl = BSplines(X, df=df, degree=degree, include_intercept=False)\n",
        "\n",
        "    def _initialize_parameters(self, X, y):\n",
        "        \"\"\"\n",
        "        Perform an initial fit to obtain a starting value for the EM algorithm.\n",
        "\n",
        "        This involves:\n",
        "          - Fitting an initial Poisson GAM (ignoring zero inflation) to obtain a baseline μ.\n",
        "          - Estimating the overall excess zero rate.\n",
        "          - Initializing the responsibilities r (P(structural zero | y = 0)).\n",
        "        \"\"\"\n",
        "        n_samples = len(y)\n",
        "        init_exog = sm.add_constant(np.zeros((n_samples, 1)))\n",
        "\n",
        "        try:\n",
        "            init_model = GLMGam(\n",
        "                y,\n",
        "                exog=init_exog,\n",
        "                smoother=self._bs_count,\n",
        "                alpha=np.zeros(len(self._bs_count.penalty_matrices)),\n",
        "                family=Poisson()\n",
        "            )\n",
        "        except Exception:\n",
        "            init_model = GLMGam(\n",
        "                y,\n",
        "                exog=init_exog,\n",
        "                smoother=self._bs_count,\n",
        "                alpha=np.full(len(self._bs_count.penalty_matrices), 1e-6),\n",
        "                family=Poisson()\n",
        "            )\n",
        "\n",
        "        init_res = init_model.fit()\n",
        "        mu_init = init_res.predict()\n",
        "        p_zero_pois = np.exp(-mu_init)\n",
        "        p_zero_obs = np.mean(y == 0)\n",
        "        avg_p_zero_pois = np.mean(p_zero_pois)\n",
        "\n",
        "        if p_zero_obs > avg_p_zero_pois:\n",
        "            pi_init = (p_zero_obs - avg_p_zero_pois) / (1 - avg_p_zero_pois)\n",
        "        else:\n",
        "            pi_init = 0.0\n",
        "\n",
        "        pi_init = np.clip(pi_init, 0.0, 0.99)\n",
        "        r = np.zeros(n_samples)\n",
        "        r[y == 0] = pi_init\n",
        "        r[y > 0] = 0.0\n",
        "        self._r = r.copy()\n",
        "        return mu_init\n",
        "\n",
        "    def _e_step(self, X, y):\n",
        "        \"\"\"\n",
        "        E-step: Update responsibilities for zero observations.\n",
        "\n",
        "        For each y_i == 0, compute:\n",
        "            r_i = π(X_i) / [π(X_i) + (1-π(X_i)) * P_count(0 | μ(X_i))].\n",
        "        \"\"\"\n",
        "        n_samples = len(y)\n",
        "        pi_pred = self.logistic_res_.predict()\n",
        "        mu_pred = self.count_res_.predict()\n",
        "\n",
        "        if self.dist.startswith('nb'):\n",
        "            p0_count = np.power(1.0 + self.dispersion_ * mu_pred, -1.0 / self.dispersion_)\n",
        "        else:\n",
        "            p0_count = np.exp(-mu_pred)\n",
        "\n",
        "        r_new = np.zeros(n_samples)\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            if y[i] == 0:\n",
        "                denom = pi_pred[i] + (1 - pi_pred[i]) * p0_count[i]\n",
        "                r_new[i] = pi_pred[i] / denom if denom > 0 else 0.0\n",
        "            else:\n",
        "                r_new[i] = 0.0\n",
        "        self._r = r_new.copy()\n",
        "\n",
        "    def _fit_logistic(self, X, r):\n",
        "        \"\"\"\n",
        "        M-step: Fit the logistic GAM for the structural zero (inflation) component.\n",
        "\n",
        "        This uses a Binomial family with a logit link.\n",
        "        \"\"\"\n",
        "        n_samples = len(r)\n",
        "        Exog = sm.add_constant(np.zeros((n_samples, 1)))\n",
        "        logistic_model = GLMGam(\n",
        "            r,\n",
        "            exog=Exog,\n",
        "            smoother=self._bs_infl,\n",
        "            alpha=np.ones(len(self._bs_infl.penalty_matrices)) * 1.0,\n",
        "            family=Binomial()\n",
        "        )\n",
        "        logistic_res = logistic_model.fit()\n",
        "        return logistic_res\n",
        "\n",
        "    def _fit_count(self, X, y, r):\n",
        "        \"\"\"\n",
        "        M-step: Fit the GAM for the count component using weights (1 - r).\n",
        "\n",
        "        For negative binomial, the current dispersion parameter is used.\n",
        "        \"\"\"\n",
        "        n_samples = len(y)\n",
        "        Exog = sm.add_constant(np.zeros((n_samples, 1)))\n",
        "        weights = 1 - r\n",
        "\n",
        "        if self.dist.startswith('nb'):\n",
        "            nb_family = NegativeBinomial(alpha=self.dispersion_ if self.dispersion_ is not None else 1.0)\n",
        "            count_model = GLMGam(\n",
        "                y,\n",
        "                exog=Exog,\n",
        "                smoother=self._bs_count,\n",
        "                alpha=np.ones(len(self._bs_count.penalty_matrices)) * 1.0,\n",
        "                family=nb_family\n",
        "            )\n",
        "        else:\n",
        "            count_model = GLMGam(\n",
        "                y,\n",
        "                exog=Exog,\n",
        "                smoother=self._bs_count,\n",
        "                alpha=np.ones(len(self._bs_count.penalty_matrices)) * 1.0,\n",
        "                family=Poisson()\n",
        "            )\n",
        "\n",
        "        count_res = count_model.fit(weights=weights)\n",
        "        return count_res\n",
        "\n",
        "    def _update_dispersion(self, y, mu_pred, weights):\n",
        "        \"\"\"\n",
        "        Update the dispersion parameter for the negative binomial distribution using a Pearson estimator.\n",
        "        \"\"\"\n",
        "        resid = y - mu_pred\n",
        "        num = np.sum(weights * ((resid**2 / np.maximum(mu_pred, 1e-8)) - 1))\n",
        "        den = np.sum(weights * mu_pred)\n",
        "        new_dispersion = max(1e-8, num / np.maximum(den, 1e-8))\n",
        "        return new_dispersion\n",
        "\n",
        "    def _compute_loglik(self, y, logistic_res, count_res, dispersion):\n",
        "        \"\"\"\n",
        "        Compute the log-likelihood for the mixture model.\n",
        "        \"\"\"\n",
        "        n_samples = len(y)\n",
        "        pi_pred = logistic_res.predict()\n",
        "        mu_pred = count_res.predict()\n",
        "        log_pmf = np.zeros(n_samples)\n",
        "\n",
        "        if self.dist.startswith('nb'):\n",
        "            size = 1.0 / dispersion\n",
        "            for i in range(n_samples):\n",
        "                y_i = int(y[i])\n",
        "                if y_i == 0:\n",
        "                    log_pmf[i] = size * np.log(size / (size + mu_pred[i]))\n",
        "                else:\n",
        "                    log_comb = lgamma(y_i + size) - lgamma(size) - lgamma(y_i + 1)\n",
        "                    log_p = size * np.log(size / (size + mu_pred[i]))\n",
        "                    log_q = y_i * np.log(mu_pred[i] / (size + mu_pred[i]))\n",
        "                    log_pmf[i] = log_comb + log_p + log_q\n",
        "        else:\n",
        "            for i in range(n_samples):\n",
        "                y_i = int(y[i])\n",
        "                log_pmf[i] = -mu_pred[i] + y_i * (np.log(mu_pred[i]) if mu_pred[i] > 0 else -np.inf) - lgamma(y_i + 1)\n",
        "\n",
        "        pmf = np.exp(log_pmf)\n",
        "        lik = np.where(y == 0, pi_pred + (1 - pi_pred) * pmf, (1 - pi_pred) * pmf)\n",
        "        loglik = np.sum(np.log(np.clip(lik, 1e-12, None)))\n",
        "        return loglik\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the Zero-Inflated GAM model using an EM algorithm.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Predictor variables.\n",
        "        y : array-like, shape (n_samples,)\n",
        "            Count response variable.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            The fitted model.\n",
        "        \"\"\"\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize spline bases\n",
        "        self._initialize_basis(X)\n",
        "        # Initialize parameters and responsibilities\n",
        "        self._initialize_parameters(X, y)\n",
        "        # Initialize dispersion if using Negative Binomial\n",
        "        if self.dist.startswith('nb'):\n",
        "            self.dispersion_ = 1.0\n",
        "        else:\n",
        "            self.dispersion_ = 0.0\n",
        "\n",
        "        loglik_old = -np.inf\n",
        "        start_time = time.time()\n",
        "        print(\"Starting EM iterations...\")\n",
        "        pbar = tqdm(total=self.max_iter, desc=\"EM iterations\", ncols=80)\n",
        "\n",
        "        for iteration in range(self.max_iter):\n",
        "            # M-step: fit logistic and count GAMs\n",
        "            self.logistic_res_ = self._fit_logistic(X, self._r)\n",
        "            self.count_res_ = self._fit_count(X, y, self._r)\n",
        "            # E-step: update responsibilities\n",
        "            self._e_step(X, y)\n",
        "            # Update dispersion for NB\n",
        "            if self.dist.startswith('nb'):\n",
        "                mu_pred = self.count_res_.predict()\n",
        "                weights = 1 - self._r\n",
        "                self.dispersion_ = self._update_dispersion(y, mu_pred, weights)\n",
        "            # Compute log-likelihood\n",
        "            loglik = self._compute_loglik(y, self.logistic_res_, self.count_res_, self.dispersion_)\n",
        "            elapsed = time.time() - start_time\n",
        "            pbar.set_description(f\"Iter {iteration+1:3d}: logL = {loglik:.4f}, {elapsed:.2f}s\")\n",
        "            pbar.update(1)\n",
        "            if loglik - loglik_old < self.tol:\n",
        "                pbar.set_description(\"Convergence reached.\")\n",
        "                break\n",
        "            loglik_old = loglik\n",
        "\n",
        "        pbar.close()\n",
        "        self.n_iter_ = iteration + 1\n",
        "        self.loglikelihood_ = loglik_old\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"EM algorithm converged after {self.n_iter_} iterations in {total_time:.2f} seconds.\")\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the expected count for each observation using\n",
        "        E[Y] = (1 - π(X)) * μ(X).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred : array, shape (n_samples,)\n",
        "            Predicted expected counts.\n",
        "        \"\"\"\n",
        "        X = np.asarray(X)\n",
        "        n_samples = X.shape[0]\n",
        "        # For new data, construct the exogenous variable as in training\n",
        "        new_exog = sm.add_constant(np.zeros((n_samples, 1)))\n",
        "        # Use the smoother part with new data via exog_smooth\n",
        "        pi_pred = self.logistic_res_.predict(exog=new_exog, exog_smooth=X)\n",
        "        mu_pred = self.count_res_.predict(exog=new_exog, exog_smooth=X)\n",
        "        return (1 - pi_pred) * mu_pred\n",
        "\n",
        "    def score(self, X, y):\n",
        "        \"\"\"\n",
        "        Compute the average log-likelihood per observation on the provided data.\n",
        "\n",
        "        This likelihood-based score is more appropriate for count models,\n",
        "        including zero-inflated models, because it measures the probability\n",
        "        of observing the data given the fitted model parameters.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Predictor variables.\n",
        "        y : array-like, shape (n_samples,)\n",
        "            Observed counts.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        score : float\n",
        "            Average log-likelihood per observation. Higher values (i.e. less negative)\n",
        "            indicate a better fit.\n",
        "        \"\"\"\n",
        "        y = np.asarray(y)\n",
        "        # Compute total log-likelihood on the provided data using the current parameters\n",
        "        total_loglik = self._compute_loglik(y, self.logistic_res_, self.count_res_, self.dispersion_)\n",
        "        # Return average log likelihood per sample\n",
        "        return total_loglik / len(y)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        \"\"\"\n",
        "        Return the parameters of the estimator.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        deep : bool, default=True\n",
        "            If True, will return the parameters for this estimator and contained subobjects.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        params : dict\n",
        "            Parameter names mapped to their values.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'dist': self.dist,\n",
        "            'df': self.df,\n",
        "            'degree': self.degree,\n",
        "            'alpha': self.alpha,\n",
        "            'max_iter': self.max_iter,\n",
        "            'tol': self.tol\n",
        "        }\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        \"\"\"\n",
        "        Set the parameters of the estimator.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "            Returns self.\n",
        "        \"\"\"\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset(n_samples=500, seed=42):\n",
        "    \"\"\"\n",
        "    Generate a synthetic zero-inflated count dataset with one predictor.\n",
        "\n",
        "    The count component follows a negative binomial distribution with true mean\n",
        "    μ(X) = exp(1.5 + 0.5*sin(X)) and the structural zero probability is given by\n",
        "    π(X) = logistic(-1.0 + 0.5*cos(2*X)).\n",
        "\n",
        "    The negative binomial sampling is performed using:\n",
        "        r = 1/alpha (with alpha set to 0.5) and p = r / (r + μ).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_samples : int\n",
        "        Number of samples to generate.\n",
        "    seed : int\n",
        "        Random seed.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X : array-like, shape (n_samples, 1)\n",
        "        The predictor values.\n",
        "    y : array-like, shape (n_samples,)\n",
        "        The generated counts.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    X = np.linspace(0, 10, n_samples)[:, None]\n",
        "    f_count = 1.5 + 0.5 * np.sin(X).ravel()     # affects log(μ)\n",
        "    mu_true = np.exp(f_count)\n",
        "    f_infl = -1.0 + 0.5 * np.cos(2 * X).ravel()   # affects logit(π)\n",
        "    pi_true = 1 / (1 + np.exp(-f_infl))\n",
        "\n",
        "    alpha_true = 0.5      # dispersion parameter\n",
        "    r = 1.0 / alpha_true  # number of successes\n",
        "    y = np.empty(n_samples, dtype=int)\n",
        "    for i in range(n_samples):\n",
        "        if np.random.rand() < pi_true[i]:\n",
        "            y[i] = 0\n",
        "        else:\n",
        "            p = r / (r + mu_true[i])\n",
        "            y[i] = np.random.negative_binomial(r, p)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "a5DRfijwoB0j"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the dataset\n",
        "X, y = generate_dataset(n_samples=10000)"
      ],
      "metadata": {
        "id": "TVMo-o9hoBx-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate and fit the ZIGAMEstimator\n",
        "model = ZIGAMEstimator(dist='nb', max_iter=100, tol=1e-4)\n",
        "model.fit(X, y)\n",
        "print(f\"EM iterations: {model.n_iter_}\")\n",
        "print(f\"Final log-likelihood: {model.loglikelihood_:.2f}\")\n",
        "print(\"Average log-likelihood:\", model.score(X, y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU1Z67UAoBvX",
        "outputId": "c11a158c-6dc9-4506-d41b-83a681d4b696"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting EM iterations...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Convergence reached.:  15%|███                 | 15/100 [00:19<01:53,  1.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EM algorithm converged after 15 iterations in 19.99 seconds.\n",
            "EM iterations: 15\n",
            "Final log-likelihood: -23288.97\n",
            "Average log-likelihood: -2.328898932863436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on new data\n",
        "X_new = np.linspace(0, 10, 5)[:, None]\n",
        "y_pred = model.predict(X_new)\n",
        "\n",
        "print(\"Predicted expected counts for new data:\")\n",
        "for X_i, y_i in zip(X_new.ravel(), y_pred):\n",
        "    print(f\"X = {X_i:.1f} --> E[Y] = {y_i:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoGeUjKLoBsg",
        "outputId": "8bf668f2-0bd4-4a79-c01f-8e7c6175061c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted expected counts for new data:\n",
            "X = 0.0 --> E[Y] = 2.76\n",
            "X = 2.5 --> E[Y] = 4.27\n",
            "X = 5.0 --> E[Y] = 2.10\n",
            "X = 7.5 --> E[Y] = 5.30\n",
            "X = 10.0 --> E[Y] = 2.16\n"
          ]
        }
      ]
    }
  ]
}