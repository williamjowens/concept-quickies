{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TBmvBeMYL3yk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import null_space, lstsq\n",
        "\n",
        "class GeneralizedReducedGradient:\n",
        "    \"\"\"\n",
        "    A class to perform optimization using the Generalized Reduced Gradient (GRG) algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, objective, eq_constraints, ineq_constraints=None, bounds=None,\n",
        "                 objective_grad=None, eq_constraints_jac=None, ineq_constraints_jac=None,\n",
        "                 tol=1e-6, max_iter=100, verbose=True):\n",
        "        \"\"\"\n",
        "        Initialize the GRG Algorithm.\n",
        "\n",
        "        Args:\n",
        "            objective (function): Objective function f(x) to minimize.\n",
        "            eq_constraints (list of functions): Equality constraints g_i(x) == 0.\n",
        "            ineq_constraints (list of functions, optional): Inequality constraints h_i(x) <= 0.\n",
        "            bounds (list of tuples, optional): Variable bounds as (min, max) for each variable.\n",
        "            objective_grad (function, optional): Gradient of the objective function.\n",
        "            eq_constraints_jac (list of functions, optional): Gradients of equality constraints.\n",
        "            ineq_constraints_jac (list of functions, optional): Gradients of inequality constraints.\n",
        "            tol (float, optional): Tolerance for constraint satisfaction and convergence.\n",
        "            max_iter (int, optional): Maximum number of iterations.\n",
        "            verbose (bool, optional): If True, print debugging information. Defaults to True.\n",
        "        \"\"\"\n",
        "        self.objective = objective\n",
        "        self.eq_constraints = eq_constraints.copy()\n",
        "        self.ineq_constraints = ineq_constraints.copy() if ineq_constraints else []\n",
        "        self.original_bounds = bounds.copy() if bounds else []\n",
        "        self.bounds = bounds.copy() if bounds else []\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Analytical gradients and Jacobians\n",
        "        self.objective_grad = objective_grad\n",
        "        self.eq_constraints_jac = eq_constraints_jac.copy() if eq_constraints_jac else []\n",
        "        self.ineq_constraints_jac = ineq_constraints_jac.copy() if ineq_constraints_jac else []\n",
        "\n",
        "        # Slack variable management\n",
        "        self.slack_vars = {}  # Mapping from inequality constraint index to slack variable index\n",
        "\n",
        "        # Scaling factors\n",
        "        self.var_scale = None\n",
        "        self.const_scale = None\n",
        "\n",
        "        # Number of original variables\n",
        "        self.num_original_vars = len(self.original_bounds) if self.original_bounds else 0\n",
        "\n",
        "    def _print(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Internal method to handle printing based on verbosity.\n",
        "\n",
        "        Args:\n",
        "            *args: Variable length argument list for print.\n",
        "            **kwargs: Arbitrary keyword arguments for print.\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            print(*args, **kwargs)\n",
        "\n",
        "    def compute_gradient(self, func_grad, x, func=None, eps=1e-8):\n",
        "        \"\"\"\n",
        "        Compute the gradient of a function at x.\n",
        "\n",
        "        Args:\n",
        "            func_grad (function or None): Analytical gradient function.\n",
        "            x (np.array): The point at which to compute the gradient.\n",
        "            func (function, optional): The function to compute the gradient for (used if func_grad is None).\n",
        "            eps (float, optional): A small perturbation for finite differences.\n",
        "\n",
        "        Returns:\n",
        "            np.array: The gradient vector.\n",
        "        \"\"\"\n",
        "        if func_grad:\n",
        "            try:\n",
        "                # Compute analytical gradient for original variables\n",
        "                grad_original = np.array(func_grad(x[:self.num_original_vars]))\n",
        "            except Exception as e:\n",
        "                self._print(f\"Analytical gradient computation error: {e}\")\n",
        "                if func:\n",
        "                    # Compute numerical gradient if analytical fails\n",
        "                    grad_original = self.compute_gradient(None, x[:self.num_original_vars], func=func, eps=eps)\n",
        "                else:\n",
        "                    # Default to zeros if no function provided\n",
        "                    grad_original = np.zeros(self.num_original_vars)\n",
        "        else:\n",
        "            if func is None:\n",
        "                raise ValueError(\"Function must be provided for numerical gradient computation.\")\n",
        "            # Compute numerical gradient for original variables\n",
        "            grad_original = np.zeros(self.num_original_vars)\n",
        "            for i in range(self.num_original_vars):\n",
        "                x_forward = np.copy(x)\n",
        "                x_backward = np.copy(x)\n",
        "                x_forward[i] += eps\n",
        "                x_backward[i] -= eps\n",
        "                try:\n",
        "                    grad_original[i] = (func(x_forward) - func(x_backward)) / (2 * eps)\n",
        "                except Exception as e:\n",
        "                    self._print(f\"Gradient computation error at index {i}: {e}\")\n",
        "                    grad_original[i] = 0.0\n",
        "\n",
        "        # Append zeros for slack variables (if any)\n",
        "        if len(x) > self.num_original_vars:\n",
        "            slack_grad = np.zeros(len(x) - self.num_original_vars)\n",
        "            grad = np.concatenate([grad_original, slack_grad])\n",
        "        else:\n",
        "            grad = grad_original\n",
        "\n",
        "        return grad\n",
        "\n",
        "    def compute_jacobian(self, active_constraints, x):\n",
        "        \"\"\"\n",
        "        Compute the Jacobian matrix for active constraints at x.\n",
        "\n",
        "        Args:\n",
        "            active_constraints (list of dicts): List of active constraints with details.\n",
        "            x (np.array): The point at which to compute the Jacobian.\n",
        "\n",
        "        Returns:\n",
        "            np.array: The Jacobian matrix.\n",
        "        \"\"\"\n",
        "        m = len(active_constraints)\n",
        "        n = len(x)\n",
        "        J = np.zeros((m, n))\n",
        "        eq_jac_idx = 0  # Index for equality Jacobians\n",
        "\n",
        "        self._print(f\"Computing Jacobian for {m} active constraints and {n} variables.\")\n",
        "\n",
        "        for i, constraint in enumerate(active_constraints):\n",
        "            if constraint['type'] == 'equality':\n",
        "                if self.eq_constraints_jac:\n",
        "                    # Use the corresponding Jacobian function\n",
        "                    if eq_jac_idx < len(self.eq_constraints_jac):\n",
        "                        J[i, :self.num_original_vars] = self.eq_constraints_jac[eq_jac_idx](x[:self.num_original_vars])\n",
        "                        self._print(f\"Constraint {i}: Equality Jacobian row: {J[i, :]}\")\n",
        "                        eq_jac_idx += 1\n",
        "                    else:\n",
        "                        # If Jacobian functions are fewer, fallback to numerical\n",
        "                        self._print(f\"Constraint {i}: Insufficient analytical Jacobians for equality constraints. Using numerical gradient.\")\n",
        "                        J[i, :self.num_original_vars] = self.compute_gradient(None, x, func=constraint['function'], eps=1e-8)[:self.num_original_vars]\n",
        "                else:\n",
        "                    # Compute numerical gradient if no analytical Jacobian provided\n",
        "                    self._print(f\"Constraint {i}: Equality Jacobian not provided. Using numerical gradient.\")\n",
        "                    J[i, :self.num_original_vars] = self.compute_gradient(None, x, func=constraint['function'], eps=1e-8)[:self.num_original_vars]\n",
        "            elif constraint['type'] == 'inequality':\n",
        "                ineq_idx = constraint['index']\n",
        "                if self.ineq_constraints_jac:\n",
        "                    if ineq_idx < len(self.ineq_constraints_jac):\n",
        "                        J[i, :self.num_original_vars] = self.ineq_constraints_jac[ineq_idx](x[:self.num_original_vars])\n",
        "                        self._print(f\"Constraint {i}: Inequality Jacobian row (original vars): {J[i, :self.num_original_vars]}\")\n",
        "                    else:\n",
        "                        self._print(f\"Constraint {i}: Insufficient analytical Jacobians for inequality constraints. Using numerical gradient.\")\n",
        "                        J[i, :self.num_original_vars] = self.compute_gradient(None, x, func=constraint['function'], eps=1e-8)[:self.num_original_vars]\n",
        "                else:\n",
        "                    # Compute numerical gradient if no analytical Jacobian provided\n",
        "                    self._print(f\"Constraint {i}: Inequality Jacobian not provided. Using numerical gradient.\")\n",
        "                    J[i, :self.num_original_vars] = self.compute_gradient(None, x, func=constraint['function'], eps=1e-8)[:self.num_original_vars]\n",
        "\n",
        "                # Set derivative with respect to slack variable to 1\n",
        "                slack_idx = constraint['slack_var_idx']\n",
        "                J[i, slack_idx] = 1.0\n",
        "                self._print(f\"Constraint {i}: Inequality Jacobian row after slack variable: {J[i, :]}\")\n",
        "\n",
        "        self._print(f\"Final Jacobian matrix:\\n{J}\")\n",
        "        return J\n",
        "\n",
        "    def scale_constraints_and_variables(self, x, active_constraints):\n",
        "        \"\"\"\n",
        "        Scale variables and constraints to improve numerical stability.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Current variable vector.\n",
        "            active_constraints (list of dicts): List of active constraints with details.\n",
        "\n",
        "        Sets:\n",
        "            self.var_scale: Scaling factors for variables.\n",
        "            self.const_scale: Scaling factors for constraints.\n",
        "        \"\"\"\n",
        "        # Variable scaling: scale variables to have maximum absolute value of 1\n",
        "        self.var_scale = np.ones_like(x)\n",
        "        for i in range(len(x)):\n",
        "            if x[i] != 0:\n",
        "                self.var_scale[i] = 1.0 / max(abs(x[i]), 1.0)\n",
        "            else:\n",
        "                self.var_scale[i] = 1.0\n",
        "\n",
        "        # Constraint scaling: scale constraints to have maximum absolute value of 1\n",
        "        num_constraints = len(active_constraints)\n",
        "        self.const_scale = np.ones(num_constraints)\n",
        "        for i in range(num_constraints):\n",
        "            # Estimate the scaling based on initial constraints evaluation\n",
        "            val = abs(active_constraints[i]['function'](x))\n",
        "            if val > 0:\n",
        "                self.const_scale[i] = 1.0 / max(val, 1.0)\n",
        "            else:\n",
        "                self.const_scale[i] = 1.0\n",
        "\n",
        "    def identify_active_constraints(self, x):\n",
        "        \"\"\"\n",
        "        Identify active equality and inequality constraints at point x.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): The current point.\n",
        "\n",
        "        Returns:\n",
        "            list of dicts: List of active constraints with details.\n",
        "        \"\"\"\n",
        "        active_constraints = []\n",
        "\n",
        "        # Add all equality constraints (they are always active)\n",
        "        for eq in self.eq_constraints:\n",
        "            active_constraints.append({'type': 'equality', 'function': eq, 'jacobian': None})\n",
        "\n",
        "        # Add active inequality constraints (those at their bounds)\n",
        "        for idx, ineq in enumerate(self.ineq_constraints):\n",
        "            if np.abs(ineq(x)) <= self.tol:\n",
        "                active_constraints.append({'type': 'inequality', 'function': ineq, 'jacobian': None, 'index': idx})\n",
        "\n",
        "        # Print active constraints\n",
        "        self._print(f\"Identified {len(active_constraints)} active constraints:\")\n",
        "        for i, constraint in enumerate(active_constraints):\n",
        "            if constraint['type'] == 'equality':\n",
        "                self._print(f\"  Constraint {i}: Equality\")\n",
        "            elif constraint['type'] == 'inequality':\n",
        "                self._print(f\"  Constraint {i}: Inequality (Index {constraint['index']})\")\n",
        "\n",
        "        return active_constraints\n",
        "\n",
        "    def add_slack_variables(self, active_constraints, x):\n",
        "        \"\"\"\n",
        "        Convert active inequality constraints to equality constraints by adding slack variables.\n",
        "\n",
        "        Args:\n",
        "            active_constraints (list of dicts): List of active constraints with details.\n",
        "            x (np.array): Current variable vector.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (updated active_constraints list with modified functions, updated x with slack variables)\n",
        "        \"\"\"\n",
        "        for constraint in active_constraints:\n",
        "            if constraint['type'] == 'inequality':\n",
        "                ineq_idx = constraint['index']\n",
        "                if ineq_idx not in self.slack_vars:\n",
        "                    # Define a new slack variable for this active inequality\n",
        "                    slack_idx = len(x)\n",
        "                    self.slack_vars[ineq_idx] = slack_idx\n",
        "\n",
        "                    # Extend bounds to include slack variable (s_i >= 0)\n",
        "                    self.bounds.append((0, None))\n",
        "\n",
        "                    # Initialize slack variable in x\n",
        "                    x = np.append(x, 0.0)\n",
        "\n",
        "                    self._print(f\"Added slack variable s_{ineq_idx} at index {slack_idx} for inequality constraint {ineq_idx}.\")\n",
        "\n",
        "                else:\n",
        "                    slack_idx = self.slack_vars[ineq_idx]\n",
        "                    self._print(f\"Using existing slack variable s_{ineq_idx} at index {slack_idx} for inequality constraint {ineq_idx}.\")\n",
        "\n",
        "                # Modify the constraint function to include the slack variable: h(x) + s = 0\n",
        "                original_ineq = self.ineq_constraints[ineq_idx]\n",
        "                def modified_constraint(x, h=original_ineq, s_idx=slack_idx):\n",
        "                    return h(x[:self.num_original_vars]) + x[s_idx]\n",
        "\n",
        "                # Update the constraint function in active_constraints\n",
        "                constraint['function'] = modified_constraint\n",
        "                constraint['slack_var_idx'] = slack_idx\n",
        "\n",
        "                self._print(f\"Modified inequality constraint {ineq_idx} to include slack variable s_{ineq_idx}.\")\n",
        "\n",
        "        return active_constraints, x\n",
        "\n",
        "    def remove_slack_variables(self, x):\n",
        "        \"\"\"\n",
        "        Remove slack variables from the variable vector and update bounds accordingly.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Current variable vector.\n",
        "\n",
        "        Returns:\n",
        "            np.array: Variable vector without slack variables.\n",
        "        \"\"\"\n",
        "        if not self.slack_vars:\n",
        "            return x\n",
        "\n",
        "        # Sort slack indices in descending order to remove from the end first\n",
        "        slack_indices = sorted(self.slack_vars.values(), reverse=True)\n",
        "        for idx in slack_indices:\n",
        "            self._print(f\"Removing slack variable at index {idx}.\")\n",
        "            # Remove slack variable from x\n",
        "            x = np.delete(x, idx)\n",
        "            # Remove corresponding bound\n",
        "            self.bounds.pop(idx)\n",
        "\n",
        "        # Clear slack_vars mapping\n",
        "        self.slack_vars = {}\n",
        "        return x\n",
        "\n",
        "    def partition_variables(self, J, x):\n",
        "        \"\"\"\n",
        "        Partition variables into independent and dependent variables based on active constraints and bounds.\n",
        "\n",
        "        Args:\n",
        "            J (np.array): Jacobian matrix of active constraints.\n",
        "            x (np.array): Current variable vector.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (independent_indices, dependent_indices)\n",
        "        \"\"\"\n",
        "        n = len(x)\n",
        "        m = J.shape[0]\n",
        "\n",
        "        # Identify variables at their bounds (considering scaling)\n",
        "        independent_indices = []\n",
        "        for i, (lower, upper) in enumerate(self.bounds):\n",
        "            if lower is not None and np.abs(x[i] - lower) <= self.tol:\n",
        "                independent_indices.append(i)\n",
        "            elif upper is not None and np.abs(x[i] - upper) <= self.tol:\n",
        "                independent_indices.append(i)\n",
        "\n",
        "        self._print(f\"Variables at bounds (independent): {independent_indices}\")\n",
        "\n",
        "        # Determine additional independent variables to satisfy the number of constraints\n",
        "        additional_indep = m - len(independent_indices)\n",
        "        if additional_indep > 0:\n",
        "            # Select variables with largest absolute gradient not already in independent_indices\n",
        "            grad = np.abs(J).sum(axis=0)\n",
        "            sorted_indices = np.argsort(-grad)\n",
        "            for i in sorted_indices:\n",
        "                if i not in independent_indices:\n",
        "                    independent_indices.append(i)\n",
        "                    additional_indep -= 1\n",
        "                    if additional_indep == 0:\n",
        "                        break\n",
        "            self._print(f\"Additional independent variables selected based on Jacobian gradients: {independent_indices[-additional_indep:] if additional_indep>0 else independent_indices}\")\n",
        "\n",
        "        # Ensure the number of independent variables matches the number of constraints\n",
        "        if len(independent_indices) < m:\n",
        "            # Add more variables if necessary\n",
        "            for i in range(n):\n",
        "                if i not in independent_indices:\n",
        "                    independent_indices.append(i)\n",
        "                    if len(independent_indices) == m:\n",
        "                        break\n",
        "            self._print(f\"Final independent variables after ensuring count: {independent_indices}\")\n",
        "\n",
        "        # Dependent variables are those not in independent_indices\n",
        "        dependent_indices = [i for i in range(n) if i not in independent_indices]\n",
        "        self._print(f\"Dependent Variables: {dependent_indices}\")\n",
        "\n",
        "        return independent_indices, dependent_indices\n",
        "\n",
        "    def compute_reduced_gradient(self, grad, J):\n",
        "        \"\"\"\n",
        "        Compute the reduced gradient.\n",
        "\n",
        "        Args:\n",
        "            grad (np.array): Gradient of the objective function.\n",
        "            J (np.array): Jacobian matrix of active constraints.\n",
        "\n",
        "        Returns:\n",
        "            np.array: Reduced gradient.\n",
        "        \"\"\"\n",
        "        if J.size == 0:\n",
        "            return grad\n",
        "\n",
        "        # Solve J^T * lambda = -grad for lambda using least squares\n",
        "        JT = J.T\n",
        "        try:\n",
        "            lambda_, residuals, rank, s = lstsq(JT, -grad, lapack_driver='gelsy')\n",
        "            self._print(f\"Lagrange multipliers (lambda): {lambda_}\")\n",
        "        except np.linalg.LinAlgError:\n",
        "            self._print(\"Singular Jacobian encountered while solving for lambda.\")\n",
        "            lambda_ = np.zeros(JT.shape[1])\n",
        "\n",
        "        # Compute reduced gradient: grad + J^T * lambda\n",
        "        reduced_grad = grad + JT @ lambda_\n",
        "\n",
        "        self._print(f\"Reduced Gradient: {reduced_grad}\")\n",
        "        return reduced_grad\n",
        "\n",
        "    def compute_null_space(self, J):\n",
        "        \"\"\"\n",
        "        Compute the null space of the Jacobian matrix J.\n",
        "\n",
        "        Args:\n",
        "            J (np.array): Jacobian matrix.\n",
        "\n",
        "        Returns:\n",
        "            np.array: Null space of J.\n",
        "        \"\"\"\n",
        "        if J.size == 0:\n",
        "            return np.identity(len(self.bounds))  # All variables are free\n",
        "\n",
        "        null_sp = null_space(J)\n",
        "        self._print(f\"Computed null space with shape {null_sp.shape}:\\n{null_sp}\")\n",
        "        return null_sp\n",
        "\n",
        "    def find_search_direction(self, reduced_grad, null_space_J):\n",
        "        \"\"\"\n",
        "        Calculate a search direction based on the reduced gradient and null space.\n",
        "\n",
        "        Args:\n",
        "            reduced_grad (np.array): Reduced gradient.\n",
        "            null_space_J (np.array): Null space matrix.\n",
        "\n",
        "        Returns:\n",
        "            np.array: Search direction.\n",
        "        \"\"\"\n",
        "        if np.linalg.norm(reduced_grad) < self.tol:\n",
        "            self._print(\"Reduced gradient norm is below tolerance. No search direction needed.\")\n",
        "            return np.zeros_like(reduced_grad)\n",
        "\n",
        "        N = null_space_J\n",
        "        if N.size == 0:\n",
        "            self._print(\"Null space is empty. No feasible directions available.\")\n",
        "            return np.zeros_like(reduced_grad)\n",
        "\n",
        "        # Find c that minimizes ||reduced_grad + N c||^2\n",
        "        # Solution: c = - (N^T N)^-1 N^T reduced_grad\n",
        "        NtN = N.T @ N\n",
        "        Nt_grad = N.T @ reduced_grad\n",
        "\n",
        "        # Regularize NtN to improve numerical stability\n",
        "        reg = 1e-8 * np.eye(NtN.shape[0])\n",
        "        try:\n",
        "            c = -np.linalg.solve(NtN + reg, Nt_grad)\n",
        "            self._print(f\"Coefficient vector c for search direction: {c}\")\n",
        "        except np.linalg.LinAlgError:\n",
        "            self._print(\"Singular matrix encountered while solving for c. Using least squares solution.\")\n",
        "            c, residuals, rank, s = lstsq(NtN + reg, -Nt_grad, lapack_driver='gelsy')\n",
        "\n",
        "        search_direction = N @ c\n",
        "        self._print(f\"Search Direction: {search_direction}\")\n",
        "        return search_direction\n",
        "\n",
        "    def line_search_step(self, x, direction, independent_indices, dependent_indices, active_constraints):\n",
        "        \"\"\"\n",
        "        Perform a line search to find the optimal step size using Wolfe conditions.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Current point.\n",
        "            direction (np.array): Search direction.\n",
        "            independent_indices (list): Indices of independent variables.\n",
        "            dependent_indices (list): Indices of dependent variables.\n",
        "            active_constraints (list of dicts): List of active constraints with details.\n",
        "\n",
        "        Returns:\n",
        "            float: Optimal step size alpha.\n",
        "            str: Termination condition.\n",
        "        \"\"\"\n",
        "        alpha = 1.0  # Initial step size\n",
        "        alpha_min = 1e-8\n",
        "        rho = 0.5     # Step size reduction factor\n",
        "        c1 = 1e-4     # Armijo condition parameter\n",
        "        c2 = 0.9      # Curvature condition parameter\n",
        "\n",
        "        f_x = self.objective(x)\n",
        "        grad_f = self.compute_gradient(self.objective_grad, x, func=self.objective)\n",
        "\n",
        "        # Directional derivative\n",
        "        grad_dir = np.dot(grad_f, direction)\n",
        "\n",
        "        self._print(f\"Initial objective value: {f_x}\")\n",
        "        self._print(f\"Directional derivative: {grad_dir}\")\n",
        "\n",
        "        if grad_dir >= 0:\n",
        "            self._print(\"Not a descent direction.\")\n",
        "            return 0, \"not_descent_direction\"\n",
        "\n",
        "        while alpha > alpha_min:\n",
        "            x_trial = np.copy(x)\n",
        "            x_trial[independent_indices] += alpha * direction[independent_indices]\n",
        "            self._print(f\"Trial step with alpha={alpha}: {x_trial}\")\n",
        "\n",
        "            # Adjust dependent variables to satisfy constraints\n",
        "            x_trial = self.newton_raphson_adjustment(x_trial, active_constraints, dependent_indices)\n",
        "\n",
        "            f_trial = self.objective(x_trial)\n",
        "            grad_f_trial = self.compute_gradient(self.objective_grad, x_trial, func=self.objective)\n",
        "            grad_dir_trial = np.dot(grad_f_trial, direction)\n",
        "\n",
        "            self._print(f\"Trial objective value: {f_trial}\")\n",
        "            self._print(f\"Trial directional derivative: {grad_dir_trial}\")\n",
        "\n",
        "            # Check Armijo condition\n",
        "            if f_trial <= f_x + c1 * alpha * grad_dir:\n",
        "                # Check curvature condition\n",
        "                if grad_dir_trial >= c2 * grad_dir:\n",
        "                    # Wolfe conditions satisfied\n",
        "                    if self.is_feasible(x_trial, active_constraints):\n",
        "                        self._print(f\"Wolfe conditions satisfied with alpha={alpha}.\")\n",
        "                        return alpha, \"wolfe_conditions_met\"\n",
        "\n",
        "            # Reduce step size\n",
        "            alpha *= rho\n",
        "            self._print(f\"Reducing step size to alpha={alpha}.\")\n",
        "\n",
        "        self._print(\"Line search terminated without satisfying Wolfe conditions.\")\n",
        "        return alpha, \"step_size_min\"\n",
        "\n",
        "    def is_feasible(self, x, active_constraints):\n",
        "        \"\"\"\n",
        "        Check if x satisfies all active constraints and variable bounds.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Point to check.\n",
        "            active_constraints (list of dicts): List of active constraints with details.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if feasible, False otherwise.\n",
        "        \"\"\"\n",
        "        # Check equality and inequality constraints\n",
        "        for constraint in active_constraints:\n",
        "            if np.abs(constraint['function'](x)) > self.tol:\n",
        "                self._print(f\"Constraint violated: {constraint['function'](x)} > tol.\")\n",
        "                return False\n",
        "\n",
        "        # Check variable bounds\n",
        "        for i, (lower, upper) in enumerate(self.bounds):\n",
        "            if lower is not None and x[i] < lower - self.tol:\n",
        "                self._print(f\"Variable x[{i}] = {x[i]} is below lower bound {lower}.\")\n",
        "                return False\n",
        "            if upper is not None and x[i] > upper + self.tol:\n",
        "                self._print(f\"Variable x[{i}] = {x[i]} is above upper bound {upper}.\")\n",
        "                return False\n",
        "\n",
        "        self._print(\"All constraints and variable bounds are satisfied.\")\n",
        "        return True\n",
        "\n",
        "    def newton_raphson_adjustment(self, x, active_constraints, dependent_indices, max_nr_iter=50):\n",
        "        \"\"\"\n",
        "        Adjust dependent variables using Newton-Raphson to satisfy active constraints.\n",
        "\n",
        "        Args:\n",
        "            x (np.array): Current point.\n",
        "            active_constraints (list of dicts): List of active constraints with details.\n",
        "            dependent_indices (list): Indices of dependent variables.\n",
        "            max_nr_iter (int, optional): Maximum Newton-Raphson iterations.\n",
        "\n",
        "        Returns:\n",
        "            np.array: Adjusted point.\n",
        "        \"\"\"\n",
        "        x_new = np.copy(x)\n",
        "        all_active = active_constraints\n",
        "\n",
        "        for iteration in range(max_nr_iter):\n",
        "            J = self.compute_jacobian(all_active, x_new)\n",
        "            residuals = np.array([constraint['function'](x_new) for constraint in all_active])\n",
        "\n",
        "            self._print(f\"  Newton-Raphson Iteration {iteration+1}: Residuals = {residuals}\")\n",
        "\n",
        "            if np.linalg.norm(residuals) < self.tol:\n",
        "                self._print(\"  Convergence achieved in Newton-Raphson adjustment.\")\n",
        "                break\n",
        "\n",
        "            if J.size == 0 or not dependent_indices:\n",
        "                self._print(\"  No constraints to satisfy or no dependent variables to adjust.\")\n",
        "                break  # No constraints to satisfy or no dependent variables to adjust\n",
        "\n",
        "            # Extract Jacobian columns corresponding to dependent variables\n",
        "            J_dep = J[:, dependent_indices]\n",
        "\n",
        "            try:\n",
        "                # Solve J_dep * delta = -residuals\n",
        "                delta_dep, residuals_lsq, rank, s = lstsq(J_dep, -residuals, lapack_driver='gelsy')\n",
        "                x_new[dependent_indices] += delta_dep\n",
        "\n",
        "                self._print(f\"  Adjusted dependent variables with delta: {delta_dep}\")\n",
        "                self._print(f\"  Updated x: {x_new}\")\n",
        "\n",
        "                # Enforce variable bounds after each update\n",
        "                for i in dependent_indices:\n",
        "                    lower, upper = self.bounds[i]\n",
        "                    if lower is not None and x_new[i] < lower:\n",
        "                        x_new[i] = lower\n",
        "                        self._print(f\"  Variable x[{i}] adjusted to lower bound: {x_new[i]}\")\n",
        "                    if upper is not None and x_new[i] > upper:\n",
        "                        x_new[i] = upper\n",
        "                        self._print(f\"  Variable x[{i}] adjusted to upper bound: {x_new[i]}\")\n",
        "\n",
        "                # Check for convergence\n",
        "                if np.linalg.norm(delta_dep) < self.tol:\n",
        "                    self._print(\"  Convergence achieved in Newton-Raphson adjustment.\")\n",
        "                    break\n",
        "            except np.linalg.LinAlgError:\n",
        "                self._print(\"  Singular Jacobian encountered during Newton-Raphson adjustment.\")\n",
        "                break\n",
        "\n",
        "        return x_new\n",
        "\n",
        "    def minimize(self, x0):\n",
        "        \"\"\"\n",
        "        Perform the GRG optimization.\n",
        "\n",
        "        Args:\n",
        "            x0 (list or np.array): Initial guess for the variables.\n",
        "\n",
        "        Returns:\n",
        "            np.array: Optimal solution found by the GRG algorithm (original variables only).\n",
        "        \"\"\"\n",
        "        x = np.array(x0, dtype=float)\n",
        "        x = self.remove_slack_variables(x)  # Ensure no residual slack variables from previous runs\n",
        "\n",
        "        for iteration in range(1, self.max_iter + 1):\n",
        "            self._print(f\"\\n--- Iteration {iteration} ---\")\n",
        "            self._print(f\"Current x: {x}\")\n",
        "\n",
        "            # Step 1: Identify active constraints at current x\n",
        "            active_constraints = self.identify_active_constraints(x)\n",
        "            active_eq = [c for c in active_constraints if c['type'] == 'equality']\n",
        "            active_ineq = [c for c in active_constraints if c['type'] == 'inequality']\n",
        "            self._print(f\"Active Equality Constraints: {len(active_eq)}\")\n",
        "            self._print(f\"Active Inequality Constraints: {len(active_ineq)}\")\n",
        "\n",
        "            # Step 2: Add slack variables for active inequality constraints\n",
        "            active_constraints, x = self.add_slack_variables(active_constraints, x)\n",
        "            self._print(f\"Total Active Constraints (including slack): {len(active_constraints)}\")\n",
        "\n",
        "            # Step 3: Scale constraints and variables\n",
        "            self.scale_constraints_and_variables(x, active_constraints)\n",
        "\n",
        "            # Step 4: Compute Jacobian of active constraints\n",
        "            J = self.compute_jacobian(active_constraints, x)\n",
        "\n",
        "            # Step 5: Partition variables into independent and dependent\n",
        "            independent_indices, dependent_indices = self.partition_variables(J, x)\n",
        "\n",
        "            # Step 6: Compute gradient and reduced gradient\n",
        "            grad = self.compute_gradient(self.objective_grad, x, func=self.objective)\n",
        "            self._print(f\"Gradient: {grad}\")\n",
        "\n",
        "            reduced_grad = self.compute_reduced_gradient(grad, J)\n",
        "            norm_reduced_grad = np.linalg.norm(reduced_grad)\n",
        "            self._print(f\"||Reduced Gradient||: {norm_reduced_grad}\")\n",
        "\n",
        "            # Check convergence based on the reduced gradient norm\n",
        "            if norm_reduced_grad < self.tol:\n",
        "                self._print(\"Convergence achieved based on reduced gradient norm.\")\n",
        "                break\n",
        "\n",
        "            # Step 7: Compute null space of J for feasible directions\n",
        "            null_space_J = self.compute_null_space(J)\n",
        "\n",
        "            if null_space_J.size == 0:\n",
        "                self._print(\"No feasible directions available. Constraints may be too restrictive.\")\n",
        "                break\n",
        "\n",
        "            # Step 8: Find the search direction\n",
        "            search_direction = self.find_search_direction(reduced_grad, null_space_J)\n",
        "\n",
        "            if np.linalg.norm(search_direction) < self.tol:\n",
        "                self._print(\"Search direction is negligible. Optimization may have converged.\")\n",
        "                break\n",
        "\n",
        "            # Step 9: Perform line search to find optimal step size\n",
        "            alpha, termination = self.line_search_step(x, search_direction, independent_indices, dependent_indices, active_constraints)\n",
        "\n",
        "            if termination in [\"step_size_min\", \"not_descent_direction\"]:\n",
        "                self._print(\"Line search terminated without satisfying Wolfe conditions.\")\n",
        "                break\n",
        "\n",
        "            # Step 10: Update variables with the step\n",
        "            x_new = np.copy(x)\n",
        "            x_new[independent_indices] += alpha * search_direction[independent_indices]\n",
        "            self._print(f\"x_new after independent step: {x_new}\")\n",
        "\n",
        "            # Step 11: Adjust dependent variables to satisfy constraints using Newton-Raphson\n",
        "            x_new = self.newton_raphson_adjustment(x_new, active_constraints, dependent_indices)\n",
        "            self._print(f\"x_new after Newton-Raphson adjustment: {x_new}\")\n",
        "\n",
        "            # Print the objective function value\n",
        "            f_x = self.objective(x)\n",
        "            self._print(f\"Objective Function Value: {f_x}\")\n",
        "\n",
        "            # Prepare for next iteration\n",
        "            x = x_new\n",
        "\n",
        "            # Step 12: Remove slack variables for the next iteration\n",
        "            x = self.remove_slack_variables(x)\n",
        "            self._print(f\"x after removing slack variables: {x}\")\n",
        "\n",
        "        else:\n",
        "            self._print(\"Maximum iterations reached without convergence.\")\n",
        "\n",
        "        # Final removal of any remaining slack variables before returning\n",
        "        x = self.remove_slack_variables(x)\n",
        "        self._print(f\"Final x after ensuring removal of all slack variables: {x}\")\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example problem implementation with analytical gradients and Jacobians\n",
        "def objective(x):\n",
        "    \"\"\"\n",
        "    Objective function to minimize: f(x) = x1^2 + x2^2\n",
        "    \"\"\"\n",
        "    return x[0]**2 + x[1]**2\n",
        "\n",
        "def objective_grad(x):\n",
        "    \"\"\"\n",
        "    Analytical gradient of the objective function: grad_f = [2*x1, 2*x2]\n",
        "    \"\"\"\n",
        "    return np.array([2 * x[0], 2 * x[1]])\n",
        "\n",
        "def eq_constraint1(x):\n",
        "    \"\"\"\n",
        "    Equality constraint: x1 + x2 = 1\n",
        "    \"\"\"\n",
        "    return x[0] + x[1] - 1\n",
        "\n",
        "def eq_constraint1_jac(x):\n",
        "    \"\"\"\n",
        "    Analytical Jacobian of the equality constraint: [1, 1]\n",
        "    \"\"\"\n",
        "    return np.array([1.0, 1.0])\n",
        "\n",
        "def ineq_constraint1(x):\n",
        "    \"\"\"\n",
        "    Inequality constraint: x1 <= 0.5\n",
        "    \"\"\"\n",
        "    return x[0] - 0.5\n",
        "\n",
        "def ineq_constraint1_jac(x):\n",
        "    \"\"\"\n",
        "    Analytical Jacobian of the inequality constraint: [1, 0]\n",
        "    \"\"\"\n",
        "    return np.array([1.0, 0.0])\n",
        "\n",
        "def ineq_constraint2(x):\n",
        "    \"\"\"\n",
        "    Inequality constraint: x1 >= 0, which is equivalent to -x1 <= 0\n",
        "    \"\"\"\n",
        "    return -x[0]\n",
        "\n",
        "def ineq_constraint2_jac(x):\n",
        "    \"\"\"\n",
        "    Analytical Jacobian of the inequality constraint: [-1, 0]\n",
        "    \"\"\"\n",
        "    return np.array([-1.0, 0.0])"
      ],
      "metadata": {
        "id": "zjxDwahBe19e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Define variable bounds\n",
        "    # x1 >= 0, x1 <= 0.5, x2 is free\n",
        "    bounds = [(0.0, 0.5), (None, None)]  # x1 bounds and x2 is unbounded\n",
        "\n",
        "    # Initialize GRG Solver with analytical gradients and Jacobians\n",
        "    grg_solver = GeneralizedReducedGradient(\n",
        "        objective=objective,\n",
        "        eq_constraints=[eq_constraint1],\n",
        "        ineq_constraints=[ineq_constraint1, ineq_constraint2],\n",
        "        bounds=bounds,\n",
        "        objective_grad=objective_grad,\n",
        "        eq_constraints_jac=[eq_constraint1_jac],\n",
        "        ineq_constraints_jac=[ineq_constraint1_jac, ineq_constraint2_jac],\n",
        "        tol=1e-6,\n",
        "        max_iter=100,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # Initial point\n",
        "    x0 = [0.13, 0.81]\n",
        "\n",
        "    # Solve the optimization problem\n",
        "    solution = grg_solver.minimize(x0)\n",
        "    print(\"\\nOptimal solution:\", solution)\n",
        "    print(\"Objective value at optimum:\", objective(solution))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flou_V9_e8E3",
        "outputId": "0888c838-9837-4bca-bf1b-8dea9e7588fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Iteration 1 ---\n",
            "Current x: [0.13 0.81]\n",
            "Identified 1 active constraints:\n",
            "  Constraint 0: Equality\n",
            "Active Equality Constraints: 1\n",
            "Active Inequality Constraints: 0\n",
            "Total Active Constraints (including slack): 1\n",
            "Computing Jacobian for 1 active constraints and 2 variables.\n",
            "Constraint 0: Equality Jacobian row: [1. 1.]\n",
            "Final Jacobian matrix:\n",
            "[[1. 1.]]\n",
            "Variables at bounds (independent): []\n",
            "Additional independent variables selected based on Jacobian gradients: [0]\n",
            "Dependent Variables: [1]\n",
            "Gradient: [0.26 1.62]\n",
            "Lagrange multipliers (lambda): [-0.94]\n",
            "Reduced Gradient: [-0.68  0.68]\n",
            "||Reduced Gradient||: 0.9616652224137047\n",
            "Computed null space with shape (2, 1):\n",
            "[[-0.70710678]\n",
            " [ 0.70710678]]\n",
            "Coefficient vector c for search direction: [-0.96166521]\n",
            "Search Direction: [ 0.67999999 -0.67999999]\n",
            "Initial objective value: 0.6730000000000002\n",
            "Directional derivative: -0.9247999907520001\n",
            "Trial step with alpha=1.0: [0.80999999 0.81      ]\n",
            "Computing Jacobian for 1 active constraints and 2 variables.\n",
            "Constraint 0: Equality Jacobian row: [1. 1.]\n",
            "Final Jacobian matrix:\n",
            "[[1. 1.]]\n",
            "  Newton-Raphson Iteration 1: Residuals = [0.61999999]\n",
            "  Adjusted dependent variables with delta: [-0.61999999]\n",
            "  Updated x: [0.80999999 0.19000001]\n",
            "Computing Jacobian for 1 active constraints and 2 variables.\n",
            "Constraint 0: Equality Jacobian row: [1. 1.]\n",
            "Final Jacobian matrix:\n",
            "[[1. 1.]]\n",
            "  Newton-Raphson Iteration 2: Residuals = [0.]\n",
            "  Convergence achieved in Newton-Raphson adjustment.\n",
            "Trial objective value: 0.6921999915680004\n",
            "Trial directional derivative: 0.8431999730720008\n",
            "Reducing step size to alpha=0.5.\n",
            "Trial step with alpha=0.5: [0.47 0.81]\n",
            "Computing Jacobian for 1 active constraints and 2 variables.\n",
            "Constraint 0: Equality Jacobian row: [1. 1.]\n",
            "Final Jacobian matrix:\n",
            "[[1. 1.]]\n",
            "  Newton-Raphson Iteration 1: Residuals = [0.28]\n",
            "  Adjusted dependent variables with delta: [-0.28]\n",
            "  Updated x: [0.47 0.53]\n",
            "Computing Jacobian for 1 active constraints and 2 variables.\n",
            "Constraint 0: Equality Jacobian row: [1. 1.]\n",
            "Final Jacobian matrix:\n",
            "[[1. 1.]]\n",
            "  Newton-Raphson Iteration 2: Residuals = [0.]\n",
            "  Convergence achieved in Newton-Raphson adjustment.\n",
            "Trial objective value: 0.5018000004079999\n",
            "Trial directional derivative: -0.08160000843199966\n",
            "All constraints and variable bounds are satisfied.\n",
            "Wolfe conditions satisfied with alpha=0.5.\n",
            "x_new after independent step: [0.47 0.81]\n",
            "Computing Jacobian for 1 active constraints and 2 variables.\n",
            "Constraint 0: Equality Jacobian row: [1. 1.]\n",
            "Final Jacobian matrix:\n",
            "[[1. 1.]]\n",
            "  Newton-Raphson Iteration 1: Residuals = [0.28]\n",
            "  Adjusted dependent variables with delta: [-0.28]\n",
            "  Updated x: [0.47 0.53]\n",
            "Computing Jacobian for 1 active constraints and 2 variables.\n",
            "Constraint 0: Equality Jacobian row: [1. 1.]\n",
            "Final Jacobian matrix:\n",
            "[[1. 1.]]\n",
            "  Newton-Raphson Iteration 2: Residuals = [0.]\n",
            "  Convergence achieved in Newton-Raphson adjustment.\n",
            "x_new after Newton-Raphson adjustment: [0.47 0.53]\n",
            "Objective Function Value: 0.6730000000000002\n",
            "x after removing slack variables: [0.47 0.53]\n",
            "\n",
            "--- Iteration 2 ---\n",
            "Current x: [0.47 0.53]\n",
            "Identified 1 active constraints:\n",
            "  Constraint 0: Equality\n",
            "Active Equality Constraints: 1\n",
            "Active Inequality Constraints: 0\n",
            "Total Active Constraints (including slack): 1\n",
            "Computing Jacobian for 1 active constraints and 2 variables.\n",
            "Constraint 0: Equality Jacobian row: [1. 1.]\n",
            "Final Jacobian matrix:\n",
            "[[1. 1.]]\n",
            "Variables at bounds (independent): []\n",
            "Additional independent variables selected based on Jacobian gradients: [0]\n",
            "Dependent Variables: [1]\n",
            "Gradient: [0.93999999 1.06000001]\n",
            "Lagrange multipliers (lambda): [-1.]\n",
            "Reduced Gradient: [-0.06000001  0.06000001]\n",
            "||Reduced Gradient||: 0.08485282335903761\n",
            "Computed null space with shape (2, 1):\n",
            "[[-0.70710678]\n",
            " [ 0.70710678]]\n",
            "Coefficient vector c for search direction: [-0.08485282]\n",
            "Search Direction: [ 0.06000001 -0.06000001]\n",
            "Initial objective value: 0.5018000004079999\n",
            "Directional derivative: -0.007200001560000023\n",
            "Trial step with alpha=1.0: [0.53 0.53]\n",
            "Computing Jacobian for 1 active constraints and 2 variables.\n",
            "Constraint 0: Equality Jacobian row: [1. 1.]\n",
            "Final Jacobian matrix:\n",
            "[[1. 1.]]\n",
            "  Newton-Raphson Iteration 1: Residuals = [0.06000001]\n",
            "  Adjusted dependent variables with delta: [-0.06000001]\n",
            "  Updated x: [0.53 0.47]\n",
            "Computing Jacobian for 1 active constraints and 2 variables.\n",
            "Constraint 0: Equality Jacobian row: [1. 1.]\n",
            "Final Jacobian matrix:\n",
            "[[1. 1.]]\n",
            "  Newton-Raphson Iteration 2: Residuals = [0.]\n",
            "  Convergence achieved in Newton-Raphson adjustment.\n",
            "Trial objective value: 0.501800000336\n",
            "Trial directional derivative: 0.007200001415999989\n",
            "Reducing step size to alpha=0.5.\n",
            "Trial step with alpha=0.5: [0.5  0.53]\n",
            "Computing Jacobian for 1 active constraints and 2 variables.\n",
            "Constraint 0: Equality Jacobian row: [1. 1.]\n",
            "Final Jacobian matrix:\n",
            "[[1. 1.]]\n",
            "  Newton-Raphson Iteration 1: Residuals = [0.03]\n",
            "  Adjusted dependent variables with delta: [-0.03]\n",
            "  Updated x: [0.5 0.5]\n",
            "Computing Jacobian for 1 active constraints and 2 variables.\n",
            "Constraint 0: Equality Jacobian row: [1. 1.]\n",
            "Final Jacobian matrix:\n",
            "[[1. 1.]]\n",
            "  Newton-Raphson Iteration 2: Residuals = [0.]\n",
            "  Convergence achieved in Newton-Raphson adjustment.\n",
            "Trial objective value: 0.49999999999999994\n",
            "Trial directional derivative: -7.200002399843086e-11\n",
            "All constraints and variable bounds are satisfied.\n",
            "Wolfe conditions satisfied with alpha=0.5.\n",
            "x_new after independent step: [0.5  0.53]\n",
            "Computing Jacobian for 1 active constraints and 2 variables.\n",
            "Constraint 0: Equality Jacobian row: [1. 1.]\n",
            "Final Jacobian matrix:\n",
            "[[1. 1.]]\n",
            "  Newton-Raphson Iteration 1: Residuals = [0.03]\n",
            "  Adjusted dependent variables with delta: [-0.03]\n",
            "  Updated x: [0.5 0.5]\n",
            "Computing Jacobian for 1 active constraints and 2 variables.\n",
            "Constraint 0: Equality Jacobian row: [1. 1.]\n",
            "Final Jacobian matrix:\n",
            "[[1. 1.]]\n",
            "  Newton-Raphson Iteration 2: Residuals = [0.]\n",
            "  Convergence achieved in Newton-Raphson adjustment.\n",
            "x_new after Newton-Raphson adjustment: [0.5 0.5]\n",
            "Objective Function Value: 0.5018000004079999\n",
            "x after removing slack variables: [0.5 0.5]\n",
            "\n",
            "--- Iteration 3 ---\n",
            "Current x: [0.5 0.5]\n",
            "Identified 2 active constraints:\n",
            "  Constraint 0: Equality\n",
            "  Constraint 1: Inequality (Index 0)\n",
            "Active Equality Constraints: 1\n",
            "Active Inequality Constraints: 1\n",
            "Added slack variable s_0 at index 2 for inequality constraint 0.\n",
            "Modified inequality constraint 0 to include slack variable s_0.\n",
            "Total Active Constraints (including slack): 2\n",
            "Computing Jacobian for 2 active constraints and 3 variables.\n",
            "Constraint 0: Equality Jacobian row: [1. 1. 0.]\n",
            "Constraint 1: Inequality Jacobian row (original vars): [1. 0.]\n",
            "Constraint 1: Inequality Jacobian row after slack variable: [1. 0. 1.]\n",
            "Final Jacobian matrix:\n",
            "[[1. 1. 0.]\n",
            " [1. 0. 1.]]\n",
            "Variables at bounds (independent): [0, 2]\n",
            "Dependent Variables: [1]\n",
            "Gradient: [1. 1. 0.]\n",
            "Lagrange multipliers (lambda): [-1.00000000e+00  4.00000104e-10]\n",
            "Reduced Gradient: [-3.99999589e-10  4.00000477e-10  4.00000104e-10]\n",
            "||Reduced Gradient||: 6.928204214644156e-10\n",
            "Convergence achieved based on reduced gradient norm.\n",
            "Removing slack variable at index 2.\n",
            "Final x after ensuring removal of all slack variables: [0.5 0.5]\n",
            "\n",
            "Optimal solution: [0.5 0.5]\n",
            "Objective value at optimum: 0.49999999999999994\n"
          ]
        }
      ]
    }
  ]
}