{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7sjP-3CctS8f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "import statsmodels.api as sm\n",
        "from scipy.optimize import minimize\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Wl814vADtdwY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression with scikit-learn\n",
        "sklearn_model = LogisticRegression(solver='lbfgs')\n",
        "sklearn_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_sklearn = sklearn_model.predict(X_test)\n",
        "y_pred_proba_sklearn = sklearn_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "log_loss_sklearn = log_loss(y_test, y_pred_proba_sklearn)\n",
        "print(f\"Accuracy (scikit-learn): {accuracy_sklearn:.4f}\")\n",
        "print(f\"Log Loss (scikit-learn): {log_loss_sklearn:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syfGbnD5tdtx",
        "outputId": "aef88455-8fc4-4b2d-da29-d5ab5144fcf6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (scikit-learn): 0.8467\n",
            "Log Loss (scikit-learn): 0.3566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression with statsmodels\n",
        "X_train_sm = sm.add_constant(X_train)  # Adding intercept term\n",
        "X_test_sm = sm.add_constant(X_test)\n",
        "\n",
        "statsmodels_model = sm.Logit(y_train, X_train_sm).fit()\n",
        "\n",
        "y_pred_proba_sm = statsmodels_model.predict(X_test_sm)\n",
        "y_pred_sm = (y_pred_proba_sm >= 0.5).astype(int)\n",
        "\n",
        "accuracy_sm = accuracy_score(y_test, y_pred_sm)\n",
        "log_loss_sm = log_loss(y_test, y_pred_proba_sm)\n",
        "print(f\"\\nAccuracy (statsmodels): {accuracy_sm:.4f}\")\n",
        "print(f\"Log Loss (statsmodels): {log_loss_sm:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VEpBgactdrN",
        "outputId": "037c3c88-6a18-419c-b246-88adc5c43795"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.322850\n",
            "         Iterations 8\n",
            "\n",
            "Accuracy (statsmodels): 0.8400\n",
            "Log Loss (statsmodels): 0.3576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class LogisticRegressionLagrange:\n",
        "    \"\"\"\n",
        "    Logistic Regression using Maximum Likelihood Estimation (MLE) with Lagrange multipliers.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    X : np.ndarray\n",
        "        The feature matrix.\n",
        "    y : np.ndarray\n",
        "        The target vector.\n",
        "    beta : np.ndarray\n",
        "        The estimated coefficients after fitting the model.\n",
        "    lambdas : np.ndarray\n",
        "        The Lagrange multipliers for the constraints.\n",
        "    constraints : list of callable\n",
        "        A list of constraint functions that return a scalar value to enforce a condition.\n",
        "\n",
        "    Methods:\n",
        "    --------\n",
        "    fit(solver='BFGS', regularization=None, alpha=1.0):\n",
        "        Fits the logistic regression model with optional regularization and constraints.\n",
        "    predict_proba(X_new):\n",
        "        Predicts probabilities for new data.\n",
        "    predict(X_new):\n",
        "        Predicts binary or multi-class class labels for new data.\n",
        "    evaluate(X_test, y_test):\n",
        "        Evaluates the model on test data, returning accuracy and log loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y, constraints=[]):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.beta = None\n",
        "        self.lambdas = None  # Lagrange multipliers\n",
        "        self.constraints = constraints  # List of constraint functions\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Computes the sigmoid function for binary classification.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def softmax(self, z):\n",
        "        \"\"\"Computes the softmax function for multi-class classification.\"\"\"\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_z / exp_z.sum(axis=1, keepdims=True)\n",
        "\n",
        "    def log_likelihood(self, beta):\n",
        "        \"\"\"Computes the negative log-likelihood for binary classification.\"\"\"\n",
        "        z = np.dot(self.X, beta)\n",
        "        return -np.sum(self.y * z - np.log(1 + np.exp(z)))\n",
        "\n",
        "    def log_likelihood_multi(self, beta):\n",
        "        \"\"\"Computes the negative log-likelihood for multi-class classification.\"\"\"\n",
        "        z = np.dot(self.X, beta)\n",
        "        prob = self.softmax(z)\n",
        "        return -np.sum(self.y * np.log(prob))\n",
        "\n",
        "    def regularization(self, beta, penalty='L2', alpha=1.0):\n",
        "        \"\"\"Adds regularization terms to the objective function.\"\"\"\n",
        "        if penalty == 'L2':\n",
        "            return alpha * np.sum(beta**2)  # L2 regularization\n",
        "        elif penalty == 'L1':\n",
        "            return alpha * np.sum(np.abs(beta))  # L1 regularization\n",
        "        elif penalty == 'ElasticNet':\n",
        "            return alpha * (0.5 * np.sum(beta**2) + 0.5 * np.sum(np.abs(beta)))  # Elastic Net\n",
        "        return 0  # No regularization\n",
        "\n",
        "    def constraint_function(self, beta):\n",
        "        \"\"\"Applies the constraints and returns the sum of all constraint violations.\"\"\"\n",
        "        if not self.constraints:\n",
        "            return 0  # No constraints\n",
        "        else:\n",
        "            return sum(constraint(beta) for constraint in self.constraints)\n",
        "\n",
        "    def lagrangian(self, params, regularization=None, alpha=1.0):\n",
        "        \"\"\"Defines the Lagrangian function.\"\"\"\n",
        "        beta = params[:self.X.shape[1]]  # The coefficients\n",
        "        lambdas = params[self.X.shape[1]:]  # The Lagrange multipliers\n",
        "\n",
        "        # Compute the log-likelihood (binary or multi-class)\n",
        "        if self.y.ndim == 1:  # Binary classification\n",
        "            log_likelihood_val = self.log_likelihood(beta)\n",
        "        else:  # Multi-class classification\n",
        "            log_likelihood_val = self.log_likelihood_multi(beta)\n",
        "\n",
        "        # Apply regularization\n",
        "        reg_val = self.regularization(beta, penalty=regularization, alpha=alpha)\n",
        "\n",
        "        # Compute constraint violations\n",
        "        constraint_val = sum(lambdas[i] * self.constraints[i](beta) for i in range(len(self.constraints)))\n",
        "\n",
        "        # Return Lagrangian\n",
        "        return log_likelihood_val + reg_val + constraint_val\n",
        "\n",
        "    def gradient(self, params, regularization=None, alpha=1.0):\n",
        "        \"\"\"Computes the gradient of the Lagrangian.\"\"\"\n",
        "        beta = params[:self.X.shape[1]]\n",
        "        lambdas = params[self.X.shape[1]:]\n",
        "\n",
        "        z = np.dot(self.X, beta)\n",
        "        if self.y.ndim == 1:  # Binary classification\n",
        "            prob = self.sigmoid(z)\n",
        "            grad_log_likelihood = np.dot(self.X.T, prob - self.y)\n",
        "        else:  # Multi-class classification\n",
        "            prob = self.softmax(z)\n",
        "            grad_log_likelihood = np.dot(self.X.T, (prob - self.y))\n",
        "\n",
        "        # Gradient of the constraint\n",
        "        grad_constraint = np.sum([self.constraints[i](beta) for i in range(len(self.constraints))])\n",
        "\n",
        "        # Gradient of regularization\n",
        "        if regularization == 'L2':\n",
        "            grad_reg = alpha * 2 * beta\n",
        "        elif regularization == 'L1':\n",
        "            grad_reg = alpha * np.sign(beta)\n",
        "        else:\n",
        "            grad_reg = 0\n",
        "\n",
        "        # Combine gradients\n",
        "        grad_lagrangian = grad_log_likelihood + grad_reg + grad_constraint\n",
        "        return np.concatenate([grad_lagrangian, np.zeros(len(lambdas))])  # No gradient for lambdas\n",
        "\n",
        "    def fit(self, solver='BFGS', regularization=None, alpha=1.0):\n",
        "        \"\"\"\n",
        "        Fits the logistic regression model using the Lagrangian approach.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        solver : str, optional\n",
        "            The optimization solver to use (default is 'BFGS').\n",
        "        regularization : str, optional\n",
        "            The type of regularization to apply ('L1', 'L2', or 'ElasticNet').\n",
        "        alpha : float, optional\n",
        "            The regularization strength (default is 1.0).\n",
        "        \"\"\"\n",
        "        initial_params = np.zeros(self.X.shape[1] + len(self.constraints))  # Initialize beta and lambdas\n",
        "\n",
        "        # Minimize the Lagrangian\n",
        "        result = minimize(self.lagrangian, initial_params, method=solver, jac=self.gradient,\n",
        "                          args=(regularization, alpha))\n",
        "        self.beta = result.x[:self.X.shape[1]]  # Extract coefficients\n",
        "        self.lambdas = result.x[self.X.shape[1]:]  # Extract Lagrange multipliers\n",
        "\n",
        "        logger.info(f\"Optimization completed with beta: {self.beta}, lambdas: {self.lambdas}\")\n",
        "        return self.beta\n",
        "\n",
        "    def predict_proba(self, X_new):\n",
        "        \"\"\"Predicts probabilities for new data.\"\"\"\n",
        "        z = np.dot(X_new, self.beta)\n",
        "        if self.y.ndim == 1:  # Binary classification\n",
        "            return self.sigmoid(z)\n",
        "        else:  # Multi-class classification\n",
        "            return self.softmax(z)\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"Predicts binary or multi-class class labels for new data.\"\"\"\n",
        "        if self.y.ndim == 1:  # Binary classification\n",
        "            return (self.predict_proba(X_new) >= 0.5).astype(int)\n",
        "        else:  # Multi-class classification\n",
        "            return np.argmax(self.predict_proba(X_new), axis=1)\n",
        "\n",
        "    def evaluate(self, X_test, y_test):\n",
        "        \"\"\"Evaluates the model on test data, returning accuracy and log loss.\"\"\"\n",
        "        y_pred_proba = self.predict_proba(X_test)\n",
        "        y_pred = self.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        log_loss_val = log_loss(y_test, y_pred_proba)\n",
        "        logger.info(f\"Evaluation completed: Accuracy = {accuracy:.4f}, Log Loss = {log_loss_val:.4f}\")\n",
        "        return accuracy, log_loss_val"
      ],
      "metadata": {
        "id": "l68oBoj-tjcM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constraint: Sum of the coefficients (excluding bias) must be <= 1\n",
        "def constraint(beta):\n",
        "    return np.sum(beta[:-1]) - 1  # Sum of all coefficients <= 1\n",
        "\n",
        "# Fit the model\n",
        "mle_model_lagrange = LogisticRegressionLagrange(X_train, y_train, constraints=[constraint])\n",
        "mle_model_lagrange.fit(solver='BFGS', regularization='L2', alpha=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_lagrange, log_loss_lagrange = mle_model_lagrange.evaluate(X_test, y_test)\n",
        "print(f\"Accuracy (Lagrange): {accuracy_lagrange:.4f}\")\n",
        "print(f\"Log Loss (Lagrange): {log_loss_lagrange:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAzyNjd4tjZ4",
        "outputId": "4c86dadb-2613-4e82-bb1c-8a849729b523"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Lagrange): 0.8400\n",
            "Log Loss (Lagrange): 0.3554\n"
          ]
        }
      ]
    }
  ]
}