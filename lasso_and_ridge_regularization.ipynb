{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train-Test Split"
      ],
      "metadata": {
        "id": "UFzbQaBp83in"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aV07aod5q4d",
        "outputId": "a855cb9d-4a3f-4caf-f78a-f3dbcbc7dd89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Ridge alpha: 4.843104261147604e-05\n",
            "Ridge Train MSE: 0.42419115731399054\n",
            "Ridge Test MSE: 0.39002514457300314\n",
            "\n",
            "Optimal Lasso alpha: 1.0\n",
            "Lasso Train MSE: 0.6505925698157706\n",
            "Lasso Test MSE: 0.6571600689645265\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Load the Wine Quality dataset from a URL\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "df = pd.read_csv(url, sep=\";\")\n",
        "\n",
        "# Split the data into features and target\n",
        "X = df.drop(\"quality\", axis=1).values\n",
        "y = df[\"quality\"].values\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the loss function for Ridge regression\n",
        "def ridge_loss(log_alpha, X, y):\n",
        "    alpha = np.exp(log_alpha).item()  # Convert log(alpha) back to alpha\n",
        "    model = Ridge(alpha=alpha)\n",
        "    model.fit(X, y)\n",
        "    y_pred = model.predict(X)\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "    penalty = alpha * np.sum(model.coef_ ** 2)\n",
        "    return mse + penalty\n",
        "\n",
        "# Define the loss function for Lasso regression\n",
        "def lasso_loss(log_alpha, X, y):\n",
        "    alpha = np.exp(log_alpha).item()  # Convert log(alpha) back to alpha\n",
        "    model = Lasso(alpha=alpha, max_iter=10000)\n",
        "    model.fit(X, y)\n",
        "    y_pred = model.predict(X)\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "    penalty = alpha * np.sum(np.abs(model.coef_))\n",
        "    return mse + penalty\n",
        "\n",
        "# Optimize alpha for Ridge\n",
        "ridge_result = minimize(ridge_loss, x0=np.log(1.0), args=(X_train, y_train), bounds=[(None, None)])\n",
        "optimal_alpha_ridge = np.exp(ridge_result.x[0])\n",
        "\n",
        "# Optimize alpha for Lasso\n",
        "lasso_result = minimize(lasso_loss, x0=np.log(1.0), args=(X_train, y_train), bounds=[(None, None)])\n",
        "optimal_alpha_lasso = np.exp(lasso_result.x[0])\n",
        "\n",
        "# Fit Ridge model with optimal alpha\n",
        "ridge_model = Ridge(alpha=optimal_alpha_ridge)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "ridge_train_mse = mean_squared_error(y_train, ridge_model.predict(X_train))\n",
        "ridge_test_mse = mean_squared_error(y_test, ridge_model.predict(X_test))\n",
        "\n",
        "# Fit Lasso model with optimal alpha\n",
        "lasso_model = Lasso(alpha=optimal_alpha_lasso, max_iter=10000)\n",
        "lasso_model.fit(X_train, y_train)\n",
        "lasso_train_mse = mean_squared_error(y_train, lasso_model.predict(X_train))\n",
        "lasso_test_mse = mean_squared_error(y_test, lasso_model.predict(X_test))\n",
        "\n",
        "# Print results\n",
        "print(f\"Optimal Ridge alpha: {optimal_alpha_ridge}\")\n",
        "print(f\"Ridge Train MSE: {ridge_train_mse}\")\n",
        "print(f\"Ridge Test MSE: {ridge_test_mse}\")\n",
        "print()\n",
        "print(f\"Optimal Lasso alpha: {optimal_alpha_lasso}\")\n",
        "print(f\"Lasso Train MSE: {lasso_train_mse}\")\n",
        "print(f\"Lasso Test MSE: {lasso_test_mse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Validated - Sklearn"
      ],
      "metadata": {
        "id": "92yckSZ686yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# Load the Wine Quality dataset from a URL\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "df = pd.read_csv(url, sep=\";\")\n",
        "\n",
        "# Split the data into features and target\n",
        "X = df.drop(\"quality\", axis=1).values\n",
        "y = df[\"quality\"].values\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Define the cross-validation function\n",
        "def cross_val_score_mse(model, X, y, cv=5):\n",
        "    \"\"\"\n",
        "    Perform K-Fold cross-validation and compute the mean squared error (MSE).\n",
        "    Args:\n",
        "        model: The regression model (Ridge or Lasso).\n",
        "        X (ndarray): Features.\n",
        "        y (ndarray): Target.\n",
        "        cv (int): Number of folds for cross-validation.\n",
        "    Returns:\n",
        "        float: Mean cross-validated MSE.\n",
        "    \"\"\"\n",
        "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
        "    mse_scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Fit the model and predict\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "\n",
        "        # Compute MSE\n",
        "        mse_scores.append(mean_squared_error(y_val, y_pred))\n",
        "\n",
        "    return np.mean(mse_scores)\n",
        "\n",
        "# Define the loss function for Ridge regression with cross-validation\n",
        "def ridge_loss(log_alpha, X, y, cv=5):\n",
        "    \"\"\"\n",
        "    Loss function for Ridge regression using log(alpha) for optimization.\n",
        "    Args:\n",
        "        log_alpha (float): Log of regularization parameter alpha.\n",
        "        X (ndarray): Features.\n",
        "        y (ndarray): Target.\n",
        "        cv (int): Number of folds for cross-validation.\n",
        "    Returns:\n",
        "        float: Cross-validated MSE.\n",
        "    \"\"\"\n",
        "    alpha = np.exp(log_alpha).item()  # Convert log(alpha) back to alpha\n",
        "    model = Ridge(alpha=alpha)\n",
        "    return cross_val_score_mse(model, X, y, cv)\n",
        "\n",
        "# Define the loss function for Lasso regression with cross-validation\n",
        "def lasso_loss(log_alpha, X, y, cv=5):\n",
        "    \"\"\"\n",
        "    Loss function for Lasso regression using log(alpha) for optimization.\n",
        "    Args:\n",
        "        log_alpha (float): Log of regularization parameter alpha.\n",
        "        X (ndarray): Features.\n",
        "        y (ndarray): Target.\n",
        "        cv (int): Number of folds for cross-validation.\n",
        "    Returns:\n",
        "        float: Cross-validated MSE.\n",
        "    \"\"\"\n",
        "    alpha = np.exp(log_alpha).item()  # Convert log(alpha) back to alpha\n",
        "    model = Lasso(alpha=alpha, max_iter=10000)\n",
        "    return cross_val_score_mse(model, X, y, cv)\n",
        "\n",
        "# Optimize alpha for Ridge\n",
        "ridge_result = minimize(ridge_loss, x0=np.log(1.0), args=(X, y), bounds=[(None, None)])\n",
        "optimal_alpha_ridge = np.exp(ridge_result.x[0])\n",
        "\n",
        "# Optimize alpha for Lasso\n",
        "lasso_result = minimize(lasso_loss, x0=np.log(1.0), args=(X, y), bounds=[(None, None)])\n",
        "optimal_alpha_lasso = np.exp(lasso_result.x[0])\n",
        "\n",
        "# Final models with optimal alphas\n",
        "ridge_model = Ridge(alpha=optimal_alpha_ridge)\n",
        "lasso_model = Lasso(alpha=optimal_alpha_lasso, max_iter=10000)\n",
        "\n",
        "# Fit the models on the full dataset\n",
        "ridge_model.fit(X, y)\n",
        "lasso_model.fit(X, y)\n",
        "\n",
        "# Compute cross-validated MSE for Ridge and Lasso\n",
        "cv_mse_ridge = cross_val_score_mse(ridge_model, X, y, cv=5)\n",
        "cv_mse_lasso = cross_val_score_mse(lasso_model, X, y, cv=5)\n",
        "\n",
        "# Print results\n",
        "print(f\"Optimal Ridge alpha: {optimal_alpha_ridge:.4f}\")\n",
        "print(f\"Cross-validated MSE for Ridge: {cv_mse_ridge:.4f}\")\n",
        "print(\"\\n\")\n",
        "print(\"Ridge Coefficients:\")\n",
        "print(ridge_model.coef_)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(f\"Optimal Lasso alpha: {optimal_alpha_lasso:.4f}\")\n",
        "print(f\"Cross-validated MSE for Lasso: {cv_mse_lasso:.4f}\")\n",
        "print(\"\\n\")\n",
        "print(\"Lasso Coefficients:\")\n",
        "print(lasso_model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wih4EQJJ5uSe",
        "outputId": "53ab92a3-dc7d-4f6b-aa77-85acc9473d69"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Ridge alpha: 83.8863\n",
            "Cross-validated MSE for Ridge: 0.4278\n",
            "\n",
            "\n",
            "Ridge Coefficients:\n",
            "[ 0.03526939 -0.15991515 -0.00133552  0.02022562 -0.08424231  0.0392259\n",
            " -0.10756028 -0.05997531 -0.07499151  0.15967092  0.25593338]\n",
            "\n",
            "\n",
            "Optimal Lasso alpha: 1.0000\n",
            "Cross-validated MSE for Lasso: 0.6523\n",
            "\n",
            "\n",
            "Lasso Coefficients:\n",
            "[ 0. -0.  0.  0. -0. -0. -0. -0. -0.  0.  0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Validated - Statsmodels"
      ],
      "metadata": {
        "id": "a7NMpLdJ9EdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.optimize import minimize\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Load the Wine Quality dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "df = pd.read_csv(url, sep=\";\")\n",
        "\n",
        "# Split the data into features and target\n",
        "X = df.drop(\"quality\", axis=1).values\n",
        "y = df[\"quality\"].values\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Add a constant for statsmodels\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Define the cross-validation function for Statsmodels\n",
        "def cross_val_score_mse_statsmodels(X, y, alpha, penalty, cv=5):\n",
        "    \"\"\"\n",
        "    Compute cross-validated MSE using Statsmodels with regularization.\n",
        "    Args:\n",
        "        X (ndarray): Features.\n",
        "        y (ndarray): Target.\n",
        "        alpha (float): Regularization parameter.\n",
        "        penalty (str): 'l1' for Lasso, 'l2' for Ridge.\n",
        "        cv (int): Number of folds for cross-validation.\n",
        "    Returns:\n",
        "        float: Mean cross-validated MSE.\n",
        "    \"\"\"\n",
        "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
        "    mse_scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Fit the regularized model\n",
        "        model = sm.OLS(y_train, X_train)\n",
        "        result = model.fit_regularized(alpha=alpha, L1_wt=1.0 if penalty == 'l1' else 0.0)\n",
        "\n",
        "        # Predict on validation set\n",
        "        y_pred = result.predict(X_val)\n",
        "\n",
        "        # Compute MSE\n",
        "        mse_scores.append(mean_squared_error(y_val, y_pred))\n",
        "\n",
        "    return np.mean(mse_scores)\n",
        "\n",
        "# Define the loss function for Ridge (L2 regularization)\n",
        "def ridge_loss(log_alpha, X, y, cv=5):\n",
        "    \"\"\"\n",
        "    Loss function for Ridge regularization.\n",
        "    Args:\n",
        "        log_alpha (float): Log of regularization parameter alpha.\n",
        "        X (ndarray): Features.\n",
        "        y (ndarray): Target.\n",
        "        cv (int): Number of folds for cross-validation.\n",
        "    Returns:\n",
        "        float: Cross-validated MSE.\n",
        "    \"\"\"\n",
        "    alpha = np.exp(log_alpha).item()  # Convert log(alpha) back to scalar\n",
        "    return cross_val_score_mse_statsmodels(X, y, alpha, penalty='l2', cv=cv)\n",
        "\n",
        "# Define the loss function for Lasso (L1 regularization)\n",
        "def lasso_loss(log_alpha, X, y, cv=5):\n",
        "    \"\"\"\n",
        "    Loss function for Lasso regularization.\n",
        "    Args:\n",
        "        log_alpha (float): Log of regularization parameter alpha.\n",
        "        X (ndarray): Features.\n",
        "        y (ndarray): Target.\n",
        "        cv (int): Number of folds for cross-validation.\n",
        "    Returns:\n",
        "        float: Cross-validated MSE.\n",
        "    \"\"\"\n",
        "    alpha = np.exp(log_alpha).item()  # Convert log(alpha) back to scalar\n",
        "    return cross_val_score_mse_statsmodels(X, y, alpha, penalty='l1', cv=cv)\n",
        "\n",
        "# Optimize alpha for Ridge\n",
        "ridge_result = minimize(ridge_loss, x0=np.log(1.0), args=(X, y), bounds=[(None, None)])\n",
        "optimal_alpha_ridge = np.exp(ridge_result.x[0])\n",
        "\n",
        "# Optimize alpha for Lasso\n",
        "lasso_result = minimize(lasso_loss, x0=np.log(1.0), args=(X, y), bounds=[(None, None)])\n",
        "optimal_alpha_lasso = np.exp(lasso_result.x[0])\n",
        "\n",
        "# Final Ridge and Lasso models\n",
        "ridge_model = sm.OLS(y, X).fit_regularized(alpha=optimal_alpha_ridge, L1_wt=0.0)\n",
        "lasso_model = sm.OLS(y, X).fit_regularized(alpha=optimal_alpha_lasso, L1_wt=1.0)\n",
        "\n",
        "# Compute cross-validated MSE for Ridge and Lasso\n",
        "cv_mse_ridge = cross_val_score_mse_statsmodels(X, y, alpha=optimal_alpha_ridge, penalty='l2', cv=5)\n",
        "cv_mse_lasso = cross_val_score_mse_statsmodels(X, y, alpha=optimal_alpha_lasso, penalty='l1', cv=5)\n",
        "\n",
        "# Print results\n",
        "print(f\"Optimal Ridge alpha: {optimal_alpha_ridge:.4f}\")\n",
        "print(f\"Cross-validated MSE for Ridge: {cv_mse_ridge:.4f}\")\n",
        "print(\"\\n\")\n",
        "print(\"Ridge Coefficients:\")\n",
        "print(ridge_model.params)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(f\"Optimal Lasso alpha: {optimal_alpha_lasso:.4f}\")\n",
        "print(f\"Cross-validated MSE for Lasso: {cv_mse_lasso:.4f}\")\n",
        "print(\"\\n\")\n",
        "print(\"Lasso Coefficients:\")\n",
        "print(lasso_model.params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx3wHkql7MCL",
        "outputId": "56428f45-8e2e-4af9-dd0e-b4be82051510"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Ridge alpha: 0.0005\n",
            "Cross-validated MSE for Ridge: 0.4288\n",
            "\n",
            "\n",
            "Ridge Coefficients:\n",
            "[ 5.63320946  0.04372845 -0.19382464 -0.03533395  0.02310372 -0.08814925\n",
            "  0.04552411 -0.10728497 -0.03407612 -0.0635953   0.15524334  0.29391203]\n",
            "\n",
            "\n",
            "Optimal Lasso alpha: 0.0093\n",
            "Cross-validated MSE for Lasso: 0.4310\n",
            "\n",
            "\n",
            "Lasso Coefficients:\n",
            "[ 5.62676592  0.05021811 -0.18819774  0.          0.00817306 -0.06915114\n",
            "  0.01678569 -0.07535471 -0.02397234  0.          0.14025734  0.28519736]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-Validated Elastic Net"
      ],
      "metadata": {
        "id": "3lbFyD989ykL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.optimize import minimize\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Load the Wine Quality dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "df = pd.read_csv(url, sep=\";\")\n",
        "\n",
        "# Split the data into features and target\n",
        "X = df.drop(\"quality\", axis=1).values\n",
        "y = df[\"quality\"].values\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Add a constant for statsmodels\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Define the cross-validation function for Statsmodels\n",
        "def cross_val_score_mse_statsmodels(X, y, alpha, L1_wt, cv=5):\n",
        "    \"\"\"\n",
        "    Compute cross-validated MSE using Statsmodels with regularization.\n",
        "    Args:\n",
        "        X (ndarray): Features.\n",
        "        y (ndarray): Target.\n",
        "        alpha (float): Regularization parameter.\n",
        "        L1_wt (float): Weight for L1 regularization (0.0 = Ridge, 1.0 = Lasso).\n",
        "        cv (int): Number of folds for cross-validation.\n",
        "    Returns:\n",
        "        float: Mean cross-validated MSE.\n",
        "    \"\"\"\n",
        "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
        "    mse_scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        X_train, X_val = X[train_idx], X[val_idx]\n",
        "        y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "        # Fit the regularized model\n",
        "        model = sm.OLS(y_train, X_train)\n",
        "        result = model.fit_regularized(alpha=alpha, L1_wt=L1_wt)\n",
        "\n",
        "        # Predict on validation set\n",
        "        y_pred = result.predict(X_val)\n",
        "\n",
        "        # Compute MSE\n",
        "        mse_scores.append(mean_squared_error(y_val, y_pred))\n",
        "\n",
        "    return np.mean(mse_scores)\n",
        "\n",
        "# Define the loss function for optimizing alpha and L1_wt\n",
        "def elastic_net_loss(params, X, y, cv=5):\n",
        "    \"\"\"\n",
        "    Loss function for Elastic Net regularization.\n",
        "    Args:\n",
        "        params (ndarray): [log_alpha, L1_wt] where:\n",
        "            log_alpha: Logarithm of regularization parameter alpha.\n",
        "            L1_wt: Weight for L1 regularization (0.0 to 1.0).\n",
        "        X (ndarray): Features.\n",
        "        y (ndarray): Target.\n",
        "        cv (int): Number of folds for cross-validation.\n",
        "    Returns:\n",
        "        float: Cross-validated MSE.\n",
        "    \"\"\"\n",
        "    log_alpha, L1_wt = params\n",
        "    alpha = np.exp(log_alpha)  # Convert log_alpha back to alpha\n",
        "    L1_wt = np.clip(L1_wt, 0, 1)  # Ensure L1_wt is between 0 and 1\n",
        "    return cross_val_score_mse_statsmodels(X, y, alpha, L1_wt, cv)\n",
        "\n",
        "# Initial guess for log_alpha and L1_wt\n",
        "initial_params = [np.log(1.0), 0.5]  # Start with alpha=1.0 and L1_wt=0.5\n",
        "\n",
        "# Optimize log_alpha and L1_wt\n",
        "result = minimize(elastic_net_loss, x0=initial_params, args=(X, y), bounds=[(None, None), (0, 1)])\n",
        "optimal_log_alpha, optimal_L1_wt = result.x\n",
        "optimal_alpha = np.exp(optimal_log_alpha)\n",
        "\n",
        "# Fit final model with optimal parameters\n",
        "final_model = sm.OLS(y, X).fit_regularized(alpha=optimal_alpha, L1_wt=optimal_L1_wt)\n",
        "\n",
        "# Compute cross-validated MSE for the final model\n",
        "cv_mse_final = cross_val_score_mse_statsmodels(X, y, alpha=optimal_alpha, L1_wt=optimal_L1_wt, cv=5)\n",
        "\n",
        "# Print results\n",
        "print(f\"Optimal alpha: {optimal_alpha:.4f}\")\n",
        "print(f\"Optimal L1_wt: {optimal_L1_wt:.4f}\")\n",
        "print(f\"Cross-validated MSE for Elastic Net: {cv_mse_final:.4f}\")\n",
        "print(\"\\nElastic Net Coefficients:\")\n",
        "print(final_model.params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYtbPGRO6r3V",
        "outputId": "8cc668dd-af57-4b01-f101-cf55e23e7e2a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal alpha: 0.0221\n",
            "Optimal L1_wt: 1.0000\n",
            "Cross-validated MSE for Elastic Net: 0.4309\n",
            "\n",
            "Elastic Net Coefficients:\n",
            "[ 5.61390416  0.02424287 -0.18927093  0.          0.         -0.05025426\n",
            "  0.         -0.05279259  0.          0.          0.12057652  0.29225094]\n"
          ]
        }
      ]
    }
  ]
}