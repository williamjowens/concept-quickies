{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm"
      ],
      "metadata": {
        "id": "HBXsOs2JL1lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import LinAlgError, norm\n",
        "from scipy.linalg import qr, lstsq\n",
        "from scipy.optimize import OptimizeResult\n",
        "\n",
        "class _GRGOptimizer:\n",
        "    \"\"\"\n",
        "    Generalized Reduced Gradient (GRG) Optimizer\n",
        "\n",
        "    Implements:\n",
        "      - Slack variables for 'ineq' constraints at initialization.\n",
        "      - Two-phase approach: feasibility, then objective.\n",
        "      - DFP updates for Hessian after enough feasible steps.\n",
        "      - A More–Thuente-like line search:\n",
        "        * Attempt bracket expansion to find an interval [alpha_lo, alpha_hi]\n",
        "          that presumably contains a local minimizer along the search direction.\n",
        "        * Once bracketed, call a 'zoom' routine that repeatedly tries a\n",
        "          cubic (or quadratic) interpolation to pick alpha in (alpha_lo, alpha_hi).\n",
        "          If alpha is too close to boundaries, we fallback to bisection = mid.\n",
        "          If we fail to converge after many zoom steps, fallback to minimal step.\n",
        "        * Enforces interval reduction factor delta in the bracket.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        fun,\n",
        "        x0,\n",
        "        args=(),\n",
        "        jac=None,\n",
        "        constraints=(),\n",
        "        bounds=None,\n",
        "        tol=1e-6,\n",
        "        maxiter=100,\n",
        "        verbose=False,\n",
        "        callback=None\n",
        "    ):\n",
        "        self.fun = fun\n",
        "        self.args = args\n",
        "        self.jac = jac\n",
        "        self.constraints = constraints\n",
        "        self.tol = tol\n",
        "        self.maxiter = maxiter\n",
        "        self.verbose = verbose\n",
        "        self.callback = callback\n",
        "\n",
        "        # separate eq vs ineq constraints\n",
        "        self.g_eq = []\n",
        "        self.eq_jacs = []\n",
        "        self.g_ineq = []\n",
        "        self.ineq_jacs = []\n",
        "\n",
        "        for c in constraints:\n",
        "            ctype = c.get('type', None)\n",
        "            cfun = c['fun']\n",
        "            cjac = c.get('jac', None)\n",
        "            if ctype == 'eq':\n",
        "                self.g_eq.append(cfun)\n",
        "                self.eq_jacs.append(cjac)\n",
        "            elif ctype == 'ineq':\n",
        "                self.g_ineq.append(cfun)\n",
        "                self.ineq_jacs.append(cjac)\n",
        "            else:\n",
        "                raise ValueError(\"Constraint must have 'eq' or 'ineq' type.\")\n",
        "\n",
        "        self.n_orig = len(x0)\n",
        "        if bounds is None:\n",
        "            bounds = [(None, None)] * self.n_orig\n",
        "        else:\n",
        "            if len(bounds) < self.n_orig:\n",
        "                bounds += [(None, None)]*(self.n_orig - len(bounds))\n",
        "\n",
        "        x0 = np.array(x0, dtype=float)\n",
        "        self.m_ineq = len(self.g_ineq)\n",
        "\n",
        "        # Introduce slack variables for ineq => g_ineq(x)+s_i=0 => s_i >=0\n",
        "        slack_init = []\n",
        "        slack_bounds = []\n",
        "        for i, gf in enumerate(self.g_ineq):\n",
        "            val = gf(x0, *self.args)\n",
        "            s_i = max(0.0, -val)\n",
        "            slack_init.append(s_i)\n",
        "            slack_bounds.append((0.0, None))\n",
        "\n",
        "        self.x = np.concatenate([x0, slack_init])\n",
        "        self.bounds = bounds + slack_bounds\n",
        "        self.slack_map = {i: self.n_orig + i for i in range(self.m_ineq)}\n",
        "        self.n = self.n_orig + self.m_ineq\n",
        "\n",
        "        self.H = None\n",
        "        self.nfev = 0\n",
        "        self.njev = 0\n",
        "\n",
        "        # feasibility\n",
        "        self.feas_phase = not self.is_feasible(self.x)\n",
        "        self.feas_iter_count = 0\n",
        "        self.feasible_steps = 0\n",
        "        self.dfp_start_steps = 5\n",
        "\n",
        "        # track line search difficulty\n",
        "        self.ls_failures = 0\n",
        "\n",
        "    def _print(self, *args):\n",
        "        if self.verbose:\n",
        "            print(*args)\n",
        "\n",
        "    def call_fun(self, x):\n",
        "        \"\"\" Evaluate objective on original dimension. \"\"\"\n",
        "        self.nfev += 1\n",
        "        return self.fun(x[:self.n_orig], *self.args)\n",
        "\n",
        "    def call_jac(self, x):\n",
        "        self.njev += 1\n",
        "        return self.jac(x[:self.n_orig], *self.args)\n",
        "\n",
        "    def eval_grad_obj(self, x, eps=1e-8):\n",
        "        \"\"\" If jac is not None => use it, else FD. Slack grad = 0. \"\"\"\n",
        "        if self.jac is not None:\n",
        "            try:\n",
        "                graw = self.call_jac(x)\n",
        "                gval = np.zeros(self.n)\n",
        "                gval[:self.n_orig] = graw\n",
        "                return gval\n",
        "            except:\n",
        "                pass\n",
        "        f0 = self.call_fun(x)\n",
        "        gval = np.zeros(self.n)\n",
        "        for i in range(self.n_orig):\n",
        "            x_f = x.copy()\n",
        "            x_f[i]+= eps\n",
        "            f1 = self.call_fun(x_f)\n",
        "            gval[i] = (f1 - f0)/eps\n",
        "        return gval\n",
        "\n",
        "    def eval_constraints(self, x):\n",
        "        \"\"\" Return eqvals, ineqvals ignoring slack. \"\"\"\n",
        "        eqvals = [f(x[:self.n_orig], *self.args) for f in self.g_eq]\n",
        "        invals= [f(x[:self.n_orig], *self.args) for f in self.g_ineq]\n",
        "        return eqvals, invals\n",
        "\n",
        "    def feasibility_violation(self, x):\n",
        "        eqvals, invals = self.eval_constraints(x)\n",
        "        eq_viol = sum(abs(v) for v in eqvals)\n",
        "\n",
        "        bviol=0.0\n",
        "        for i, (lb, ub) in enumerate(self.bounds):\n",
        "            val = x[i]\n",
        "            if lb is not None and val<lb-self.tol:\n",
        "                bviol+=(lb-val)\n",
        "            if ub is not None and val>ub+self.tol:\n",
        "                bviol+=(val-ub)\n",
        "        return eq_viol + bviol\n",
        "\n",
        "    def is_feasible(self, x):\n",
        "        return self.feasibility_violation(x)<= self.tol\n",
        "\n",
        "    def identify_active_constraints(self, x):\n",
        "        eqvals, invals= self.eval_constraints(x)\n",
        "        active=[]\n",
        "        for i,v in enumerate(eqvals):\n",
        "            active.append(('eq', i))\n",
        "        for i,v in enumerate(invals):\n",
        "            if abs(v)<= self.tol:\n",
        "                active.append(('ineq', i))\n",
        "        return active\n",
        "\n",
        "    def _eval_jac_eq(self, x, eps=1e-8):\n",
        "        meq = len(self.g_eq)\n",
        "        J = np.zeros((meq, self.n))\n",
        "        for i, gf in enumerate(self.g_eq):\n",
        "            jf = self.eq_jacs[i]\n",
        "            if jf is not None:\n",
        "                grad = jf(x[:self.n_orig], *self.args)\n",
        "                J[i,:self.n_orig]= grad\n",
        "            else:\n",
        "                base= gf(x[:self.n_orig], *self.args)\n",
        "                for jj in range(self.n_orig):\n",
        "                    xx = x.copy()\n",
        "                    xx[jj]+= eps\n",
        "                    f1 = gf(xx[:self.n_orig], *self.args)\n",
        "                    J[i,jj] = (f1-base)/ eps\n",
        "        return J\n",
        "\n",
        "    def _eval_jac_ineq(self, x, eps=1e-8):\n",
        "        minq = len(self.g_ineq)\n",
        "        J= np.zeros((minq, self.n))\n",
        "        for i, gf in enumerate(self.g_ineq):\n",
        "            jf = self.ineq_jacs[i]\n",
        "            if jf is not None:\n",
        "                grad = jf(x[:self.n_orig], *self.args)\n",
        "                J[i,:self.n_orig] = grad\n",
        "            else:\n",
        "                base = gf(x[:self.n_orig], *self.args)\n",
        "                for jj in range(self.n_orig):\n",
        "                    xx= x.copy()\n",
        "                    xx[jj]+= eps\n",
        "                    f1= gf(xx[:self.n_orig], *self.args)\n",
        "                    J[i,jj] = (f1-base)/ eps\n",
        "            # slack\n",
        "            s_idx = self.slack_map[i]\n",
        "            J[i, s_idx] = 1.0\n",
        "        return J\n",
        "\n",
        "    def form_jacobian_active(self, x, active):\n",
        "        eqJ= self._eval_jac_eq(x)\n",
        "        inJ= self._eval_jac_ineq(x)\n",
        "        m= len(active)\n",
        "        A= np.zeros((m, self.n))\n",
        "        row=0\n",
        "        for (ctype, i) in active:\n",
        "            if ctype=='eq':\n",
        "                A[row,:] = eqJ[i,:]\n",
        "            else:\n",
        "                A[row,:] = inJ[i,:]\n",
        "            row+=1\n",
        "        return A\n",
        "\n",
        "    def is_bound_active(self, x, idx):\n",
        "        lb, ub= self.bounds[idx]\n",
        "        val = x[idx]\n",
        "        if lb is not None and abs(val-lb)< self.tol:\n",
        "            return True\n",
        "        if ub is not None and abs(val-ub)< self.tol:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def partition_variables(self, x, active, J):\n",
        "        \"\"\"\n",
        "        rank reveal => dep, indep. Avoid bound-active if possible\n",
        "        \"\"\"\n",
        "        if J.size==0 or len(active)==0:\n",
        "            return np.array([],dtype=int), np.arange(self.n, dtype=int)\n",
        "        Q, R, pivot = qr(J, mode='economic', pivoting=True)\n",
        "        rank = np.linalg.matrix_rank(J)\n",
        "        dep_i = pivot[:rank]\n",
        "        for i, di in enumerate(dep_i):\n",
        "            if self.is_bound_active(x, di):\n",
        "                candidate=None\n",
        "                for c in pivot[rank:]:\n",
        "                    if not self.is_bound_active(x, c):\n",
        "                        candidate=c\n",
        "                        break\n",
        "                if candidate is not None:\n",
        "                    dep_i[i] = candidate\n",
        "        dep_i= np.unique(dep_i)\n",
        "        all_i= np.arange(self.n)\n",
        "        indep_i= np.array([ii for ii in all_i if ii not in dep_i])\n",
        "        return dep_i, indep_i\n",
        "\n",
        "    def compute_reduced_gradient(self, x, active, J):\n",
        "        gf = self.eval_grad_obj(x)\n",
        "        if len(active)==0:\n",
        "            return gf, gf\n",
        "        JT= J.T\n",
        "        try:\n",
        "            lam, _, _, _= lstsq(JT, -gf, lapack_driver='gelsy')\n",
        "        except LinAlgError:\n",
        "            lam= np.zeros(JT.shape[1])\n",
        "        red = gf + JT@ lam\n",
        "        return red, gf\n",
        "\n",
        "    def newton_raphson(self, x, active, dep_indices, max_iter=50):\n",
        "        \"\"\"\n",
        "        Solve constraints => B dy=-r. Then bound fix.\n",
        "        Return (x, success).\n",
        "        \"\"\"\n",
        "        for it in range(max_iter):\n",
        "            eqvals, invals= self.eval_constraints(x)\n",
        "            res=[]\n",
        "            for (ctype, i) in active:\n",
        "                if ctype=='eq':\n",
        "                    val= self.g_eq[i](x[:self.n_orig], *self.args)\n",
        "                    res.append(val)\n",
        "                else:\n",
        "                    s_idx= self.slack_map[i]\n",
        "                    val= self.g_ineq[i](x[:self.n_orig], *self.args)+ x[s_idx]\n",
        "                    res.append(val)\n",
        "            rr= np.array(res)\n",
        "            if norm(rr)< self.tol:\n",
        "                return x, True\n",
        "            J= self.form_jacobian_active(x, active)\n",
        "            if J.size==0:\n",
        "                return x, True\n",
        "            B= J[:,dep_indices]\n",
        "            try:\n",
        "                dy, _, rnk, _= lstsq(B, -rr, lapack_driver='gelsy')\n",
        "                if rnk< len(dep_indices):\n",
        "                    self._print(\"[DEBUG] NewtonR => rank deficiency.\")\n",
        "                    return x, False\n",
        "                x[dep_indices]+= dy\n",
        "            except LinAlgError:\n",
        "                self._print(\"[DEBUG] NewtonR => LinAlg error.\")\n",
        "                return x, False\n",
        "            # bound fix\n",
        "            for ii,(lb, ub) in enumerate(self.bounds):\n",
        "                if lb is not None and x[ii]< lb:\n",
        "                    x[ii]= lb\n",
        "                if ub is not None and x[ii]> ub:\n",
        "                    x[ii]= ub\n",
        "        self._print(\"[DEBUG] NewtonR => no converge => max_iter\")\n",
        "        return x, False\n",
        "\n",
        "    def dfp_update(self, H, s, y):\n",
        "        sy= s@y\n",
        "        yHy= y @H @y\n",
        "        if abs(sy)<1e-14 or abs(yHy)<1e-14:\n",
        "            self._print(\"[DEBUG] skip DFP => small denom.\")\n",
        "            return H\n",
        "        s_sT= np.outer(s, s)\n",
        "        Hy= H@ y\n",
        "        return H + (s_sT/sy) - (np.outer(Hy,Hy)/yHy)\n",
        "\n",
        "    def minimal_step_attempt(self, x, d, active, dep, indep):\n",
        "        alpha=1e-6\n",
        "        xx= x.copy()\n",
        "        xx[indep]+= alpha*d[indep]\n",
        "        xx,nok= self.newton_raphson(xx, active, dep)\n",
        "        if nok:\n",
        "            oldf= self.call_fun(x)\n",
        "            newf= self.call_fun(xx)\n",
        "            if self.is_feasible(xx) or newf< oldf:\n",
        "                self._print(\"[DEBUG] minimal step => success alpha=\", alpha)\n",
        "                return alpha, 'success', xx\n",
        "        self._print(\"[DEBUG] minimal step => no improve.\")\n",
        "        return alpha, 'no_improve', x\n",
        "\n",
        "    def _compute_feas_dir(self, x):\n",
        "        eqvals, invals= self.eval_constraints(x)\n",
        "        grad_viol= np.zeros(self.n)\n",
        "        eqJ= self._eval_jac_eq(x)\n",
        "        for i,val in enumerate(eqvals):\n",
        "            if abs(val)> self.tol:\n",
        "                sgn= 1.0 if val>0 else -1.0\n",
        "                grad_viol[:self.n_orig]+= sgn* eqJ[i,:self.n_orig]\n",
        "        # bound\n",
        "        for i,(lb, ub) in enumerate(self.bounds):\n",
        "            val= x[i]\n",
        "            if lb is not None and val<(lb-self.tol):\n",
        "                grad_viol[i]-=1.0\n",
        "            if ub is not None and val>(ub+self.tol):\n",
        "                grad_viol[i]+=1.0\n",
        "        if norm(grad_viol)<1e-14:\n",
        "            return -grad_viol\n",
        "        return -grad_viol\n",
        "\n",
        "    def _cubic_interpolate(self, al, fl, dl, ah, fh, dh):\n",
        "        \"\"\"\n",
        "        Standard More–Thuente style cubic interpolation.\n",
        "        clamp alpha in [0.1*al+0.9*ah, 0.9*al+0.1*ah].\n",
        "        \"\"\"\n",
        "        d1= dl+ dh - 3*(fl- fh)/(al- ah)\n",
        "        d2sq= d1*d1 - dl*dh\n",
        "        if d2sq<0:\n",
        "            self._print(\"[DEBUG] cubic => negative discriminant => mid.\")\n",
        "            return 0.5*(al+ ah)\n",
        "        d2= np.sqrt(d2sq)\n",
        "        # sign depends on al<ah or not\n",
        "        if ah< al:\n",
        "            d2= -d2\n",
        "        alpha_c = ah - ( (ah-al)*( dh+ d2 - d1 ) / (dh- dl+ 2*d2 ) )\n",
        "        # safeguard\n",
        "        lower= 0.1* al + 0.9* ah\n",
        "        upper= 0.9* al + 0.1* ah\n",
        "        if lower> upper:\n",
        "            lower, upper= upper, lower\n",
        "        alpha_c = max(min(alpha_c, upper), lower)\n",
        "        self._print(f\"[DEBUG] cubic => alpha_c={alpha_c}\")\n",
        "        return alpha_c\n",
        "\n",
        "    def _bisect(self, al, ah):\n",
        "        return 0.5*(al+ ah)\n",
        "\n",
        "    def line_search(self, x, d, active, dep, indep):\n",
        "        \"\"\"\n",
        "        Attempt bracket then zoom. If repeated fails => minimal step fallback.\n",
        "        \"\"\"\n",
        "        tries= 2\n",
        "        for attempt in range(1, tries+1):\n",
        "            alpha, st, xnew= self._mt_search(x, d, active, dep, indep)\n",
        "            self._print(f\"[DEBUG] line_search attempt={attempt}, status={st}, alpha={alpha}\")\n",
        "            if st=='success':\n",
        "                return alpha, st, xnew\n",
        "            if st in ['nr_fail','not_descent','max_ls_iter']:\n",
        "                return alpha, st, xnew\n",
        "            # st='no_improve' => repeat\n",
        "        # fallback => minimal\n",
        "        alpha_ms, st_ms, x_ms= self.minimal_step_attempt(x, d, active, dep, indep)\n",
        "        return alpha_ms, st_ms, x_ms\n",
        "\n",
        "    def _mt_search(self, x, d, active, dep, indep):\n",
        "        \"\"\"\n",
        "        Entire More–Thuente: bracket => zoom.\n",
        "        We'll set c1=1e-4, c2=0.9, xtrapf=2.0 expansions\n",
        "        delta=0.66 for forced bisection\n",
        "        \"\"\"\n",
        "        c1=1e-4\n",
        "        c2=0.9\n",
        "        delta=0.66\n",
        "        xtrapf=2.0\n",
        "        f0= self.call_fun(x)\n",
        "        g0= self.eval_grad_obj(x)\n",
        "        dd0= g0@ d\n",
        "        if dd0>=0:\n",
        "            self._print(\"[DEBUG] not descent => dd0=\", dd0)\n",
        "            return 0.0, 'not_descent', x\n",
        "\n",
        "        # Try alpha=1 => if NR fail => keep halving\n",
        "        alpha=1.0\n",
        "        fa, dda, xa= self._eval_alpha(x, alpha, d, active, dep, indep)\n",
        "        if fa is None:\n",
        "            for rep in range(5):\n",
        "                alpha*=0.5\n",
        "                fa, dda, xa= self._eval_alpha(x, alpha, d, active, dep, indep)\n",
        "                if fa is not None:\n",
        "                    break\n",
        "            else:\n",
        "                return alpha, 'nr_fail', x\n",
        "        if fa is None:\n",
        "            return alpha, 'nr_fail', x\n",
        "\n",
        "        # check quick success\n",
        "        if fa<= f0 + c1*alpha*dd0 and abs(dda)<= c2*abs(dd0):\n",
        "            self._print(f\"[DEBUG] immediate strong wolfe => alpha={alpha}\")\n",
        "            return alpha, 'success', xa\n",
        "\n",
        "        # bracket expand\n",
        "        alpha_lo= 0.0\n",
        "        f_lo= f0\n",
        "        dd_lo= dd0\n",
        "        alpha_hi= None\n",
        "        f_hi=None\n",
        "        dd_hi=None\n",
        "\n",
        "        prev_alpha= alpha_lo\n",
        "        prev_f= f_lo\n",
        "        prev_dd= dd_lo\n",
        "        max_br= 10\n",
        "        self._print(\"[DEBUG] bracket expansion phase.\")\n",
        "        for ib in range(1, max_br+1):\n",
        "            # check conditions\n",
        "            if (fa> f0 + c1*alpha*dd0) or (ib>1 and fa>= prev_f):\n",
        "                alpha_hi= alpha\n",
        "                f_hi= fa\n",
        "                dd_hi= dda\n",
        "                break\n",
        "            if abs(dda)<= c2*abs(dd0):\n",
        "                return alpha, 'success', xa\n",
        "            if dda>=0:\n",
        "                alpha_hi= alpha\n",
        "                f_hi= fa\n",
        "                dd_hi= dda\n",
        "                break\n",
        "            # else update and expand\n",
        "            prev_alpha= alpha\n",
        "            prev_f= fa\n",
        "            prev_dd= dda\n",
        "            alpha*= xtrapf\n",
        "            fa, dda, xa= self._eval_alpha(x, alpha, d, active, dep, indep)\n",
        "            self._print(f\"[DEBUG] bracket iter={ib}, alpha={alpha}, f={fa}, dd={dda}\")\n",
        "            if fa is None:\n",
        "                return alpha, 'nr_fail', x\n",
        "        else:\n",
        "            # never bracket => no_improve\n",
        "            self._print(\"[DEBUG] bracket => no bracket => no_improve.\")\n",
        "            return alpha, 'no_improve', xa\n",
        "\n",
        "        if alpha_hi is None:\n",
        "            # no bracket => no_improve\n",
        "            return alpha, 'no_improve', xa\n",
        "\n",
        "        if ib==1 and alpha_hi== alpha:\n",
        "            # set lo => previous\n",
        "            alpha_lo= prev_alpha\n",
        "            f_lo= prev_f\n",
        "            dd_lo= prev_dd\n",
        "        else:\n",
        "            alpha_lo= prev_alpha\n",
        "            f_lo= prev_f\n",
        "            dd_lo= prev_dd\n",
        "\n",
        "        # now zoom\n",
        "        return self._zoom(x, d, active, dep, indep,\n",
        "                          alpha_lo, alpha_hi,\n",
        "                          f_lo, f_hi,\n",
        "                          dd_lo, dd_hi,\n",
        "                          f0, dd0,\n",
        "                          c1, c2,\n",
        "                          delta)\n",
        "\n",
        "    def _zoom(self, x, d, active, dep, indep,\n",
        "              alpha_lo, alpha_hi,\n",
        "              f_lo, f_hi,\n",
        "              dd_lo, dd_hi,\n",
        "              f0, dd0,\n",
        "              c1, c2,\n",
        "              delta):\n",
        "        max_zoom=15\n",
        "        old_width= abs(alpha_hi- alpha_lo)\n",
        "        self._print(f\"[DEBUG] zoom => [lo={alpha_lo}, hi={alpha_hi}]\")\n",
        "        for iz in range(1, max_zoom+1):\n",
        "            width= abs(alpha_hi- alpha_lo)\n",
        "            if width<= 1e-14:\n",
        "                self._print(\"[DEBUG] zoom => bracket collapsed => no_improve.\")\n",
        "                return alpha_lo, 'no_improve', x\n",
        "            # if width >= delta*old_width => do bisection\n",
        "            if width>= delta* old_width:\n",
        "                alpha_j= self._bisect(alpha_lo, alpha_hi)\n",
        "                self._print(\"[DEBUG] no sufficient interval shrink => bisection => alpha_j=\", alpha_j)\n",
        "            else:\n",
        "                # do cubic\n",
        "                alpha_j= self._cubic_interpolate(alpha_lo, f_lo, dd_lo, alpha_hi, f_hi, dd_hi)\n",
        "                # check if too close => bisection\n",
        "                eps_small= 1e-6\n",
        "                if (abs(alpha_j-alpha_lo)< eps_small*width) or (abs(alpha_j-alpha_hi)< eps_small*width):\n",
        "                    self._print(\"[DEBUG] alpha too close => bisection.\")\n",
        "                    alpha_j= self._bisect(alpha_lo, alpha_hi)\n",
        "\n",
        "            old_width= width\n",
        "            fj, ddj, xj= self._eval_alpha(x, alpha_j, d, active, dep, indep)\n",
        "            self._print(f\"[DEBUG] zoom iter={iz}, alpha_j={alpha_j}, f={fj}, dd={ddj}\")\n",
        "\n",
        "            if fj is None:\n",
        "                # fallback => half alpha_j\n",
        "                alpha_j*=0.5\n",
        "                fj, ddj, xj= self._eval_alpha(x, alpha_j, d, active, dep, indep)\n",
        "                if fj is None:\n",
        "                    self._print(\"[DEBUG] zoom => repeated NR fail => stop.\")\n",
        "                    return alpha_j, 'nr_fail', x\n",
        "\n",
        "            # Armijo check\n",
        "            if (fj> f0 + c1* alpha_j*dd0) or (fj>= f_lo):\n",
        "                alpha_hi= alpha_j\n",
        "                f_hi= fj\n",
        "                dd_hi= ddj\n",
        "            else:\n",
        "                # check curvature\n",
        "                if abs(ddj)<= c2* abs(dd0):\n",
        "                    self._print(f\"[DEBUG] zoom => success => alpha={alpha_j}\")\n",
        "                    return alpha_j, 'success', xj\n",
        "                if ddj*(alpha_hi - alpha_lo)>=0:\n",
        "                    alpha_hi= alpha_lo\n",
        "                    f_hi= f_lo\n",
        "                    dd_hi= dd_lo\n",
        "                alpha_lo= alpha_j\n",
        "                f_lo= fj\n",
        "                dd_lo= ddj\n",
        "        self._print(\"[DEBUG] zoom => max_zoom => no_improve.\")\n",
        "        return alpha_lo, 'no_improve', x\n",
        "\n",
        "    def _eval_alpha(self, x, alpha, d, active, dep, indep):\n",
        "        \"\"\"\n",
        "        x + alpha d => newton => feasible? => compute f, dd.\n",
        "        If newton fail => None\n",
        "        \"\"\"\n",
        "        x_trial= x.copy()\n",
        "        x_trial[indep]+= alpha*d[indep]\n",
        "        x_nr, ok= self.newton_raphson(x_trial, active, dep)\n",
        "        if not ok:\n",
        "            return None, None, x_trial\n",
        "        fv= self.call_fun(x_nr)\n",
        "        gv= self.eval_grad_obj(x_nr)\n",
        "        dd= gv@ d\n",
        "        return fv, dd, x_nr\n",
        "\n",
        "    def check_kkt(self, red_grad):\n",
        "        return norm(red_grad)< self.tol\n",
        "\n",
        "    def run(self):\n",
        "        x= self.x\n",
        "        self.H= np.eye(self.n)\n",
        "        success=False\n",
        "        message=\"\"\n",
        "\n",
        "        for it in range(1, self.maxiter+1):\n",
        "            self._print(f\"\\n--- Iteration {it} ---\")\n",
        "\n",
        "            if self.feas_phase:\n",
        "                self.feas_iter_count+=1\n",
        "                if self.feas_iter_count>50:\n",
        "                    message=\"Feasibility not met in 50 steps.\"\n",
        "                    success=False\n",
        "                    break\n",
        "\n",
        "            active= self.identify_active_constraints(x)\n",
        "            J= self.form_jacobian_active(x, active)\n",
        "            dep, indep= self.partition_variables(x, active, J)\n",
        "\n",
        "            if self.feas_phase and self.is_feasible(x):\n",
        "                self.feas_phase= False\n",
        "                self.feasible_steps=0\n",
        "                self.H= np.eye(self.n)\n",
        "                self._print(\"[DEBUG] transition => objective phase\")\n",
        "\n",
        "            if not self.feas_phase:\n",
        "                redg, gf= self.compute_reduced_gradient(x, active, J)\n",
        "                rg_n= norm(redg)\n",
        "                self._print(f\"[DEBUG] reduced grad norm={rg_n}\")\n",
        "                if self.check_kkt(redg):\n",
        "                    message= \"KKT => done\"\n",
        "                    success=True\n",
        "                    break\n",
        "                use_dfp= (self.feasible_steps>= self.dfp_start_steps)\n",
        "                if use_dfp:\n",
        "                    if self.H is None or self.H.shape[0]!= self.n:\n",
        "                        self.H= np.eye(self.n)\n",
        "                    d= -self.H@ redg\n",
        "                    self._print(\"[DEBUG] using DFP direction.\")\n",
        "                else:\n",
        "                    d= -gf\n",
        "                    self._print(\"[DEBUG] using steepest direction.\")\n",
        "            else:\n",
        "                self._print(\"[DEBUG] feasibility => reduce violation.\")\n",
        "                d= self._compute_feas_dir(x)\n",
        "                success=False\n",
        "                message=\"\"\n",
        "\n",
        "            alpha, st, xnew= self.line_search(x, d, active, dep, indep)\n",
        "            self._print(f\"[DEBUG] line search => alpha={alpha}, status={st}\")\n",
        "            if st in ['not_descent','nr_fail','no_improve','max_ls_iter']:\n",
        "                message= f\"line search => {st}\"\n",
        "                success=False\n",
        "                break\n",
        "\n",
        "            if not self.feas_phase:\n",
        "                if self.is_feasible(xnew):\n",
        "                    self.feasible_steps+=1\n",
        "                if self.feasible_steps>= self.dfp_start_steps:\n",
        "                    s= xnew- x\n",
        "                    gf_new= self.eval_grad_obj(xnew)\n",
        "                    y= gf_new- gf\n",
        "                    self.H= self.dfp_update(self.H, s, y)\n",
        "\n",
        "            x= xnew\n",
        "            if self.callback is not None:\n",
        "                self.callback(x)\n",
        "        else:\n",
        "            message= \"Max iteration.\"\n",
        "            success=False\n",
        "\n",
        "        xfinal= x[:self.n_orig]\n",
        "        res= OptimizeResult()\n",
        "        res.x= xfinal\n",
        "        res.fun= self.call_fun(x)\n",
        "        fullg= self.eval_grad_obj(x)\n",
        "        res.jac= fullg[:self.n_orig]\n",
        "        res.hess_inv= self.H[:self.n_orig,:self.n_orig]\n",
        "        res.nit= it\n",
        "        res.nfev= self.nfev\n",
        "        res.njev= self.njev\n",
        "        res.success= success\n",
        "        res.message= message\n",
        "        return res"
      ],
      "metadata": {
        "id": "LK0QfJn6hxLz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interface"
      ],
      "metadata": {
        "id": "m4ONymo3MLdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grg_minimize(\n",
        "    fun,\n",
        "    x0,\n",
        "    args=(),\n",
        "    jac=None,\n",
        "    constraints=(),\n",
        "    bounds=None,\n",
        "    tol=1e-6,\n",
        "    options=None,\n",
        "    callback=None,\n",
        "    verbose=False\n",
        "):\n",
        "    \"\"\"\n",
        "    A SciPy-like interface to the GRG optimizer with a More–Thuente-inspired line search.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    fun : callable\n",
        "        Objective function: fun(x, *args)-> float\n",
        "    x0 : array-like\n",
        "        Initial guess.\n",
        "    args : tuple\n",
        "        Additional arguments.\n",
        "    jac : callable or None\n",
        "        Gradient or None => finite diff.\n",
        "    constraints : list of dict\n",
        "        'type': 'eq' or 'ineq', 'fun', possibly 'jac'.\n",
        "    bounds : list of (float, float)\n",
        "        For each original var.\n",
        "    tol : float\n",
        "        Tolerance for feasibility & KKT.\n",
        "    options : dict\n",
        "        e.g. {'maxiter':100}\n",
        "    callback : callable\n",
        "        Called each iteration\n",
        "    verbose : bool\n",
        "        Print debug info\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    OptimizeResult\n",
        "        With x, fun, jac, hess_inv, nit, nfev, njev, success, message\n",
        "    \"\"\"\n",
        "    if options is None:\n",
        "        options={}\n",
        "    maxiter= options.get('maxiter',100)\n",
        "\n",
        "    optimizer= _GRGOptimizer(\n",
        "        fun=fun, x0=x0, args=args,\n",
        "        jac=jac, constraints=constraints,\n",
        "        bounds=bounds,\n",
        "        tol=tol, maxiter=maxiter,\n",
        "        verbose=verbose, callback=callback\n",
        "    )\n",
        "    return optimizer.run()"
      ],
      "metadata": {
        "id": "qVMrZ3CYMLWO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Cases"
      ],
      "metadata": {
        "id": "jGHPCUluMLNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 1: Simple Unconstrained Quadratic\n",
        "# Minimize f(x) = x^2\n",
        "# Should converge easily to x = 0.\n",
        "\n",
        "def fun1(x, *args):\n",
        "    return x[0]**2\n",
        "\n",
        "def grad1(x, *args):\n",
        "    return np.array([2 * x[0]])\n",
        "\n",
        "x0_1 = np.array([2.0])  # start from 2\n",
        "\n",
        "res1 = grg_minimize(fun1, x0_1, jac=grad1, constraints=[], bounds=None, tol=1e-8, verbose=True)\n",
        "print(\"Test 1 Result:\", res1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4VVYXqdMOVb",
        "outputId": "06783ec1-df30-4571-849b-07bff8b91c4c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Iteration 1 ---\n",
            "[DEBUG] reduced grad norm=4.0\n",
            "[DEBUG] using steepest direction.\n",
            "[DEBUG] bracket expansion phase.\n",
            "[DEBUG] zoom => [lo=0.0, hi=1.0]\n",
            "[DEBUG] no sufficient interval shrink => bisection => alpha_j= 0.5\n",
            "[DEBUG] zoom iter=1, alpha_j=0.5, f=0.0, dd=0.0\n",
            "[DEBUG] zoom => success => alpha=0.5\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=0.5\n",
            "[DEBUG] line search => alpha=0.5, status=success\n",
            "\n",
            "--- Iteration 2 ---\n",
            "[DEBUG] reduced grad norm=0.0\n",
            "Test 1 Result:   message: KKT => done\n",
            "  success: True\n",
            "      fun: 0.0\n",
            "        x: [ 0.000e+00]\n",
            "      nit: 2\n",
            "      jac: [ 0.000e+00]\n",
            " hess_inv: [[ 1.000e+00]]\n",
            "     nfev: 4\n",
            "     njev: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 2: Unconstrained 2D Quadratic\n",
        "# Minimize f(x, y) = (x - 1)^2 + (y + 2)^2\n",
        "# Known minimum at (1, -2).\n",
        "\n",
        "def fun2(x, *args):\n",
        "    return (x[0] - 1)**2 + (x[1] + 2)**2\n",
        "\n",
        "def grad2(x, *args):\n",
        "    return np.array([2 * (x[0] - 1), 2 * (x[1] + 2)])\n",
        "\n",
        "x0_2 = np.array([0.0, 0.0]) # start from (0, 0)\n",
        "\n",
        "res2 = grg_minimize(fun2, x0_2, jac=grad2, constraints=[], bounds=None, tol=1e-8, verbose=True)\n",
        "print(\"Test 2 Result:\", res2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f219c1VMS-Q",
        "outputId": "8590c0d8-e08e-489e-ae37-e208a88e2f42"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Iteration 1 ---\n",
            "[DEBUG] reduced grad norm=4.47213595499958\n",
            "[DEBUG] using steepest direction.\n",
            "[DEBUG] bracket expansion phase.\n",
            "[DEBUG] zoom => [lo=0.0, hi=1.0]\n",
            "[DEBUG] no sufficient interval shrink => bisection => alpha_j= 0.5\n",
            "[DEBUG] zoom iter=1, alpha_j=0.5, f=0.0, dd=0.0\n",
            "[DEBUG] zoom => success => alpha=0.5\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=0.5\n",
            "[DEBUG] line search => alpha=0.5, status=success\n",
            "\n",
            "--- Iteration 2 ---\n",
            "[DEBUG] reduced grad norm=0.0\n",
            "Test 2 Result:   message: KKT => done\n",
            "  success: True\n",
            "      fun: 0.0\n",
            "        x: [ 1.000e+00 -2.000e+00]\n",
            "      nit: 2\n",
            "      jac: [ 0.000e+00  0.000e+00]\n",
            " hess_inv: [[ 1.000e+00  0.000e+00]\n",
            "            [ 0.000e+00  1.000e+00]]\n",
            "     nfev: 4\n",
            "     njev: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 3: Problem with an Equality Constraint\n",
        "# Minimize f(x, y) = x^2 + y^2 subject to x + y = 1 (an equality constraint).\n",
        "# The solution should lie on the line y = 1 - x. The minimum is at (0.5, 0.5).\n",
        "\n",
        "def fun3(x, *args):\n",
        "    return x[0]**2 + x[1]**2\n",
        "\n",
        "def grad3(x, *args):\n",
        "    return np.array([2*x[0], 2*x[1]])\n",
        "\n",
        "def eq_constr3(x, *args):\n",
        "    return x[0] + x[1] - 1.0\n",
        "\n",
        "# Constraint dict format: {'type' : 'eq', 'fun' : ..., 'jac' : ...}\n",
        "def eq_jac3(x, *args):\n",
        "    return np.array([1.0, 1.0])\n",
        "\n",
        "constraints_3 = [{'type' : 'eq', 'fun' : eq_constr3, 'jac' : eq_jac3}]\n",
        "x0_3 = np.array([2.0, -1.0]) # start from a point not on the line\n",
        "\n",
        "res3 = grg_minimize(fun3, x0_3, jac=grad3, constraints=constraints_3, bounds=None, tol=1e-8, verbose=True)\n",
        "print(\"Test 3 Result:\", res3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyjGH5D3MS7o",
        "outputId": "c2b621ae-55a9-47aa-cce9-492621ebf331"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Iteration 1 ---\n",
            "[DEBUG] reduced grad norm=4.242640687119285\n",
            "[DEBUG] using steepest direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 2 ---\n",
            "[DEBUG] reduced grad norm=1.4142135623730951\n",
            "[DEBUG] using steepest direction.\n",
            "[DEBUG] bracket expansion phase.\n",
            "[DEBUG] zoom => [lo=0.0, hi=1.0]\n",
            "[DEBUG] no sufficient interval shrink => bisection => alpha_j= 0.5\n",
            "[DEBUG] zoom iter=1, alpha_j=0.5, f=1.0, dd=0.0\n",
            "[DEBUG] cubic => alpha_c=0.16666666666666669\n",
            "[DEBUG] zoom iter=2, alpha_j=0.16666666666666669, f=0.5555555555555556, dd=-2.6666666666666665\n",
            "[DEBUG] zoom => success => alpha=0.16666666666666669\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=0.16666666666666669\n",
            "[DEBUG] line search => alpha=0.16666666666666669, status=success\n",
            "\n",
            "--- Iteration 3 ---\n",
            "[DEBUG] reduced grad norm=0.47140452079103157\n",
            "[DEBUG] using steepest direction.\n",
            "[DEBUG] bracket expansion phase.\n",
            "[DEBUG] zoom => [lo=0.0, hi=1.0]\n",
            "[DEBUG] no sufficient interval shrink => bisection => alpha_j= 0.5\n",
            "[DEBUG] zoom iter=1, alpha_j=0.5, f=1.0, dd=-1.3333333333333335\n",
            "[DEBUG] cubic => alpha_c=0.0770329993461985\n",
            "[DEBUG] zoom iter=2, alpha_j=0.0770329993461985, f=0.5081807400950108, dd=-2.08527466782898\n",
            "[DEBUG] no sufficient interval shrink => bisection => alpha_j= 0.2885164996730992\n",
            "[DEBUG] zoom iter=3, alpha_j=0.2885164996730992, f=0.595067184587885, dd=-1.7093040005811568\n",
            "[DEBUG] cubic => alpha_c=0.11449644921746216\n",
            "[DEBUG] zoom iter=4, alpha_j=0.11449644921746216, f=0.5003922651699249, dd=-2.018672979168956\n",
            "[DEBUG] no sufficient interval shrink => bisection => alpha_j= 0.2015064744452807\n",
            "[DEBUG] zoom iter=5, alpha_j=0.2015064744452807, f=0.5208115222472761, dd=-1.8639884898750565\n",
            "[DEBUG] cubic => alpha_c=0.1308177627176997\n",
            "[DEBUG] zoom iter=6, alpha_j=0.1308177627176997, f=0.5001203426241403, dd=-1.9896573107240894\n",
            "[DEBUG] zoom => success => alpha=0.1308177627176997\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=0.1308177627176997\n",
            "[DEBUG] line search => alpha=0.1308177627176997, status=success\n",
            "\n",
            "--- Iteration 4 ---\n",
            "[DEBUG] reduced grad norm=0.0219401571681054\n",
            "[DEBUG] using steepest direction.\n",
            "[DEBUG] bracket expansion phase.\n",
            "[DEBUG] zoom => [lo=0.0, hi=1.0]\n",
            "[DEBUG] no sufficient interval shrink => bisection => alpha_j= 0.5\n",
            "[DEBUG] zoom iter=1, alpha_j=0.5, f=1.0, dd=-2.031028067827732\n",
            "[DEBUG] cubic => alpha_c=0.06343921129039715\n",
            "[DEBUG] zoom iter=2, alpha_j=0.06343921129039715, f=0.5098594583617202, dd=-2.004357087268993\n",
            "[DEBUG] cubic => alpha_c=0.012136122212720278\n",
            "[DEBUG] zoom iter=3, alpha_j=0.012136122212720278, f=0.5007765629331142, dd=-2.0012228074005733\n",
            "[DEBUG] cubic => alpha_c=0.0024732957689702958\n",
            "[DEBUG] zoom iter=4, alpha_j=0.0024732957689702958, f=0.5002077513606686, dd=-2.0006324725310916\n",
            "[DEBUG] cubic => alpha_c=0.0005103654362097771\n",
            "[DEBUG] zoom iter=5, alpha_j=0.0005103654362097771, f=0.500136437509849, dd=-2.0005125504535775\n",
            "[DEBUG] cubic => alpha_c=0.0001055832514362276\n",
            "[DEBUG] zoom iter=6, alpha_j=0.0001055832514362276, f=0.5001235894529865, dd=-2.000487820935811\n",
            "[DEBUG] cubic => alpha_c=2.1854346697706227e-05\n",
            "[DEBUG] zoom iter=7, alpha_j=2.1854346697706227e-05, f=0.5001210111280703, dd=-2.000482705652789\n",
            "[DEBUG] cubic => alpha_c=4.5240564489515915e-06\n",
            "[DEBUG] zoom iter=8, alpha_j=4.5240564489515915e-06, f=0.5001204808587973, dd=-2.000481646886527\n",
            "[DEBUG] cubic => alpha_c=9.365436241431038e-07\n",
            "[DEBUG] zoom iter=9, alpha_j=9.365436241431038e-07, f=0.5001203712341552, dd=-2.0004814277131904\n",
            "[DEBUG] cubic => alpha_c=1.9387867306261468e-07\n",
            "[DEBUG] zoom iter=10, alpha_j=1.9387867306261468e-07, f=0.5001203485465662, dd=-2.0004813823412673\n",
            "[DEBUG] cubic => alpha_c=4.013585198036083e-08\n",
            "[DEBUG] zoom iter=11, alpha_j=4.013585198036083e-08, f=0.5001203438501611, dd=-2.0004813729485966\n",
            "[DEBUG] cubic => alpha_c=8.308737165233083e-09\n",
            "[DEBUG] zoom iter=12, alpha_j=8.308737165233083e-09, f=0.5001203345712074, dd=-2.000481354390695\n",
            "[DEBUG] no sufficient interval shrink => bisection => alpha_j= 2.4222294572796956e-08\n",
            "[DEBUG] zoom iter=13, alpha_j=2.4222294572796956e-08, f=0.5001203433640525, dd=-2.000481371976383\n",
            "[DEBUG] cubic => alpha_c=1.0766599564422035e-08\n",
            "[DEBUG] zoom iter=14, alpha_j=1.0766599564422035e-08, f=0.5001203429530245, dd=-2.0004813711543292\n",
            "[DEBUG] cubic => alpha_c=8.554523405151978e-09\n",
            "[DEBUG] zoom iter=15, alpha_j=8.554523405151978e-09, f=0.5001203343329883, dd=-2.000481353914257\n",
            "[DEBUG] zoom => max_zoom => no_improve.\n",
            "[DEBUG] line_search attempt=1, status=no_improve, alpha=8.554523405151978e-09\n",
            "[DEBUG] bracket expansion phase.\n",
            "[DEBUG] zoom => [lo=0.0, hi=1.0]\n",
            "[DEBUG] no sufficient interval shrink => bisection => alpha_j= 0.5\n",
            "[DEBUG] zoom iter=1, alpha_j=0.5, f=1.0, dd=-2.031028067827732\n",
            "[DEBUG] cubic => alpha_c=0.06343921129039715\n",
            "[DEBUG] zoom iter=2, alpha_j=0.06343921129039715, f=0.5098594583617202, dd=-2.004357087268993\n",
            "[DEBUG] cubic => alpha_c=0.012136122212720278\n",
            "[DEBUG] zoom iter=3, alpha_j=0.012136122212720278, f=0.5007765629331142, dd=-2.0012228074005733\n",
            "[DEBUG] cubic => alpha_c=0.0024732957689702958\n",
            "[DEBUG] zoom iter=4, alpha_j=0.0024732957689702958, f=0.5002077513606686, dd=-2.0006324725310916\n",
            "[DEBUG] cubic => alpha_c=0.0005103654362097771\n",
            "[DEBUG] zoom iter=5, alpha_j=0.0005103654362097771, f=0.500136437509849, dd=-2.0005125504535775\n",
            "[DEBUG] cubic => alpha_c=0.0001055832514362276\n",
            "[DEBUG] zoom iter=6, alpha_j=0.0001055832514362276, f=0.5001235894529865, dd=-2.000487820935811\n",
            "[DEBUG] cubic => alpha_c=2.1854346697706227e-05\n",
            "[DEBUG] zoom iter=7, alpha_j=2.1854346697706227e-05, f=0.5001210111280703, dd=-2.000482705652789\n",
            "[DEBUG] cubic => alpha_c=4.5240564489515915e-06\n",
            "[DEBUG] zoom iter=8, alpha_j=4.5240564489515915e-06, f=0.5001204808587973, dd=-2.000481646886527\n",
            "[DEBUG] cubic => alpha_c=9.365436241431038e-07\n",
            "[DEBUG] zoom iter=9, alpha_j=9.365436241431038e-07, f=0.5001203712341552, dd=-2.0004814277131904\n",
            "[DEBUG] cubic => alpha_c=1.9387867306261468e-07\n",
            "[DEBUG] zoom iter=10, alpha_j=1.9387867306261468e-07, f=0.5001203485465662, dd=-2.0004813823412673\n",
            "[DEBUG] cubic => alpha_c=4.013585198036083e-08\n",
            "[DEBUG] zoom iter=11, alpha_j=4.013585198036083e-08, f=0.5001203438501611, dd=-2.0004813729485966\n",
            "[DEBUG] cubic => alpha_c=8.308737165233083e-09\n",
            "[DEBUG] zoom iter=12, alpha_j=8.308737165233083e-09, f=0.5001203345712074, dd=-2.000481354390695\n",
            "[DEBUG] no sufficient interval shrink => bisection => alpha_j= 2.4222294572796956e-08\n",
            "[DEBUG] zoom iter=13, alpha_j=2.4222294572796956e-08, f=0.5001203433640525, dd=-2.000481371976383\n",
            "[DEBUG] cubic => alpha_c=1.0766599564422035e-08\n",
            "[DEBUG] zoom iter=14, alpha_j=1.0766599564422035e-08, f=0.5001203429530245, dd=-2.0004813711543292\n",
            "[DEBUG] cubic => alpha_c=8.554523405151978e-09\n",
            "[DEBUG] zoom iter=15, alpha_j=8.554523405151978e-09, f=0.5001203343329883, dd=-2.000481353914257\n",
            "[DEBUG] zoom => max_zoom => no_improve.\n",
            "[DEBUG] line_search attempt=2, status=no_improve, alpha=8.554523405151978e-09\n",
            "[DEBUG] minimal step => success alpha= 1e-06\n",
            "[DEBUG] line search => alpha=1e-06, status=success\n",
            "\n",
            "--- Iteration 5 ---\n",
            "[DEBUG] reduced grad norm=0.021942941714915797\n",
            "[DEBUG] using steepest direction.\n",
            "[DEBUG] bracket expansion phase.\n",
            "[DEBUG] zoom => [lo=0.0, hi=1.0]\n",
            "[DEBUG] no sufficient interval shrink => bisection => alpha_j= 0.5\n",
            "[DEBUG] zoom iter=1, alpha_j=0.5, f=1.0, dd=-2.0310320057715963\n",
            "[DEBUG] cubic => alpha_c=0.0634391852198285\n",
            "[DEBUG] zoom iter=2, alpha_j=0.0634391852198285, f=0.5098596925651342, dd=-2.0043576920068524\n",
            "[DEBUG] cubic => alpha_c=0.012136092101646047\n",
            "[DEBUG] zoom iter=3, alpha_j=0.012136092101646047, f=0.5007766363115186, dd=-2.001223020372099\n",
            "[DEBUG] cubic => alpha_c=0.002473283668653129\n",
            "[DEBUG] zoom iter=4, alpha_j=0.002473283668653129, f=0.5002077908136551, dd=-2.000632612861246\n",
            "[DEBUG] cubic => alpha_c=0.0005103616600502809\n",
            "[DEBUG] zoom iter=5, alpha_j=0.0005103616600502809, f=0.5001364698810568, dd=-2.0005126763122463\n",
            "[DEBUG] cubic => alpha_c=0.00010558220285907709\n",
            "[DEBUG] zoom iter=6, alpha_j=0.00010558220285907709, f=0.5001236203719513, dd=-2.0004879438720433\n",
            "[DEBUG] cubic => alpha_c=2.1854074167708716e-05\n",
            "[DEBUG] zoom iter=7, alpha_j=2.1854074167708716e-05, f=0.500121041751744, dd=-2.0004828279974616\n",
            "[DEBUG] cubic => alpha_c=4.5239885386087625e-06\n",
            "[DEBUG] zoom iter=8, alpha_j=4.5239885386087625e-06, f=0.5001205114226196, dd=-2.0004817691114463\n",
            "[DEBUG] cubic => alpha_c=9.365271859555151e-07\n",
            "[DEBUG] zoom iter=9, alpha_j=9.365271859555151e-07, f=0.5001204017858623, dd=-2.0004815499138764\n",
            "[DEBUG] cubic => alpha_c=1.9387477744232393e-07\n",
            "[DEBUG] zoom iter=10, alpha_j=1.9387477744232393e-07, f=0.5001203790958229, dd=-2.000481504537052\n",
            "[DEBUG] cubic => alpha_c=4.0134943512435173e-08\n",
            "[DEBUG] zoom iter=11, alpha_j=4.0134943512435173e-08, f=0.5001203743989223, dd=-2.0004814951433905\n",
            "[DEBUG] cubic => alpha_c=8.308527976259354e-09\n",
            "[DEBUG] zoom iter=12, alpha_j=8.308527976259354e-09, f=0.5001203651200782, dd=-2.0004814765857084\n",
            "[DEBUG] no sufficient interval shrink => bisection => alpha_j= 2.4221735744347264e-08\n",
            "[DEBUG] zoom iter=13, alpha_j=2.4221735744347264e-08, f=0.5001203739127636, dd=-2.000481494171077\n",
            "[DEBUG] cubic => alpha_c=1.0766333930612418e-08\n",
            "[DEBUG] zoom iter=14, alpha_j=1.0766333930612418e-08, f=0.5001203735016932, dd=-2.0004814933489383\n",
            "[DEBUG] cubic => alpha_c=8.554308571694661e-09\n",
            "[DEBUG] zoom iter=15, alpha_j=8.554308571694661e-09, f=0.5001203648818655, dd=-2.000481476109283\n",
            "[DEBUG] zoom => max_zoom => no_improve.\n",
            "[DEBUG] line_search attempt=1, status=no_improve, alpha=8.554308571694661e-09\n",
            "[DEBUG] bracket expansion phase.\n",
            "[DEBUG] zoom => [lo=0.0, hi=1.0]\n",
            "[DEBUG] no sufficient interval shrink => bisection => alpha_j= 0.5\n",
            "[DEBUG] zoom iter=1, alpha_j=0.5, f=1.0, dd=-2.0310320057715963\n",
            "[DEBUG] cubic => alpha_c=0.0634391852198285\n",
            "[DEBUG] zoom iter=2, alpha_j=0.0634391852198285, f=0.5098596925651342, dd=-2.0043576920068524\n",
            "[DEBUG] cubic => alpha_c=0.012136092101646047\n",
            "[DEBUG] zoom iter=3, alpha_j=0.012136092101646047, f=0.5007766363115186, dd=-2.001223020372099\n",
            "[DEBUG] cubic => alpha_c=0.002473283668653129\n",
            "[DEBUG] zoom iter=4, alpha_j=0.002473283668653129, f=0.5002077908136551, dd=-2.000632612861246\n",
            "[DEBUG] cubic => alpha_c=0.0005103616600502809\n",
            "[DEBUG] zoom iter=5, alpha_j=0.0005103616600502809, f=0.5001364698810568, dd=-2.0005126763122463\n",
            "[DEBUG] cubic => alpha_c=0.00010558220285907709\n",
            "[DEBUG] zoom iter=6, alpha_j=0.00010558220285907709, f=0.5001236203719513, dd=-2.0004879438720433\n",
            "[DEBUG] cubic => alpha_c=2.1854074167708716e-05\n",
            "[DEBUG] zoom iter=7, alpha_j=2.1854074167708716e-05, f=0.500121041751744, dd=-2.0004828279974616\n",
            "[DEBUG] cubic => alpha_c=4.5239885386087625e-06\n",
            "[DEBUG] zoom iter=8, alpha_j=4.5239885386087625e-06, f=0.5001205114226196, dd=-2.0004817691114463\n",
            "[DEBUG] cubic => alpha_c=9.365271859555151e-07\n",
            "[DEBUG] zoom iter=9, alpha_j=9.365271859555151e-07, f=0.5001204017858623, dd=-2.0004815499138764\n",
            "[DEBUG] cubic => alpha_c=1.9387477744232393e-07\n",
            "[DEBUG] zoom iter=10, alpha_j=1.9387477744232393e-07, f=0.5001203790958229, dd=-2.000481504537052\n",
            "[DEBUG] cubic => alpha_c=4.0134943512435173e-08\n",
            "[DEBUG] zoom iter=11, alpha_j=4.0134943512435173e-08, f=0.5001203743989223, dd=-2.0004814951433905\n",
            "[DEBUG] cubic => alpha_c=8.308527976259354e-09\n",
            "[DEBUG] zoom iter=12, alpha_j=8.308527976259354e-09, f=0.5001203651200782, dd=-2.0004814765857084\n",
            "[DEBUG] no sufficient interval shrink => bisection => alpha_j= 2.4221735744347264e-08\n",
            "[DEBUG] zoom iter=13, alpha_j=2.4221735744347264e-08, f=0.5001203739127636, dd=-2.000481494171077\n",
            "[DEBUG] cubic => alpha_c=1.0766333930612418e-08\n",
            "[DEBUG] zoom iter=14, alpha_j=1.0766333930612418e-08, f=0.5001203735016932, dd=-2.0004814933489383\n",
            "[DEBUG] cubic => alpha_c=8.554308571694661e-09\n",
            "[DEBUG] zoom iter=15, alpha_j=8.554308571694661e-09, f=0.5001203648818655, dd=-2.000481476109283\n",
            "[DEBUG] zoom => max_zoom => no_improve.\n",
            "[DEBUG] line_search attempt=2, status=no_improve, alpha=8.554308571694661e-09\n",
            "[DEBUG] minimal step => success alpha= 1e-06\n",
            "[DEBUG] line search => alpha=1e-06, status=success\n",
            "\n",
            "--- Iteration 6 ---\n",
            "[DEBUG] reduced grad norm=0.021945726256157077\n",
            "[DEBUG] using DFP direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 7 ---\n",
            "[DEBUG] reduced grad norm=1.0532500405730103e-15\n",
            "Test 3 Result:   message: KKT => done\n",
            "  success: True\n",
            "      fun: 0.5\n",
            "        x: [ 5.000e-01  5.000e-01]\n",
            "      nit: 7\n",
            "      jac: [ 1.000e+00  1.000e+00]\n",
            " hess_inv: [[ 7.500e-01  2.500e-01]\n",
            "            [ 2.500e-01  7.500e-01]]\n",
            "     nfev: 89\n",
            "     njev: 94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 4: Inequality Constraint\n",
        "# Minimize f(x) = x^2 subject to x ≥ 0 (inequality).\n",
        "# Start from x = -2. Must move into feasible region and then minimize at x = 0.\n",
        "\n",
        "def fun4(x, *args):\n",
        "    return x[0]**2\n",
        "\n",
        "def grad4(x, *args):\n",
        "    return np.array([2 * x[0]])\n",
        "\n",
        "def ineq_constr4(x, *args):\n",
        "    # x ≥ 0 means -x ≤ 0\n",
        "    return -x[0]\n",
        "\n",
        "constraints_4 = [{'type' : 'ineq', 'fun' : ineq_constr4}]\n",
        "x0_4 = np.array([-2.0])\n",
        "\n",
        "res4 = grg_minimize(fun4, x0_4, jac=grad4, constraints=constraints_4, bounds=None, tol=1e-8, verbose=True)\n",
        "print(\"Test 4 Result:\", res4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrK6VzRlMS4x",
        "outputId": "6795b617-6eac-4029-b5ba-4af249c20d2b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Iteration 1 ---\n",
            "[DEBUG] reduced grad norm=4.0\n",
            "[DEBUG] using steepest direction.\n",
            "[DEBUG] bracket expansion phase.\n",
            "[DEBUG] zoom => [lo=0.0, hi=1.0]\n",
            "[DEBUG] no sufficient interval shrink => bisection => alpha_j= 0.5\n",
            "[DEBUG] zoom iter=1, alpha_j=0.5, f=0.0, dd=0.0\n",
            "[DEBUG] zoom => success => alpha=0.5\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=0.5\n",
            "[DEBUG] line search => alpha=0.5, status=success\n",
            "\n",
            "--- Iteration 2 ---\n",
            "[DEBUG] reduced grad norm=0.0\n",
            "Test 4 Result:   message: KKT => done\n",
            "  success: True\n",
            "      fun: 0.0\n",
            "        x: [ 0.000e+00]\n",
            "      nit: 2\n",
            "      jac: [ 0.000e+00]\n",
            " hess_inv: [[ 1.000e+00]]\n",
            "     nfev: 4\n",
            "     njev: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 5: Problem with both bounds and constraints\n",
        "# Minimize f(x, y) = (x - 2)^2 + (y - 3)^2 with bound x ≥ 1 and eq constraint: x + y =5\n",
        "# The solution: from x + y = 5, if we want to minimize distance to (2, 3),\n",
        "# The unconstrained min would be (2, 3). This satisfies x ≥ 1. So solution should be (2, 3).\n",
        "\n",
        "def fun5(x, *args):\n",
        "    return (x[0] - 2)**2 + (x[1] - 3)**2\n",
        "\n",
        "def grad5(x, *args):\n",
        "    return np.array([2 * (x[0] - 2), 2 * (x[1] - 3)])\n",
        "\n",
        "def eq_constr5(x, *args):\n",
        "    return x[0] + x[1] - 5.0\n",
        "\n",
        "def eq_jac5(x, *args):\n",
        "    return np.array([1.0, 1.0])\n",
        "\n",
        "constraints_5 = [{'type' : 'eq', 'fun' : eq_constr5, 'jac' : eq_jac5}]\n",
        "bounds_5 = [(1.0, None), (None, None)] # x ≥ 1\n",
        "\n",
        "x0_5 = np.array([1.5, 4.0]) # start from a feasible point close to solution\n",
        "res5 = grg_minimize(fun5, x0_5, jac=grad5, constraints=constraints_5, bounds=bounds_5, tol=1e-8, verbose=True)\n",
        "print(\"Test 5 Result:\", res5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-brOLEOMS15",
        "outputId": "a33e4a60-4e86-4161-e17d-5b634b80367f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Iteration 1 ---\n",
            "[DEBUG] feasibility => reduce violation.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 2 ---\n",
            "[DEBUG] transition => objective phase\n",
            "[DEBUG] reduced grad norm=0.0\n",
            "Test 5 Result:   message: KKT => done\n",
            "  success: True\n",
            "      fun: 0.0\n",
            "        x: [ 2.000e+00  3.000e+00]\n",
            "      nit: 2\n",
            "      jac: [ 0.000e+00  0.000e+00]\n",
            " hess_inv: [[ 1.000e+00  0.000e+00]\n",
            "            [ 0.000e+00  1.000e+00]]\n",
            "     nfev: 3\n",
            "     njev: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 6: Flat Region (Potential line search trouble)\n",
        "# Minimize f(x) = |x|^3, which is flat at x = 0. A weird shape that can cause line search to try multiple steps.\n",
        "# The minimum is at x = 0. Start from x = 10. Check if line search can handle flatness near zero.\n",
        "\n",
        "def fun6(x, *args):\n",
        "    return abs(x[0])**3\n",
        "\n",
        "def grad6(x, *args):\n",
        "    # Grad of |x|^3 = 3|x|^2 * sign(x) but handle x = 0:\n",
        "    val = x[0]\n",
        "    if val > 0:\n",
        "        return np.array([3 * val**2])\n",
        "    elif val < 0:\n",
        "        return np.array([-3 * val**2])\n",
        "    else:\n",
        "        # Non-differentiable at zero, approximate gradient as 0 for code simplicity\n",
        "        return np.array([0.0])\n",
        "\n",
        "x0_6 = np.array([10.0])\n",
        "res6 = grg_minimize(fun6, x0_6, jac=grad6, constraints=[], bounds=None, tol=1e-8, verbose=True)\n",
        "print(\"Test 6 Result:\", res6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEfGeeYzMSzC",
        "outputId": "31d80514-410b-494a-9575-8194dd6ee278"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Iteration 1 ---\n",
            "[DEBUG] reduced grad norm=300.0\n",
            "[DEBUG] using steepest direction.\n",
            "[DEBUG] bracket expansion phase.\n",
            "[DEBUG] zoom => [lo=0.0, hi=1.0]\n",
            "[DEBUG] no sufficient interval shrink => bisection => alpha_j= 0.5\n",
            "[DEBUG] zoom iter=1, alpha_j=0.5, f=2744000.0, dd=17640000.0\n",
            "[DEBUG] cubic => alpha_c=0.06763333313741393\n",
            "[DEBUG] zoom iter=2, alpha_j=0.06763333313741393, f=1089.5473703297262, dd=95295.68891135429\n",
            "[DEBUG] cubic => alpha_c=0.03357496292351557\n",
            "[DEBUG] zoom iter=3, alpha_j=0.03357496292351557, f=0.0003809027569635412, dd=4.729173566982428\n",
            "[DEBUG] zoom => success => alpha=0.03357496292351557\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=0.03357496292351557\n",
            "[DEBUG] line search => alpha=0.03357496292351557, status=success\n",
            "\n",
            "--- Iteration 2 ---\n",
            "[DEBUG] reduced grad norm=0.015763911889941428\n",
            "[DEBUG] using steepest direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 3 ---\n",
            "[DEBUG] reduced grad norm=0.009653165018819198\n",
            "[DEBUG] using steepest direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 4 ---\n",
            "[DEBUG] reduced grad norm=0.006647263106929455\n",
            "[DEBUG] using steepest direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 5 ---\n",
            "[DEBUG] reduced grad norm=0.0049024295844477105\n",
            "[DEBUG] using steepest direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 6 ---\n",
            "[DEBUG] reduced grad norm=0.0037854603540341146\n",
            "[DEBUG] using DFP direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 7 ---\n",
            "[DEBUG] reduced grad norm=0.0010724859980501121\n",
            "[DEBUG] using DFP direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 8 ---\n",
            "[DEBUG] reduced grad norm=0.00045679140562868083\n",
            "[DEBUG] using DFP direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 9 ---\n",
            "[DEBUG] reduced grad norm=0.00016725138764410376\n",
            "[DEBUG] using DFP direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 10 ---\n",
            "[DEBUG] reduced grad norm=6.491820198242345e-05\n",
            "[DEBUG] using DFP direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 11 ---\n",
            "[DEBUG] reduced grad norm=2.4644585192736774e-05\n",
            "[DEBUG] using DFP direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 12 ---\n",
            "[DEBUG] reduced grad norm=9.435501642961515e-06\n",
            "[DEBUG] using DFP direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 13 ---\n",
            "[DEBUG] reduced grad norm=3.6008119433942846e-06\n",
            "[DEBUG] using DFP direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 14 ---\n",
            "[DEBUG] reduced grad norm=1.375858681879051e-06\n",
            "[DEBUG] using DFP direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 15 ---\n",
            "[DEBUG] reduced grad norm=5.25462537512329e-07\n",
            "[DEBUG] using DFP direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 16 ---\n",
            "[DEBUG] reduced grad norm=2.007188543269857e-07\n",
            "[DEBUG] using DFP direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 17 ---\n",
            "[DEBUG] reduced grad norm=7.666631753574292e-08\n",
            "[DEBUG] using DFP direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 18 ---\n",
            "[DEBUG] reduced grad norm=2.9284140899932436e-08\n",
            "[DEBUG] using DFP direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 19 ---\n",
            "[DEBUG] reduced grad norm=1.1185515358625714e-08\n",
            "[DEBUG] using DFP direction.\n",
            "[DEBUG] immediate strong wolfe => alpha=1.0\n",
            "[DEBUG] line_search attempt=1, status=success, alpha=1.0\n",
            "[DEBUG] line search => alpha=1.0, status=success\n",
            "\n",
            "--- Iteration 20 ---\n",
            "[DEBUG] reduced grad norm=4.272491227666495e-09\n",
            "Test 6 Result:   message: KKT => done\n",
            "  success: True\n",
            "      fun: 5.3745225259703814e-14\n",
            "        x: [-3.774e-05]\n",
            "      nit: 20\n",
            "      jac: [-4.272e-09]\n",
            " hess_inv: [[ 3.374e+03]]\n",
            "     nfev: 42\n",
            "     njev: 77\n"
          ]
        }
      ]
    }
  ]
}