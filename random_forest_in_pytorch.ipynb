{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DecisionTreeModel"
      ],
      "metadata": {
        "id": "hnAwl9NJauRm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "njZkG_gyZiN7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "import numpy as np\n",
        "import logging\n",
        "from typing import Optional, Union, Dict, Any, Tuple\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class DecisionTreeNode:\n",
        "    \"\"\"\n",
        "    A node in the Decision Tree.\n",
        "\n",
        "    Attributes:\n",
        "        feature_index (Optional[int]): Index of the feature used for splitting.\n",
        "        threshold (Optional[float]): Threshold value for the split.\n",
        "        left (Optional[DecisionTreeNode]): Left child node.\n",
        "        right (Optional[DecisionTreeNode]): Right child node.\n",
        "        value (Optional[Union[float, int]]): Value to predict if the node is a leaf.\n",
        "        gain (float): Gain achieved by the split.\n",
        "        class_counts (Optional[np.ndarray]): Class counts in the node (for classification).\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 feature_index: Optional[int] = None,\n",
        "                 threshold: Optional[float] = None,\n",
        "                 left: Optional['DecisionTreeNode'] = None,\n",
        "                 right: Optional['DecisionTreeNode'] = None,\n",
        "                 value: Optional[Union[float, int]] = None,\n",
        "                 gain: float = 0.0,\n",
        "                 class_counts: Optional[np.ndarray] = None):\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "        self.gain = gain\n",
        "        self.class_counts = class_counts\n",
        "\n",
        "\n",
        "class DecisionTreeModel(BaseEstimator, ClassifierMixin, RegressorMixin):\n",
        "    \"\"\"\n",
        "    A Decision Tree model implemented in PyTorch, compatible with scikit-learn.\n",
        "    Supports both classification and regression tasks with GPU acceleration.\n",
        "\n",
        "    Parameters:\n",
        "        max_depth (Optional[int]): Maximum depth of the tree. If None, nodes are expanded until all leaves are pure.\n",
        "        min_samples_split (int): Minimum number of samples required to split an internal node.\n",
        "        max_features (Union[int, float, str, None]): Number or fraction of features to consider when looking for the best split.\n",
        "            If int, then consider `max_features` features at each split.\n",
        "            If float, then `max_features` is a fraction and `int(max_features * n_features)` features are considered.\n",
        "            If 'sqrt', then `max_features = sqrt(n_features)`.\n",
        "            If 'log2', then `max_features = log2(n_features)`.\n",
        "            If None, then `max_features = n_features`.\n",
        "        task (str): 'classification' or 'regression'.\n",
        "        random_state (Optional[int]): Seed for reproducibility.\n",
        "        min_gain (float): Minimum gain required to make a split. Splits with gain below this threshold are ignored.\n",
        "        classes_ (Optional[np.ndarray]): Global classes for classification alignment.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 max_depth: Optional[int] = None,\n",
        "                 min_samples_split: int = 2,\n",
        "                 max_features: Union[int, float, str, None] = None,\n",
        "                 task: str = 'classification',\n",
        "                 random_state: Optional[int] = None,\n",
        "                 min_gain: float = 1e-7,\n",
        "                 classes_: Optional[np.ndarray] = None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_features = max_features\n",
        "        self.task = task\n",
        "        self.random_state = random_state\n",
        "        self.min_gain = min_gain\n",
        "        self.root_ = None\n",
        "        self.is_fitted_ = False\n",
        "        self.device = 'cpu'\n",
        "        self.classes_ = classes_  # Store global classes\n",
        "        self.n_classes_ = len(self.classes_) if self.classes_ is not None else None\n",
        "\n",
        "        if self.random_state is not None:\n",
        "            torch.manual_seed(self.random_state)\n",
        "            np.random.seed(self.random_state)\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) -> 'DecisionTreeModel':\n",
        "        \"\"\"\n",
        "        Build the decision tree from the training set (X, y).\n",
        "\n",
        "        Parameters:\n",
        "            X (np.ndarray): Training data of shape (n_samples, n_features).\n",
        "            y (np.ndarray): Target values of shape (n_samples,).\n",
        "\n",
        "        Returns:\n",
        "            self\n",
        "        \"\"\"\n",
        "        # Validate input\n",
        "        X, y = check_X_y(X, y)\n",
        "        n_samples, n_features = X.shape\n",
        "        self.n_features_ = n_features  # Store for later use\n",
        "\n",
        "        # Validate task\n",
        "        self._validate_fit(X, y)\n",
        "\n",
        "        # Determine device\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Convert to torch tensors and move to device\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        if self.task == 'classification':\n",
        "            y_tensor = torch.tensor(y, dtype=torch.long).to(self.device)\n",
        "            if self.classes_ is None:\n",
        "                self.classes_ = torch.unique(y_tensor).cpu().numpy()\n",
        "                self.n_classes_ = len(self.classes_)\n",
        "            else:\n",
        "                self.n_classes_ = len(self.classes_)\n",
        "        elif self.task == 'regression':\n",
        "            y_tensor = torch.tensor(y, dtype=torch.float32).to(self.device)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid task. Choose 'classification' or 'regression'.\")\n",
        "\n",
        "        # Build the tree\n",
        "        self.root_ = self._build_tree(X_tensor, y_tensor, depth=0)\n",
        "        self.is_fitted_ = True\n",
        "        return self\n",
        "\n",
        "    def _build_tree(self, X: torch.Tensor, y: torch.Tensor, depth: int) -> DecisionTreeNode:\n",
        "        \"\"\"\n",
        "        Recursively build the decision tree.\n",
        "\n",
        "        Parameters:\n",
        "            X (torch.Tensor): Feature matrix.\n",
        "            y (torch.Tensor): Target vector.\n",
        "            depth (int): Current depth of the tree.\n",
        "\n",
        "        Returns:\n",
        "            DecisionTreeNode: The root node of the subtree.\n",
        "        \"\"\"\n",
        "        num_samples, num_features = X.shape\n",
        "\n",
        "        # Check for pure node or stopping criteria\n",
        "        if self.task == 'classification':\n",
        "            unique_classes = torch.unique(y)\n",
        "            if len(unique_classes) == 1:\n",
        "                leaf_value, class_counts = self._calculate_leaf_value(y)\n",
        "                return DecisionTreeNode(value=leaf_value, class_counts=class_counts)\n",
        "        elif self.task == 'regression':\n",
        "            if torch.all(y == y[0]):\n",
        "                leaf_value = self._calculate_leaf_value(y)\n",
        "                return DecisionTreeNode(value=leaf_value)\n",
        "\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            if self.task == 'classification':\n",
        "                leaf_value, class_counts = self._calculate_leaf_value(y)\n",
        "                return DecisionTreeNode(value=leaf_value, class_counts=class_counts)\n",
        "            elif self.task == 'regression':\n",
        "                leaf_value = self._calculate_leaf_value(y)\n",
        "                return DecisionTreeNode(value=leaf_value)\n",
        "\n",
        "        if num_samples < self.min_samples_split:\n",
        "            if self.task == 'classification':\n",
        "                leaf_value, class_counts = self._calculate_leaf_value(y)\n",
        "                return DecisionTreeNode(value=leaf_value, class_counts=class_counts)\n",
        "            elif self.task == 'regression':\n",
        "                leaf_value = self._calculate_leaf_value(y)\n",
        "                return DecisionTreeNode(value=leaf_value)\n",
        "\n",
        "        # Find the best split\n",
        "        best_split = self._get_best_split(X, y, num_features)\n",
        "\n",
        "        if best_split[\"gain\"] < self.min_gain:\n",
        "            if self.task == 'classification':\n",
        "                leaf_value, class_counts = self._calculate_leaf_value(y)\n",
        "                return DecisionTreeNode(value=leaf_value, class_counts=class_counts)\n",
        "            elif self.task == 'regression':\n",
        "                leaf_value = self._calculate_leaf_value(y)\n",
        "                return DecisionTreeNode(value=leaf_value)\n",
        "\n",
        "        # Recursively build left and right subtrees\n",
        "        left_subtree = self._build_tree(best_split[\"X_left\"], best_split[\"y_left\"], depth + 1)\n",
        "        right_subtree = self._build_tree(best_split[\"X_right\"], best_split[\"y_right\"], depth + 1)\n",
        "\n",
        "        return DecisionTreeNode(\n",
        "            feature_index=best_split[\"feature_index\"],\n",
        "            threshold=best_split[\"threshold\"],\n",
        "            left=left_subtree,\n",
        "            right=right_subtree,\n",
        "            gain=best_split[\"gain\"]\n",
        "        )\n",
        "\n",
        "    def _get_best_split(self, X: torch.Tensor, y: torch.Tensor, num_features: int) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Find the best split for the current node.\n",
        "\n",
        "        Parameters:\n",
        "            X (torch.Tensor): Feature matrix.\n",
        "            y (torch.Tensor): Target vector.\n",
        "            num_features (int): Number of features.\n",
        "\n",
        "        Returns:\n",
        "            dict: Information about the best split.\n",
        "        \"\"\"\n",
        "        best_split = {}\n",
        "        max_gain = -1\n",
        "\n",
        "        if self.task == 'classification':\n",
        "            parent_loss = self._gini(y)\n",
        "        elif self.task == 'regression':\n",
        "            parent_loss = self._mse(y)\n",
        "\n",
        "        # Determine number of features to consider\n",
        "        if self.max_features is None:\n",
        "            n_features = num_features\n",
        "        elif isinstance(self.max_features, int):\n",
        "            n_features = min(self.max_features, num_features)\n",
        "        elif isinstance(self.max_features, float):\n",
        "            n_features = max(1, int(self.max_features * num_features))\n",
        "        elif isinstance(self.max_features, str):\n",
        "            if self.max_features == 'sqrt':\n",
        "                n_features = max(1, int(np.sqrt(num_features)))\n",
        "            elif self.max_features == 'log2':\n",
        "                n_features = max(1, int(np.log2(num_features)))\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported string value for max_features. Use 'sqrt' or 'log2'.\")\n",
        "        else:\n",
        "            raise ValueError(\"max_features must be int, float, str ('sqrt' or 'log2'), or None\")\n",
        "\n",
        "        # Randomly select features to consider\n",
        "        feature_indices = torch.randperm(num_features)[:n_features].tolist()\n",
        "\n",
        "        for feature_index in feature_indices:\n",
        "            X_column = X[:, feature_index]\n",
        "            # Sort the feature values and corresponding targets\n",
        "            sorted_indices = torch.argsort(X_column)\n",
        "            X_sorted = X_column[sorted_indices]\n",
        "            y_sorted = y[sorted_indices]\n",
        "\n",
        "            # Compute potential split thresholds (unique midpoints)\n",
        "            unique_values = torch.unique(X_sorted)\n",
        "            if len(unique_values) == 1:\n",
        "                continue  # No split possible on this feature\n",
        "            thresholds = (unique_values[:-1] + unique_values[1:]) / 2.0\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                # Determine the split\n",
        "                left_mask = X_sorted <= threshold\n",
        "                right_mask = X_sorted > threshold\n",
        "\n",
        "                y_left = y_sorted[left_mask]\n",
        "                y_right = y_sorted[right_mask]\n",
        "                X_left = X[sorted_indices[left_mask], :]\n",
        "                X_right = X[sorted_indices[right_mask], :]\n",
        "\n",
        "                if len(y_left) == 0 or len(y_right) == 0:\n",
        "                    continue  # Skip invalid splits\n",
        "\n",
        "                # Calculate loss\n",
        "                if self.task == 'classification':\n",
        "                    loss = (len(y_left) / len(y)) * self._gini(y_left) + \\\n",
        "                           (len(y_right) / len(y)) * self._gini(y_right)\n",
        "                elif self.task == 'regression':\n",
        "                    loss = (len(y_left) / len(y)) * self._mse(y_left) + \\\n",
        "                           (len(y_right) / len(y)) * self._mse(y_right)\n",
        "\n",
        "                gain = parent_loss - loss\n",
        "\n",
        "                if gain > max_gain:\n",
        "                    max_gain = gain\n",
        "                    best_split = {\n",
        "                        \"feature_index\": feature_index,\n",
        "                        \"threshold\": threshold.item(),\n",
        "                        \"X_left\": X_left,    # Full feature matrix for left split\n",
        "                        \"y_left\": y_left,\n",
        "                        \"X_right\": X_right,  # Full feature matrix for right split\n",
        "                        \"y_right\": y_right,\n",
        "                        \"gain\": gain\n",
        "                    }\n",
        "\n",
        "        return best_split\n",
        "\n",
        "    def _gini(self, y: torch.Tensor) -> float:\n",
        "        \"\"\"\n",
        "        Calculate Gini impurity for classification.\n",
        "\n",
        "        Parameters:\n",
        "            y (torch.Tensor): Target vector.\n",
        "\n",
        "        Returns:\n",
        "            float: Gini impurity.\n",
        "        \"\"\"\n",
        "        if self.classes_ is None:\n",
        "            raise AttributeError(\"Classes not defined. Ensure that classes_ is set correctly.\")\n",
        "        class_counts = torch.bincount(y, minlength=self.n_classes_).float()\n",
        "        probabilities = class_counts / len(y)\n",
        "        gini = 1.0 - torch.sum(probabilities ** 2)\n",
        "        return gini.item()\n",
        "\n",
        "    def _mse(self, y: torch.Tensor) -> float:\n",
        "        \"\"\"\n",
        "        Calculate Mean Squared Error for regression.\n",
        "\n",
        "        Parameters:\n",
        "            y (torch.Tensor): Target vector.\n",
        "\n",
        "        Returns:\n",
        "            float: MSE.\n",
        "        \"\"\"\n",
        "        mse = torch.mean((y - torch.mean(y)) ** 2)\n",
        "        return mse.item()\n",
        "\n",
        "    def _calculate_leaf_value(self, y: torch.Tensor) -> Union[float, int, Tuple[int, np.ndarray]]:\n",
        "        \"\"\"\n",
        "        Calculate the value to assign to a leaf node.\n",
        "\n",
        "        Parameters:\n",
        "            y (torch.Tensor): Target vector.\n",
        "\n",
        "        Returns:\n",
        "            float or int or tuple: Prediction value and class counts (for classification).\n",
        "        \"\"\"\n",
        "        if self.task == 'classification':\n",
        "            # Ensure class_counts includes all classes by setting minlength\n",
        "            counts = torch.bincount(y, minlength=self.n_classes_).cpu().numpy()\n",
        "            most_common = torch.mode(y).values.item()\n",
        "            return most_common, counts\n",
        "        elif self.task == 'regression':\n",
        "            return torch.mean(y).item()\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict target values for samples in X.\n",
        "\n",
        "        Parameters:\n",
        "            X (np.ndarray): Input data of shape (n_queries, n_features).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted class labels or regression values.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, 'is_fitted_')\n",
        "        X = check_array(X)\n",
        "        # Convert to torch tensor and move to device\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        predictions = []\n",
        "\n",
        "        for x in X_tensor:\n",
        "            predictions.append(self._traverse_tree(x))\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict class probabilities for samples in X.\n",
        "\n",
        "        Parameters:\n",
        "            X (np.ndarray): Input data of shape (n_queries, n_features).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted probabilities of shape (n_queries, n_classes).\n",
        "        \"\"\"\n",
        "        if self.task != 'classification':\n",
        "            raise AttributeError(\"predict_proba is only available for classification tasks.\")\n",
        "        check_is_fitted(self, 'is_fitted_')\n",
        "        if self.classes_ is None:\n",
        "            raise AttributeError(\"Classes not found. Ensure that the model is fitted properly.\")\n",
        "        X = check_array(X)\n",
        "\n",
        "        n_samples = X.shape[0]\n",
        "        n_classes = len(self.classes_)\n",
        "        proba = np.zeros((n_samples, n_classes))\n",
        "\n",
        "        # Convert to torch tensor and move to device\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "\n",
        "        for i, x in enumerate(X_tensor):\n",
        "            node = self.root_\n",
        "            while node.value is None:\n",
        "                if x[node.feature_index].item() <= node.threshold:\n",
        "                    node = node.left\n",
        "                else:\n",
        "                    node = node.right\n",
        "            # Normalize class counts to probabilities\n",
        "            if node.class_counts is not None:\n",
        "                proba[i] = node.class_counts / np.sum(node.class_counts)\n",
        "            else:\n",
        "                # Fallback if class_counts are not available\n",
        "                proba[i, node.value] = 1.0\n",
        "\n",
        "        return proba\n",
        "\n",
        "    def _traverse_tree(self, x: torch.Tensor) -> Union[float, int]:\n",
        "        \"\"\"\n",
        "        Traverse the tree to make a prediction for a single sample.\n",
        "\n",
        "        Parameters:\n",
        "            x (torch.Tensor): Single sample.\n",
        "\n",
        "        Returns:\n",
        "            float or int: Predicted value.\n",
        "        \"\"\"\n",
        "        node = self.root_\n",
        "        while node.value is None:\n",
        "            if x[node.feature_index].item() <= node.threshold:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "        return node.value\n",
        "\n",
        "    def score(self, X: np.ndarray, y: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Compute the score of the model.\n",
        "        For classification: accuracy.\n",
        "        For regression: R² score.\n",
        "\n",
        "        Parameters:\n",
        "            X (np.ndarray): Test samples of shape (n_samples, n_features).\n",
        "            y (np.ndarray): True labels or target values of shape (n_samples,).\n",
        "\n",
        "        Returns:\n",
        "            float: Score.\n",
        "        \"\"\"\n",
        "        X = check_array(X)\n",
        "        y = np.array(y)\n",
        "        y_pred = self.predict(X)\n",
        "\n",
        "        if self.task == 'classification':\n",
        "            return accuracy_score(y, y_pred) * 100.0  # Percentage\n",
        "        elif self.task == 'regression':\n",
        "            return r2_score(y, y_pred)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid task. Choose 'classification' or 'regression'.\")\n",
        "\n",
        "    def feature_importances_(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Calculate feature importances based on the trained tree.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Array of feature importances.\n",
        "        \"\"\"\n",
        "        if not self.is_fitted_:\n",
        "            raise AttributeError(\"This DecisionTreeModel instance is not fitted yet.\")\n",
        "\n",
        "        importances = np.zeros(self.n_features_)\n",
        "\n",
        "        def _accumulate_importances(node: DecisionTreeNode):\n",
        "            if node is None or node.value is not None:\n",
        "                return\n",
        "            importances[node.feature_index] += node.gain\n",
        "            _accumulate_importances(node.left)\n",
        "            _accumulate_importances(node.right)\n",
        "\n",
        "        _accumulate_importances(self.root_)\n",
        "        total_gain = np.sum(importances)\n",
        "        if total_gain > 0:\n",
        "            importances /= total_gain\n",
        "        return importances\n",
        "\n",
        "    @property\n",
        "    def max_features_(self) -> int:\n",
        "        \"\"\"\n",
        "        Determine the number of features based on max_features parameter.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of features to consider.\n",
        "        \"\"\"\n",
        "        if self.max_features is None:\n",
        "            return self.n_features_\n",
        "        elif isinstance(self.max_features, int):\n",
        "            return min(self.max_features, self.n_features_)\n",
        "        elif isinstance(self.max_features, float):\n",
        "            return max(1, int(self.max_features * self.n_features_))\n",
        "        elif isinstance(self.max_features, str):\n",
        "            if self.max_features == 'sqrt':\n",
        "                return max(1, int(np.sqrt(self.n_features_)))\n",
        "            elif self.max_features == 'log2':\n",
        "                return max(1, int(np.log2(self.n_features_)))\n",
        "        raise ValueError(\"Invalid max_features parameter.\")\n",
        "\n",
        "    def _serialize_node(self, node: Optional[DecisionTreeNode]) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Serialize a DecisionTreeNode into a dictionary.\n",
        "\n",
        "        Parameters:\n",
        "            node (Optional[DecisionTreeNode]): Node to serialize.\n",
        "\n",
        "        Returns:\n",
        "            Optional[Dict[str, Any]]: Serialized node.\n",
        "        \"\"\"\n",
        "        if node is None:\n",
        "            return None\n",
        "        return {\n",
        "            'feature_index': node.feature_index,\n",
        "            'threshold': node.threshold,\n",
        "            'value': node.value,\n",
        "            'gain': node.gain,\n",
        "            'class_counts': node.class_counts.tolist() if node.class_counts is not None else None,\n",
        "            'left': self._serialize_node(node.left),\n",
        "            'right': self._serialize_node(node.right)\n",
        "        }\n",
        "\n",
        "    def _deserialize_node(self, node_dict: Optional[Dict[str, Any]]) -> Optional[DecisionTreeNode]:\n",
        "        \"\"\"\n",
        "        Deserialize a dictionary into a DecisionTreeNode.\n",
        "\n",
        "        Parameters:\n",
        "            node_dict (Optional[Dict[str, Any]]): Serialized node.\n",
        "\n",
        "        Returns:\n",
        "            Optional[DecisionTreeNode]: Deserialized node.\n",
        "        \"\"\"\n",
        "        if node_dict is None:\n",
        "            return None\n",
        "        class_counts = np.array(node_dict['class_counts']) if node_dict['class_counts'] is not None else None\n",
        "        node = DecisionTreeNode(\n",
        "            feature_index=node_dict['feature_index'],\n",
        "            threshold=node_dict['threshold'],\n",
        "            value=node_dict['value'],\n",
        "            gain=node_dict.get('gain', 0.0),\n",
        "            class_counts=class_counts\n",
        "        )\n",
        "        node.left = self._deserialize_node(node_dict['left'])\n",
        "        node.right = self._deserialize_node(node_dict['right'])\n",
        "        return node\n",
        "\n",
        "    def load_model_from_dict(self, node_dict: Optional[Dict[str, Any]]) -> None:\n",
        "        \"\"\"\n",
        "        Load a trained Decision Tree model from a dictionary.\n",
        "\n",
        "        Parameters:\n",
        "            node_dict (Optional[Dict[str, Any]]): Serialized node.\n",
        "        \"\"\"\n",
        "        self.root_ = self._deserialize_node(node_dict)\n",
        "        self.is_fitted_ = True\n",
        "        logger.info(\"Decision Tree loaded from dictionary.\")\n",
        "\n",
        "    def save_model(self, filepath: str) -> None:\n",
        "        \"\"\"\n",
        "        Save the trained Decision Tree model to a file.\n",
        "\n",
        "        Parameters:\n",
        "            filepath (str): Path to the file where the model will be saved.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, 'is_fitted_')\n",
        "        state = {\n",
        "            'max_depth': self.max_depth,\n",
        "            'min_samples_split': self.min_samples_split,\n",
        "            'max_features': self.max_features,\n",
        "            'task': self.task,\n",
        "            'random_state': self.random_state,\n",
        "            'min_gain': self.min_gain,\n",
        "            'n_features_': self.n_features_,\n",
        "            'classes_': self.classes_.tolist() if self.classes_ is not None else None,\n",
        "            'root_': self._serialize_node(self.root_)\n",
        "        }\n",
        "        torch.save(state, filepath)  # Use torch's save method\n",
        "        logger.info(f\"Decision Tree model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath: str) -> None:\n",
        "        \"\"\"\n",
        "        Load a trained Decision Tree model from a file.\n",
        "\n",
        "        Parameters:\n",
        "            filepath (str): Path to the file from which the model will be loaded.\n",
        "        \"\"\"\n",
        "        state = torch.load(filepath, map_location=self.device)\n",
        "        self.max_depth = state['max_depth']\n",
        "        self.min_samples_split = state['min_samples_split']\n",
        "        self.max_features = state['max_features']\n",
        "        self.task = state['task']\n",
        "        self.random_state = state['random_state']\n",
        "        self.min_gain = state.get('min_gain', 1e-7)\n",
        "        self.n_features_ = state['n_features_']\n",
        "        self.classes_ = np.array(state['classes_']) if state['classes_'] is not None else None\n",
        "        self.n_classes_ = len(self.classes_) if self.classes_ is not None else None\n",
        "        self.root_ = self._deserialize_node(state['root_'])\n",
        "        self.is_fitted_ = True\n",
        "        logger.info(f\"Decision Tree model loaded from {filepath}\")\n",
        "\n",
        "    def _validate_fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Validate the fit parameters.\n",
        "\n",
        "        Parameters:\n",
        "            X (np.ndarray): Feature matrix.\n",
        "            y (np.ndarray): Target vector.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If task is invalid.\n",
        "        \"\"\"\n",
        "        if self.task not in ['classification', 'regression']:\n",
        "            raise ValueError(\"Invalid task. Choose 'classification' or 'regression'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RandomForestModel"
      ],
      "metadata": {
        "id": "M6tM2cx0awhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "from sklearn.metrics import accuracy_score, r2_score, roc_auc_score\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import logging\n",
        "from typing import Optional, Union, Dict, Any, List\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class RandomForestModel(BaseEstimator, ClassifierMixin, RegressorMixin):\n",
        "    \"\"\"\n",
        "    A Random Forest model implemented in PyTorch, compatible with scikit-learn.\n",
        "    Supports both classification and regression tasks with GPU acceleration.\n",
        "\n",
        "    Parameters:\n",
        "        n_estimators (int): Number of trees in the forest.\n",
        "        max_depth (Optional[int]): Maximum depth of the trees. If None, nodes are expanded until all leaves are pure.\n",
        "        min_samples_split (int): Minimum number of samples required to split an internal node.\n",
        "        max_features (Union[int, float, str, None]): Number or fraction of features to consider when looking for the best split.\n",
        "            If int, then consider `max_features` features at each split.\n",
        "            If float, then `max_features` is a fraction and `int(max_features * n_features)` features are considered.\n",
        "            If 'sqrt', then `max_features = sqrt(n_features)`.\n",
        "            If 'log2', then `max_features = log2(n_features)`.\n",
        "            If None, then `max_features = n_features`.\n",
        "        bootstrap (bool): Whether bootstrap samples are used when building trees.\n",
        "        random_state (Optional[int]): Seed for reproducibility.\n",
        "        task (str): 'classification' or 'regression'.\n",
        "        min_gain (float): Minimum gain required to make a split. Splits with gain below this threshold are ignored.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_estimators: int = 100,\n",
        "                 max_depth: Optional[int] = None,\n",
        "                 min_samples_split: int = 2,\n",
        "                 max_features: Union[int, float, str, None] = None,\n",
        "                 bootstrap: bool = True,\n",
        "                 random_state: Optional[int] = None,\n",
        "                 task: str = 'classification',\n",
        "                 min_gain: float = 1e-7):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_features = max_features\n",
        "        self.bootstrap = bootstrap\n",
        "        self.random_state = random_state\n",
        "        self.task = task\n",
        "        self.min_gain = min_gain\n",
        "        self.trees_: List['DecisionTreeModel'] = []\n",
        "        self.is_fitted_ = False\n",
        "        self.oob_score_: Optional[float] = None\n",
        "        self.classes_: Optional[np.ndarray] = None  # To store unique classes for classification\n",
        "\n",
        "        if self.random_state is not None:\n",
        "            torch.manual_seed(self.random_state)\n",
        "            np.random.seed(self.random_state)\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray) -> 'RandomForestModel':\n",
        "        \"\"\"\n",
        "        Build a forest of trees from the training set (X, y).\n",
        "\n",
        "        Parameters:\n",
        "            X (np.ndarray): Training data of shape (n_samples, n_features).\n",
        "            y (np.ndarray): Target values of shape (n_samples,).\n",
        "\n",
        "        Returns:\n",
        "            self\n",
        "        \"\"\"\n",
        "        # Validate input\n",
        "        X, y = check_X_y(X, y)\n",
        "        n_samples, n_features = X.shape\n",
        "        self.n_features_ = n_features  # Store for later use\n",
        "\n",
        "        # Validate task\n",
        "        self._validate_fit(X, y)\n",
        "\n",
        "        # Determine device\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "        # For classification, store unique classes\n",
        "        if self.task == 'classification':\n",
        "            self.classes_ = np.unique(y)\n",
        "            logger.info(f\"Classes found: {self.classes_}\")\n",
        "\n",
        "        # Initialize OOB predictions\n",
        "        if self.bootstrap:\n",
        "            if self.task == 'classification':\n",
        "                # For classification, store votes for each class per sample\n",
        "                self.oob_votes_ = [Counter() for _ in range(n_samples)]\n",
        "            elif self.task == 'regression':\n",
        "                # For regression, accumulate predictions\n",
        "                self.oob_preds_ = np.zeros(n_samples)\n",
        "                self.oob_counts_ = np.zeros(n_samples)\n",
        "        else:\n",
        "            self.oob_score_ = None  # OOB score not applicable\n",
        "\n",
        "        # Function to train a single tree\n",
        "        def train_tree(i: int) -> Dict[str, Any]:\n",
        "            # Bootstrap sampling\n",
        "            if self.bootstrap:\n",
        "                indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
        "                X_sample = X[indices]\n",
        "                y_sample = y[indices]\n",
        "                oob_indices = np.setdiff1d(np.arange(n_samples), indices)\n",
        "            else:\n",
        "                X_sample = X\n",
        "                y_sample = y\n",
        "                oob_indices = np.array([])\n",
        "\n",
        "            # Generate unique random state\n",
        "            tree_random_state = self.random_state + i if self.random_state is not None else None\n",
        "\n",
        "            # Initialize and train Decision Tree\n",
        "            tree = DecisionTreeModel(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                max_features=self.max_features,\n",
        "                task=self.task,\n",
        "                random_state=tree_random_state,\n",
        "                min_gain=self.min_gain,\n",
        "                classes_=self.classes_ if self.task == 'classification' else None  # Pass global classes\n",
        "            )\n",
        "            tree.fit(X_sample, y_sample)\n",
        "\n",
        "            # Collect OOB predictions\n",
        "            oob_pred = None\n",
        "            if self.bootstrap and len(oob_indices) > 0:\n",
        "                if self.task == 'classification':\n",
        "                    oob_pred = tree.predict(X[oob_indices])\n",
        "                elif self.task == 'regression':\n",
        "                    oob_pred = tree.predict(X[oob_indices])\n",
        "\n",
        "            return {\n",
        "                'tree': tree,\n",
        "                'oob_pred': oob_pred,\n",
        "                'oob_indices': oob_indices\n",
        "            }\n",
        "\n",
        "        # Train trees in parallel\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            futures = [executor.submit(train_tree, i) for i in range(self.n_estimators)]\n",
        "            for future in as_completed(futures):\n",
        "                result = future.result()\n",
        "                tree = result['tree']\n",
        "                self.trees_.append(tree)\n",
        "                logger.info(f\"Trained tree {len(self.trees_)}/{self.n_estimators}\")\n",
        "\n",
        "                if self.bootstrap:\n",
        "                    if self.task == 'classification' and result['oob_pred'] is not None:\n",
        "                        for idx, pred in zip(result['oob_indices'], result['oob_pred']):\n",
        "                            self.oob_votes_[idx][pred] += 1\n",
        "                    elif self.task == 'regression' and result['oob_pred'] is not None:\n",
        "                        self.oob_preds_[result['oob_indices']] += result['oob_pred']\n",
        "                        self.oob_counts_[result['oob_indices']] += 1\n",
        "\n",
        "        # Calculate OOB score if applicable\n",
        "        if self.bootstrap:\n",
        "            if self.task == 'classification':\n",
        "                # Determine majority vote for each OOB sample\n",
        "                oob_pred_majority = np.array([\n",
        "                    vote.most_common(1)[0][0] if len(vote) > 0 else -1  # Assign -1 if no votes\n",
        "                    for vote in self.oob_votes_\n",
        "                ])\n",
        "                # Filter out samples with no OOB votes\n",
        "                valid_mask = oob_pred_majority != -1\n",
        "                if np.any(valid_mask):\n",
        "                    self.oob_score_ = accuracy_score(y[valid_mask], oob_pred_majority[valid_mask]) * 100.0\n",
        "                    logger.info(f\"OOB Score: {self.oob_score_:.2f}%\")\n",
        "                else:\n",
        "                    self.oob_score_ = None\n",
        "                    logger.warning(\"No OOB samples were found. OOB score is not available.\")\n",
        "            elif self.task == 'regression':\n",
        "                # Avoid division by zero\n",
        "                valid_mask = self.oob_counts_ > 0\n",
        "                if np.any(valid_mask):\n",
        "                    self.oob_preds_[valid_mask] /= self.oob_counts_[valid_mask]\n",
        "                    self.oob_score_ = r2_score(y[valid_mask], self.oob_preds_[valid_mask])\n",
        "                    logger.info(f\"OOB R² Score: {self.oob_score_:.4f}\")\n",
        "                else:\n",
        "                    self.oob_score_ = None\n",
        "                    logger.warning(\"No OOB samples were found. OOB score is not available.\")\n",
        "\n",
        "        self.is_fitted_ = True\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict class probabilities for samples in X.\n",
        "\n",
        "        Parameters:\n",
        "            X (np.ndarray): Input data of shape (n_queries, n_features).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted probabilities of shape (n_queries, n_classes).\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, 'is_fitted_')\n",
        "        if self.task != 'classification':\n",
        "            raise AttributeError(\"predict_proba is only available for classification tasks.\")\n",
        "        if self.classes_ is None:\n",
        "            raise AttributeError(\"Classes not found. Ensure that the model is fitted properly.\")\n",
        "        X = check_array(X)\n",
        "\n",
        "        n_samples = X.shape[0]\n",
        "        n_classes = len(self.classes_)\n",
        "        proba = np.zeros((n_samples, n_classes))\n",
        "\n",
        "        # Iterate over each tree and accumulate probabilities\n",
        "        for tree in self.trees_:\n",
        "            tree_proba = tree.predict_proba(X)\n",
        "            tree_classes = tree.classes_\n",
        "\n",
        "            # Create a mapping from tree classes to indices in self.classes_\n",
        "            class_indices = {cls: idx for idx, cls in enumerate(self.classes_)}\n",
        "\n",
        "            # Initialize tree_proba_aligned with zeros\n",
        "            tree_proba_aligned = np.zeros((n_samples, n_classes))\n",
        "\n",
        "            # Assign probabilities to the corresponding class indices\n",
        "            for i, cls in enumerate(tree_classes):\n",
        "                if cls in class_indices:\n",
        "                    tree_proba_aligned[:, class_indices[cls]] = tree_proba[:, i]\n",
        "\n",
        "            # Accumulate the aligned probabilities\n",
        "            proba += tree_proba_aligned\n",
        "\n",
        "        # Average the probabilities\n",
        "        proba /= self.n_estimators\n",
        "\n",
        "        return proba\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict target values for samples in X.\n",
        "\n",
        "        Parameters:\n",
        "            X (np.ndarray): Input data of shape (n_queries, n_features).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Predicted class labels or regression values.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, 'is_fitted_')\n",
        "        X = check_array(X)\n",
        "\n",
        "        # Collect predictions from all trees\n",
        "        if self.task == 'classification':\n",
        "            # Collect class predictions\n",
        "            tree_preds = np.array([tree.predict(X) for tree in self.trees_])\n",
        "            # Transpose to shape (n_samples, n_estimators)\n",
        "            tree_preds = tree_preds.T\n",
        "            # Majority vote\n",
        "            y_pred = np.array([\n",
        "                Counter(row).most_common(1)[0][0] if len(row) > 0 else self.classes_[0] for row in tree_preds\n",
        "            ])\n",
        "        elif self.task == 'regression':\n",
        "            # Collect regression predictions\n",
        "            tree_preds = np.array([tree.predict(X) for tree in self.trees_])\n",
        "            # Average predictions\n",
        "            y_pred = np.mean(tree_preds, axis=0)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid task. Choose 'classification' or 'regression'.\")\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "    def score(self, X: np.ndarray, y: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Compute the score of the model.\n",
        "        For classification: accuracy.\n",
        "        For regression: R² score.\n",
        "\n",
        "        Parameters:\n",
        "            X (np.ndarray): Test samples of shape (n_samples, n_features).\n",
        "            y (np.ndarray): True labels or target values of shape (n_samples,).\n",
        "\n",
        "        Returns:\n",
        "            float: Score.\n",
        "        \"\"\"\n",
        "        X = check_array(X)\n",
        "        y = np.array(y)\n",
        "        y_pred = self.predict(X)\n",
        "\n",
        "        if self.task == 'classification':\n",
        "            return accuracy_score(y, y_pred) * 100.0  # Percentage\n",
        "        elif self.task == 'regression':\n",
        "            return r2_score(y, y_pred)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid task. Choose 'classification' or 'regression'.\")\n",
        "\n",
        "    def feature_importances_(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Calculate feature importances based on all trees in the forest.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Array of feature importances.\n",
        "        \"\"\"\n",
        "        if not self.is_fitted_:\n",
        "            raise AttributeError(\"This RandomForestModel instance is not fitted yet.\")\n",
        "\n",
        "        # Initialize importances array with the total number of features\n",
        "        importances = np.zeros(self.n_features_)\n",
        "\n",
        "        for tree in self.trees_:\n",
        "            importances += tree.feature_importances_()\n",
        "\n",
        "        importances /= self.n_estimators\n",
        "        return importances\n",
        "\n",
        "    @property\n",
        "    def oob_score(self) -> Optional[float]:\n",
        "        \"\"\"\n",
        "        Get the out-of-bag (OOB) score.\n",
        "\n",
        "        Returns:\n",
        "            Optional[float]: OOB score if bootstrap is True; otherwise, None.\n",
        "        \"\"\"\n",
        "        return self.oob_score_\n",
        "\n",
        "    def save_model(self, filepath: str) -> None:\n",
        "        \"\"\"\n",
        "        Save the trained Random Forest model to a file.\n",
        "\n",
        "        Parameters:\n",
        "            filepath (str): Path to the file where the model will be saved.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, 'is_fitted_')\n",
        "        state = {\n",
        "            'n_estimators': self.n_estimators,\n",
        "            'max_depth': self.max_depth,\n",
        "            'min_samples_split': self.min_samples_split,\n",
        "            'max_features': self.max_features,\n",
        "            'bootstrap': self.bootstrap,\n",
        "            'random_state': self.random_state,\n",
        "            'task': self.task,\n",
        "            'min_gain': self.min_gain,\n",
        "            'classes_': self.classes_.tolist() if self.classes_ is not None else None,\n",
        "            'trees_': [tree._serialize_node(tree.root_) for tree in self.trees_]\n",
        "        }\n",
        "        torch.save(state, filepath)\n",
        "        logger.info(f\"Random Forest model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath: str) -> None:\n",
        "        \"\"\"\n",
        "        Load a trained Random Forest model from a file.\n",
        "\n",
        "        Parameters:\n",
        "            filepath (str): Path to the file from which the model will be loaded.\n",
        "        \"\"\"\n",
        "        state = torch.load(filepath, map_location=self.device if hasattr(self, 'device') else 'cpu')\n",
        "\n",
        "        self.n_estimators = state['n_estimators']\n",
        "        self.max_depth = state['max_depth']\n",
        "        self.min_samples_split = state['min_samples_split']\n",
        "        self.max_features = state['max_features']\n",
        "        self.bootstrap = state['bootstrap']\n",
        "        self.random_state = state['random_state']\n",
        "        self.task = state['task']\n",
        "        self.min_gain = state.get('min_gain', 1e-7)\n",
        "        self.classes_ = np.array(state['classes_']) if state['classes_'] is not None else None\n",
        "        self.trees_ = []\n",
        "\n",
        "        for i, tree_dict in enumerate(state['trees_']):\n",
        "            tree = DecisionTreeModel(\n",
        "                max_depth=self.max_depth,\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                max_features=self.max_features,\n",
        "                task=self.task,\n",
        "                random_state=self.random_state + i if self.random_state is not None else None,\n",
        "                min_gain=self.min_gain,\n",
        "                classes_=self.classes_ if self.task == 'classification' else None  # Pass global classes\n",
        "            )\n",
        "            tree.load_model_from_dict(tree_dict)  # Load tree from its dict\n",
        "            tree.device = self.device if hasattr(self, 'device') else 'cpu'  # Ensure tree uses the same device\n",
        "            self.trees_.append(tree)\n",
        "            logger.info(f\"Loaded tree {i + 1}/{self.n_estimators}\")\n",
        "\n",
        "        # Note: OOB score cannot be recalculated without original training data\n",
        "        if self.bootstrap:\n",
        "            self.oob_score_ = None\n",
        "            logger.warning(\"OOB score cannot be recalculated after loading the model without original training data.\")\n",
        "\n",
        "        self.is_fitted_ = True\n",
        "        logger.info(f\"Random Forest model loaded from {filepath}\")\n",
        "\n",
        "    def get_params(self, deep: bool = True) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Get parameters for this estimator.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Any]: Parameter names mapped to their values.\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'n_estimators': self.n_estimators,\n",
        "            'max_depth': self.max_depth,\n",
        "            'min_samples_split': self.min_samples_split,\n",
        "            'max_features': self.max_features,\n",
        "            'bootstrap': self.bootstrap,\n",
        "            'random_state': self.random_state,\n",
        "            'task': self.task,\n",
        "            'min_gain': self.min_gain\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params: Any) -> 'RandomForestModel':\n",
        "        \"\"\"\n",
        "        Set the parameters of this estimator.\n",
        "\n",
        "        Parameters:\n",
        "            **params: Estimator parameters.\n",
        "\n",
        "        Returns:\n",
        "            self\n",
        "        \"\"\"\n",
        "        for key, value in params.items():\n",
        "            setattr(self, key, value)\n",
        "        return self\n",
        "\n",
        "    def _validate_fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Validate the fit parameters.\n",
        "\n",
        "        Parameters:\n",
        "            X (np.ndarray): Feature matrix.\n",
        "            y (np.ndarray): Target vector.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If task is invalid.\n",
        "        \"\"\"\n",
        "        if self.task not in ['classification', 'regression']:\n",
        "            raise ValueError(\"Invalid task. Choose 'classification' or 'regression'.\")\n",
        "\n",
        "    def _compute_score(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Compute the OOB score based on the task.\n",
        "\n",
        "        Parameters:\n",
        "            y_true (np.ndarray): True target values.\n",
        "            y_pred (np.ndarray): OOB predicted values.\n",
        "\n",
        "        Returns:\n",
        "            float: OOB score.\n",
        "        \"\"\"\n",
        "        if self.task == 'classification':\n",
        "            return accuracy_score(y_true, y_pred) * 100.0\n",
        "        elif self.task == 'regression':\n",
        "            return r2_score(y_true, y_pred)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid task. Choose 'classification' or 'regression'.\")"
      ],
      "metadata": {
        "id": "aex6d4C0ad42"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration"
      ],
      "metadata": {
        "id": "cBt3l05Rayn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_iris, load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, r2_score, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "\n",
        "\n",
        "def classification_comparison():\n",
        "    \"\"\"\n",
        "    Compare custom RandomForestModel with scikit-learn's RandomForestClassifier on the Iris dataset.\n",
        "    \"\"\"\n",
        "    print(\"=== Classification Comparison: Custom RandomForestModel vs. scikit-learn ===\")\n",
        "    iris = load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "\n",
        "    # Split into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Feature Scaling (Optional but Recommended)\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Determine number of features for 'sqrt'\n",
        "    n_features = X_train.shape[1]\n",
        "    max_features = int(np.sqrt(n_features))\n",
        "    print(f\"Number of features: {n_features}, max_features set to: {max_features}\")\n",
        "\n",
        "    # Custom RandomForestModel\n",
        "    print(\"\\n--- Custom RandomForestModel ---\")\n",
        "    custom_rf = RandomForestModel(\n",
        "        n_estimators=10,\n",
        "        max_depth=5,\n",
        "        min_samples_split=2,\n",
        "        max_features=max_features,\n",
        "        bootstrap=True,\n",
        "        random_state=42,\n",
        "        task='classification'\n",
        "    )\n",
        "    start_time = time.time()\n",
        "    custom_rf.fit(X_train, y_train)\n",
        "    train_time_custom = time.time() - start_time\n",
        "    print(f\"Training Time: {train_time_custom:.4f} seconds\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    y_pred_custom = custom_rf.predict(X_test)\n",
        "    y_pred_proba_custom = custom_rf.predict_proba(X_test)\n",
        "    predict_time_custom = time.time() - start_time\n",
        "    accuracy_custom = accuracy_score(y_test, y_pred_custom) * 100.0\n",
        "    roc_auc_custom = roc_auc_score(y_test, y_pred_proba_custom, multi_class='ovo')\n",
        "    print(f\"Prediction Time: {predict_time_custom:.4f} seconds\")\n",
        "    print(f\"Accuracy: {accuracy_custom:.2f}%\")\n",
        "    print(f\"ROC AUC: {roc_auc_custom:.4f}\")\n",
        "\n",
        "    # scikit-learn RandomForestClassifier\n",
        "    print(\"\\n--- scikit-learn RandomForestClassifier ---\")\n",
        "    sklearn_rf = RandomForestClassifier(\n",
        "        n_estimators=10,\n",
        "        max_depth=5,\n",
        "        min_samples_split=2,\n",
        "        max_features='sqrt',\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "    start_time = time.time()\n",
        "    sklearn_rf.fit(X_train, y_train)\n",
        "    train_time_sklearn = time.time() - start_time\n",
        "    print(f\"Training Time: {train_time_sklearn:.4f} seconds\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    y_pred_sklearn = sklearn_rf.predict(X_test)\n",
        "    y_pred_proba_sklearn = sklearn_rf.predict_proba(X_test)\n",
        "    predict_time_sklearn = time.time() - start_time\n",
        "    accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn) * 100.0\n",
        "    roc_auc_sklearn = roc_auc_score(y_test, y_pred_proba_sklearn, multi_class='ovo')\n",
        "    print(f\"Prediction Time: {predict_time_sklearn:.4f} seconds\")\n",
        "    print(f\"Accuracy: {accuracy_sklearn:.2f}%\")\n",
        "    print(f\"ROC AUC: {roc_auc_sklearn:.4f}\")\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n--- Summary ---\")\n",
        "    print(f\"Custom RandomForestModel Training Time: {train_time_custom:.4f} seconds\")\n",
        "    print(f\"scikit-learn RandomForestClassifier Training Time: {train_time_sklearn:.4f} seconds\")\n",
        "    print(f\"Custom RandomForestModel Prediction Time: {predict_time_custom:.4f} seconds\")\n",
        "    print(f\"scikit-learn RandomForestClassifier Prediction Time: {predict_time_sklearn:.4f} seconds\")\n",
        "    print(f\"Custom RandomForestModel Accuracy: {accuracy_custom:.2f}%\")\n",
        "    print(f\"Custom RandomForestModel ROC AUC: {roc_auc_custom:.4f}\")\n",
        "    print(f\"scikit-learn RandomForestClassifier Accuracy: {accuracy_sklearn:.2f}%\")\n",
        "    print(f\"scikit-learn RandomForestClassifier ROC AUC: {roc_auc_sklearn:.4f}\")\n",
        "\n",
        "    return accuracy_custom, roc_auc_custom, accuracy_sklearn, roc_auc_sklearn\n",
        "\n",
        "\n",
        "def regression_comparison():\n",
        "    \"\"\"\n",
        "    Compare custom RandomForestModel with scikit-learn's RandomForestRegressor on the Diabetes dataset.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Regression Comparison: Custom RandomForestModel vs. scikit-learn ===\")\n",
        "    diabetes = load_diabetes()\n",
        "    X, y = diabetes.data, diabetes.target\n",
        "\n",
        "    # Split into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Feature Scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Determine number of features for 'sqrt'\n",
        "    n_features = X_train.shape[1]\n",
        "    max_features = int(np.sqrt(n_features))\n",
        "    print(f\"Number of features: {n_features}, max_features set to: {max_features}\")\n",
        "\n",
        "    # Custom RandomForestModel\n",
        "    print(\"\\n--- Custom RandomForestModel ---\")\n",
        "    custom_rf = RandomForestModel(\n",
        "        n_estimators=10,\n",
        "        max_depth=5,\n",
        "        min_samples_split=2,\n",
        "        max_features=max_features,\n",
        "        bootstrap=True,\n",
        "        random_state=42,\n",
        "        task='regression'\n",
        "    )\n",
        "    start_time = time.time()\n",
        "    custom_rf.fit(X_train, y_train)\n",
        "    train_time_custom = time.time() - start_time\n",
        "    print(f\"Training Time: {train_time_custom:.4f} seconds\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    y_pred_custom = custom_rf.predict(X_test)\n",
        "    predict_time_custom = time.time() - start_time\n",
        "    r2_custom = r2_score(y_test, y_pred_custom)\n",
        "    rmse_custom = np.sqrt(np.mean((y_test - y_pred_custom) ** 2))\n",
        "    print(f\"Prediction Time: {predict_time_custom:.4f} seconds\")\n",
        "    print(f\"R² Score: {r2_custom:.4f}\")\n",
        "    print(f\"RMSE: {rmse_custom:.4f}\")\n",
        "\n",
        "    # scikit-learn RandomForestRegressor\n",
        "    print(\"\\n--- scikit-learn RandomForestRegressor ---\")\n",
        "    sklearn_rf = RandomForestRegressor(\n",
        "        n_estimators=10,\n",
        "        max_depth=5,\n",
        "        min_samples_split=2,\n",
        "        max_features='sqrt',\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "    start_time = time.time()\n",
        "    sklearn_rf.fit(X_train, y_train)\n",
        "    train_time_sklearn = time.time() - start_time\n",
        "    print(f\"Training Time: {train_time_sklearn:.4f} seconds\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    y_pred_sklearn = sklearn_rf.predict(X_test)\n",
        "    predict_time_sklearn = time.time() - start_time\n",
        "    r2_sklearn = r2_score(y_test, y_pred_sklearn)\n",
        "    rmse_sklearn = np.sqrt(np.mean((y_test - y_pred_sklearn) ** 2))\n",
        "    print(f\"Prediction Time: {predict_time_sklearn:.4f} seconds\")\n",
        "    print(f\"R² Score: {r2_sklearn:.4f}\")\n",
        "    print(f\"RMSE: {rmse_sklearn:.4f}\")\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n--- Summary ---\")\n",
        "    print(f\"Custom RandomForestModel Training Time: {train_time_custom:.4f} seconds\")\n",
        "    print(f\"scikit-learn RandomForestRegressor Training Time: {train_time_sklearn:.4f} seconds\")\n",
        "    print(f\"Custom RandomForestModel Prediction Time: {predict_time_custom:.4f} seconds\")\n",
        "    print(f\"scikit-learn RandomForestRegressor Prediction Time: {predict_time_sklearn:.4f} seconds\")\n",
        "    print(f\"Custom RandomForestModel R² Score: {r2_custom:.4f}\")\n",
        "    print(f\"Custom RandomForestModel RMSE: {rmse_custom:.4f}\")\n",
        "    print(f\"scikit-learn RandomForestRegressor R² Score: {r2_sklearn:.4f}\")\n",
        "    print(f\"scikit-learn RandomForestRegressor RMSE: {rmse_sklearn:.4f}\")\n",
        "\n",
        "    return r2_custom, rmse_custom, r2_sklearn, rmse_sklearn\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    accuracy_custom, roc_auc_custom, accuracy_sklearn, roc_auc_sklearn = classification_comparison()\n",
        "    r2_custom, rmse_custom, r2_sklearn, rmse_sklearn = regression_comparison()\n",
        "\n",
        "    # Create DataFrame\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Model': ['CUSTOM', 'SKLEARN'],\n",
        "        'Accuracy': [accuracy_custom, accuracy_sklearn],\n",
        "        'ROC AUC': [roc_auc_custom, roc_auc_sklearn],\n",
        "        'R² Score': [r2_custom, r2_sklearn],\n",
        "        'RMSE': [rmse_custom, rmse_sklearn]\n",
        "    })\n",
        "    print(\"\\n\")\n",
        "    display(metrics_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TWa5YY7YalLS",
        "outputId": "73c5783b-1485-4d4c-a4c6-ab8ce6fdea2c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Classification Comparison: Custom RandomForestModel vs. scikit-learn ===\n",
            "Number of features: 4, max_features set to: 2\n",
            "\n",
            "--- Custom RandomForestModel ---\n",
            "Training Time: 1.4191 seconds\n",
            "Prediction Time: 0.0354 seconds\n",
            "Accuracy: 93.33%\n",
            "ROC AUC: 0.9892\n",
            "\n",
            "--- scikit-learn RandomForestClassifier ---\n",
            "Training Time: 0.0143 seconds\n",
            "Prediction Time: 0.0024 seconds\n",
            "Accuracy: 96.67%\n",
            "ROC AUC: 1.0000\n",
            "\n",
            "--- Summary ---\n",
            "Custom RandomForestModel Training Time: 1.4191 seconds\n",
            "scikit-learn RandomForestClassifier Training Time: 0.0143 seconds\n",
            "Custom RandomForestModel Prediction Time: 0.0354 seconds\n",
            "scikit-learn RandomForestClassifier Prediction Time: 0.0024 seconds\n",
            "Custom RandomForestModel Accuracy: 93.33%\n",
            "Custom RandomForestModel ROC AUC: 0.9892\n",
            "scikit-learn RandomForestClassifier Accuracy: 96.67%\n",
            "scikit-learn RandomForestClassifier ROC AUC: 1.0000\n",
            "\n",
            "=== Regression Comparison: Custom RandomForestModel vs. scikit-learn ===\n",
            "Number of features: 10, max_features set to: 3\n",
            "\n",
            "--- Custom RandomForestModel ---\n",
            "Training Time: 9.1572 seconds\n",
            "Prediction Time: 0.0657 seconds\n",
            "R² Score: 0.4126\n",
            "RMSE: 55.7871\n",
            "\n",
            "--- scikit-learn RandomForestRegressor ---\n",
            "Training Time: 0.0160 seconds\n",
            "Prediction Time: 0.0017 seconds\n",
            "R² Score: 0.4236\n",
            "RMSE: 55.2637\n",
            "\n",
            "--- Summary ---\n",
            "Custom RandomForestModel Training Time: 9.1572 seconds\n",
            "scikit-learn RandomForestRegressor Training Time: 0.0160 seconds\n",
            "Custom RandomForestModel Prediction Time: 0.0657 seconds\n",
            "scikit-learn RandomForestRegressor Prediction Time: 0.0017 seconds\n",
            "Custom RandomForestModel R² Score: 0.4126\n",
            "Custom RandomForestModel RMSE: 55.7871\n",
            "scikit-learn RandomForestRegressor R² Score: 0.4236\n",
            "scikit-learn RandomForestRegressor RMSE: 55.2637\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     Model   Accuracy   ROC AUC  R² Score       RMSE\n",
              "0   CUSTOM  93.333333  0.989167  0.412588  55.787068\n",
              "1  SKLEARN  96.666667  1.000000  0.423558  55.263706"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4aebcaeb-73ae-4ec8-a492-860020064c65\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>ROC AUC</th>\n",
              "      <th>R² Score</th>\n",
              "      <th>RMSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CUSTOM</td>\n",
              "      <td>93.333333</td>\n",
              "      <td>0.989167</td>\n",
              "      <td>0.412588</td>\n",
              "      <td>55.787068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SKLEARN</td>\n",
              "      <td>96.666667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.423558</td>\n",
              "      <td>55.263706</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4aebcaeb-73ae-4ec8-a492-860020064c65')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4aebcaeb-73ae-4ec8-a492-860020064c65 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4aebcaeb-73ae-4ec8-a492-860020064c65');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-abb7cd7c-7889-4c94-b86a-57d5ec79c9d2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-abb7cd7c-7889-4c94-b86a-57d5ec79c9d2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-abb7cd7c-7889-4c94-b86a-57d5ec79c9d2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_eb41a729-0fac-420c-9624-24ba9a27b705\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('metrics_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_eb41a729-0fac-420c-9624-24ba9a27b705 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('metrics_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "metrics_df",
              "summary": "{\n  \"name\": \"metrics_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"SKLEARN\",\n          \"CUSTOM\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.357022603955165,\n        \"min\": 93.33333333333333,\n        \"max\": 96.66666666666667,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          96.66666666666667,\n          93.33333333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ROC AUC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0076603234628542065,\n        \"min\": 0.9891666666666667,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.9891666666666667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"R\\u00b2 Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0077568317027433544,\n        \"min\": 0.4125881817570647,\n        \"max\": 0.4235579983521299,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.4235579983521299,\n          0.4125881817570647\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMSE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3700728053355347,\n        \"min\": 55.26370598276117,\n        \"max\": 55.78706796313214,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          55.26370598276117,\n          55.78706796313214\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}